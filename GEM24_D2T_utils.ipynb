{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_D2T_StratifiedSampling/blob/main/GEM24_D2T_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - D2T data selection"
      ],
      "metadata": {
        "id": "NpKJM2gx_ELW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare repo"
      ],
      "metadata": {
        "id": "imLY7mxmOvjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install packages\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\n",
        "! pip install json2html\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DxwHcdRaLqEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hyry7e09e7P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters"
      ],
      "metadata": {
        "id": "oMZuDfUnTEA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WudT_LOiIoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Filepath definition (upload file(s) in TESTDATA folder after running!)\n",
        "\n",
        "# project_dir_path = os.path.join('/', 'content', 'drive', 'MyDrive', 'WebNLG_data_selection')\n",
        "project_dir_path = '/content'\n",
        "rdf_path = os.path.join(project_dir_path, 'testdata')\n",
        "csv_path = os.path.join(project_dir_path, 'csv_sampling')\n",
        "\n",
        "subtask = 'D2T-1'#@param['D2T-1', 'D2T-2']\n",
        "dataset = 'CFA'#@param['CFA', 'FA', 'FI']\n",
        "# seed used for GEM'24: 49\n",
        "seed = 49#@param\n",
        "seed = int(seed)\n",
        "datacode = subtask+'-'+dataset\n",
        "\n",
        "output_path = os.path.join(csv_path, datacode+'_samplingData.csv')\n",
        "\n",
        "if not os.path.exists(rdf_path):\n",
        "  os.makedirs(rdf_path)\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "  os.makedirs(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create csv file with sampling info. Run once for each file."
      ],
      "metadata": {
        "id": "Jo3-O0qFO0yV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOxH6Hr9kcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function for sampling.\n",
        "\n",
        "def extract_data(rdf_filepath, stratify_categories, exclude_size):\n",
        "\n",
        "  '''\n",
        "      This method:\n",
        "      a. extracts the required entries (RDF triple(s), number of triples, property and category) from the json file.\n",
        "      b. categorizes the triple and verbalisation pair as seen/unseen category based on its presence in the training set.\n",
        "      c. groups the required extracted entry field (in this case, number of triples and property) for stratified selection.\n",
        "  '''\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "  original_id = 1\n",
        "  for filename in os.listdir(rdf_filepath):\n",
        "    if '.xml' in filename and datacode in filename:\n",
        "      tree = ET.parse(f\"{rdf_filepath}/{filename}\")\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # extract triples\n",
        "      for entry in root.findall('./entries/entry'):\n",
        "        triples = []\n",
        "        pred = []\n",
        "        for triple in entry.find('modifiedtripleset').findall('mtriple'):\n",
        "          str_triple = triple.text\n",
        "          triples.append(str_triple)\n",
        "          only_pred = str_triple.split('|')[1]\n",
        "          pred.append(only_pred)\n",
        "        if exclude_size == 'none' or (exclude_size == '1 only' and int(entry.attrib['size']) > 1) or (exclude_size == '1 and 2' and int(entry.attrib['size']) > 2):\n",
        "          curr_entry = {\n",
        "              'id': count,\n",
        "              'original_id': original_id,\n",
        "              'triples': triples.copy(),\n",
        "              'property': pred.copy(),\n",
        "              'num_triples': int(entry.attrib['size']),\n",
        "              'category': 'unseen' if entry.attrib['category'] in ['Athlete', 'Artist', 'CelestialBody', 'MeanOfTransportation', 'Politician'] else 'seen',\n",
        "              'category_all': entry.attrib['category']\n",
        "          }\n",
        "          if stratify_categories == 'seenUnseen':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category']\n",
        "          elif stratify_categories == 'allCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category_all']\n",
        "          elif stratify_categories == 'ignoreCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])\n",
        "          data.append(curr_entry)\n",
        "          count += 1\n",
        "        original_id += 1\n",
        "\n",
        "  # Remove data points for which there is only one member in a stratify category (triggers an error when stratifying, needs 2 members min)\n",
        "  clean_data = []\n",
        "  # Make a dico with the count of instances of each strat_field\n",
        "  count_strat_field_instances = {}\n",
        "  for datapoint in data:\n",
        "    if datapoint['strat_field'] in count_strat_field_instances:\n",
        "      count_strat_field_instances[datapoint['strat_field']] += 1\n",
        "    else:\n",
        "      count_strat_field_instances[datapoint['strat_field']] = 1\n",
        "  # If a count of a strat_field is one, do no include it in the final dataset\n",
        "  for datapoint_clean in data:\n",
        "    if count_strat_field_instances[datapoint_clean['strat_field']] == 1:\n",
        "      print(f\"  Removed datapoint  {datapoint_clean['strat_field']} because there is only one member!\")\n",
        "    else:\n",
        "      clean_data.append(datapoint_clean)\n",
        "\n",
        "  return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idE1moVjJL3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling parameters\n",
        "\n",
        "stratify_categories = 'ignoreCategories'#@param['allCategories', 'seenUnseen', 'ignoreCategories']\n",
        "number_samples = \"180\"#@param[50, 100, 120, 150, 175, 180, 200, 300, 400, 500]\n",
        "num_samples = int(number_samples)\n",
        "exclude_size = '1 only'#@param['none', '1 only', '1 and 2']\n",
        "# Get data\n",
        "data=extract_data(rdf_path, stratify_categories, exclude_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN3pAE5N-ti-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Stratified selection using train_test_split\n",
        "\n",
        "# tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# X_train, X_test, = train_test_split(tset, test_size=num_samples, random_state=seed, stratify=tset['strat_field'], shuffle=True)\n",
        "# print(len(X_train), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Balanced selection using groupby\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# I found three ways, not sure what they exactly do; the second and third ones allow for specifying the random_state\n",
        "# X_test = tset.groupby(tset['strat_field']).apply(lambda s: s.sample(30))\n",
        "# X_test = tset.sample(frac = 1.0, random_state=seed, axis=0).groupby(tset['strat_field']).head(30)\n",
        "# The third one below seems more controlled; grouby uses axis=0\n",
        "X_test = tset.groupby(by=tset['strat_field']).sample(n = 30, random_state=seed)\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wjXcRashq9v2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print some numbers\n",
        "# tset['num_triples']\n",
        "# len(tset.loc[tset['category'] == 'unseen'])\n",
        "# print(X_test['num_triples'])\n",
        "\n",
        "# Show mean of column that contains triple number in each input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)\n",
        "# print(X_test.loc[:, 'num_triples'].mean())\n",
        "print(f\"{round(X_test['num_triples'].mean(), 2)} triples per input on average\")\n",
        "\n",
        "def count_num_instances(pd_column):\n",
        "  count = {}\n",
        "  for category in pd_column:\n",
        "    if category in count:\n",
        "      count[category] += 1\n",
        "    else:\n",
        "      count[category] = 1\n",
        "\n",
        "  for count_category in sorted(count):\n",
        "    print(f'{count_category}\\t{count[count_category]}')\n",
        "    # print(f'{count[count_category]}')\n",
        "  print('-----------------')\n",
        "\n",
        "count_num_instances(X_test['num_triples'])\n",
        "count_num_instances(X_test['category_all'])"
      ],
      "metadata": {
        "id": "wT8jGcydF28v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play around with groupby\n",
        "# df_test_gb = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
        "#                               'Parrot', 'Parrot'],\n",
        "#                    'Max Speed': [380., 370., 24., 26.]})\n",
        "# print(df_test_gb)\n",
        "\n",
        "# print(df_test_gb.groupby(['Animal']).mean())"
      ],
      "metadata": {
        "id": "ge6TruTBPHqt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoBNN96VZEQp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create CSV file\n",
        "X_test.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Create HTML tables for inputs. Run once for each file."
      ],
      "metadata": {
        "id": "HrZjTEDs_MDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from pandas dataframe (running the code above before)\n",
        "from json2html import *\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "path_out_pd = '/content/tables_pd'\n",
        "if not os.path.exists(path_out_pd):\n",
        "  os.makedirs(path_out_pd)\n",
        "\n",
        "for ind in X_test.index:\n",
        "  triple_set = X_test['triples'][ind]\n",
        "  orig_id = X_test['original_id'][ind]\n",
        "  # orig_id starts numbering at 1, while the lists of outputs texts will start numbering at 0, so we need to remove 1 from the original ID to maintain alignment with output files lines\n",
        "  with codecs.open(os.path.join(path_out_pd, f'{datacode}_{str(orig_id-1).rjust(4, \"0\")}.html'), 'w', 'utf-8') as fo:\n",
        "    list_dico_input = []\n",
        "    for triple in triple_set:\n",
        "      dico_triples = {}\n",
        "      dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "      dico_triples['Property'] = triple.split(' | ')[1]\n",
        "      dico_triples['Object'] = triple.split(' | ')[2]\n",
        "      list_dico_input.append(dico_triples)\n",
        "    fo.write(json2html.convert(json = list_dico_input))"
      ],
      "metadata": {
        "id": "T6yqW7aCRTsd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from HuggingFace data\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# from json2html import *\n",
        "# import codecs\n",
        "# import os\n",
        "# import re\n",
        "# import json\n",
        "\n",
        "# # struct2text: common_gen, cs_restaurants, dart, e2e_nlg, totto, web_nlg_en, web_nlg_ru\n",
        "# # schema_guided_dialog\n",
        "# dataset_name = 'web_nlg_en'\n",
        "# dataset = load_dataset('gem', dataset_name)\n",
        "# # subSets = ['test', 'validation', 'train']\n",
        "# subSets = ['test']\n",
        "\n",
        "# for subSet in subSets:\n",
        "#   x = 0\n",
        "#   subSet_inputs = dataset[subSet]\n",
        "#   while x < len (subSet_inputs):\n",
        "#     # fileName_out = 'out_tables/'+dataset_name+'-'+subSet+'-'+str(x)+'.html'\n",
        "#     fileName_out = 'tables_/'+subSet_inputs[x]['gem_id']+'.html'\n",
        "#     if not os.path.exists('tables_'):\n",
        "#       os.makedirs('tables_')\n",
        "#     print('Processing '+fileName_out)\n",
        "#     fo = codecs.open(fileName_out, 'w', 'utf-8')\n",
        "#     list_dico_input = []\n",
        "#     if dataset_name == 'web_nlg_en' or dataset_name == 'web_nlg_ru':\n",
        "#       for triple in subSet_inputs[x]['input']:\n",
        "#         dico_triples = {}\n",
        "#         dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "#         dico_triples['Property'] = triple.split(' | ')[1]\n",
        "#         dico_triples['Object'] = triple.split(' | ')[2]\n",
        "#         list_dico_input.append(dico_triples)\n",
        "#     elif dataset_name == 'common_gen':\n",
        "#       dico_concepts = {}\n",
        "#       dico_concepts['Concepts'] = subSet_inputs[x]['concepts']\n",
        "#       list_dico_input.append(dico_concepts)\n",
        "#     elif dataset_name == 'cs_restaurants':\n",
        "#       dico_DAs = {}\n",
        "#       DA = subSet_inputs[x]['dialog_act'].split('(')[0]\n",
        "#       triples = subSet_inputs[x]['dialog_act'].split('(')[1].split(')')[0]\n",
        "#       dico_DAs['Dialogue Act'] = DA\n",
        "#       if re.search(',', triples):\n",
        "#         dico_DAs['Topics'] = triples.split(',')\n",
        "#       else:\n",
        "#         dico_DAs['Topic'] = triples\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     elif dataset_name == 'e2e_nlg':\n",
        "#       list_properties = subSet_inputs[x]['meaning_representation'].split(', ')\n",
        "#       for input_property in list_properties:\n",
        "#         dico_properties = {}\n",
        "#         prop_name = input_property.split('[')[0]\n",
        "#         prop_value = input_property.split('[')[1].split(']')[0]\n",
        "#         dico_properties['Property'] = prop_name\n",
        "#         dico_properties['Value'] = prop_value\n",
        "#         list_dico_input.append(dico_properties)\n",
        "#     elif dataset_name == 'schema_guided_dialog':\n",
        "#       dico_DAs = {}\n",
        "#       dico_DAs['Dialogue Acts'] = subSet_inputs[x]['dialog_acts']\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     fo.write(json2html.convert(json = list_dico_input))\n",
        "#     fo.close()\n",
        "#     x += 1"
      ],
      "metadata": {
        "id": "b25TnJS9_gWU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ],
      "metadata": {
        "id": "6nYTNyqSQ2X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download tables\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/html_tables.zip'\n",
        "!zip -r {zip_name_inter} /content/tables_pd\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "pRSvuVPY4iKG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download CSVs\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/CSVs.zip'\n",
        "!zip -r {zip_name_inter} /content/csv_sampling\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "JNnuk92B41tu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check the contents of the created CSVs\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob(os.path.join(csv_path, '*.csv'))\n",
        "\n",
        "dico_properties = {}\n",
        "\n",
        "for csv_file in sorted(csv_files):\n",
        "  props_unique = []\n",
        "  count_props_all = 0\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  # print(tail)\n",
        "  df_triple_sets = pd.read_csv(csv_file)['triples']\n",
        "  # print(df_triple_sets)\n",
        "  for triple_set in df_triple_sets:\n",
        "    triples_list = triple_set.replace('\"', \"'\").replace(\"['\", \"\").replace(\"']\", \"\").split(\"', '\")\n",
        "    properties_list = [triple.split(' | ')[1] for triple in triples_list]\n",
        "    for triple_property in properties_list:\n",
        "      count_props_all += 1\n",
        "      if triple_property not in props_unique:\n",
        "        props_unique.append(triple_property)\n",
        "  dico_properties[tail] = props_unique\n",
        "  # print(f'  {count_props_all} properties found (810 expected).')\n",
        "\n",
        "for dataset in dico_properties:\n",
        "  print(dataset, ':', len(dico_properties[dataset]), 'different properties found:')\n",
        "  print('  ', sorted(dico_properties[dataset]))"
      ],
      "metadata": {
        "id": "fKc0BNTSNX-Q",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Create individual files for sampled system outputs\n",
        "\n",
        "First upload all system outputs in a folder named \"sys_outputs\", and generate the corresponding csv file(s) with the code above (or upload manually in a folder called \"csv_sampling\")."
      ],
      "metadata": {
        "id": "DSxbJ9sfV_cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one text file per sampled input/output per team per language per test set\n",
        "import os\n",
        "import glob\n",
        "from pandas import *\n",
        "import codecs\n",
        "\n",
        "out_sampled_folder = '/content/d2t_outputs-sampled'\n",
        "\n",
        "# The 3 CSV files for each task have the same IDs sampled, so we can just use one file per task\n",
        "list_csv_D2T1_OIDs = read_csv('/content/csv_sampling/D2T-1-FA_samplingData.csv')['original_id'].tolist()\n",
        "list_csv_D2T2_OIDs = read_csv('/content/csv_sampling/D2T-2-FA_samplingData.csv')['original_id'].tolist()\n",
        "\n",
        "# The original IDs are numbered starting from 1, we want a number starting from 0 to aling with list indices in the system output files; make it into a dic for easy access afterwards\n",
        "list_csv_ids = {}\n",
        "list_csv_ids['D2T-1'] = [OID1-1 for OID1 in sorted(list_csv_D2T1_OIDs)]\n",
        "list_csv_ids['D2T-2'] = [OID2-1 for OID2 in sorted(list_csv_D2T2_OIDs)]\n",
        "\n",
        "for sys_output_path in glob.glob(os.path.join('/content/sys_outputs', '*.txt')):\n",
        "  head, tail = os.path.split(sys_output_path)\n",
        "  # Get parameters of every output file\n",
        "  team_ID = tail.split('-', 1)[0]\n",
        "  lang_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[1]\n",
        "  data_code_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  subtask_code_out = data_code_out.rsplit('-', 1)[0]\n",
        "  # print(tail)\n",
        "  # print(f'  {team_ID}')\n",
        "  # print(f'  {lang_out}')\n",
        "  # print(f'  {data_code_out}')\n",
        "  # print(f'  {subtask_code_out}')\n",
        "\n",
        "  # Create subfolder to store sampled system outputs\n",
        "  dest_folder_sample = os.path.join(out_sampled_folder, data_code_out, lang_out, team_ID)\n",
        "  if not os.path.exists(dest_folder_sample):\n",
        "    os.makedirs(dest_folder_sample)\n",
        "\n",
        "  # Read sys output\n",
        "  sys_output_all_lines = codecs.open(sys_output_path).readlines()\n",
        "\n",
        "  for id_sampled in list_csv_ids[subtask_code_out]:\n",
        "    # Create text files the last part of the name of which matches the name of the sampled input files\n",
        "    dest_filename_sample = os.path.join(dest_folder_sample, '['+team_ID+'_'+lang_out+']_'+data_code_out+'_'+str(id_sampled).rjust(4, \"0\")+'.txt')\n",
        "    with codecs.open(dest_filename_sample, 'w', 'utf-8') as fo:\n",
        "        fo.write(sys_output_all_lines[id_sampled].strip())\n"
      ],
      "metadata": {
        "id": "vvjhq5OkV9T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download sampled output text files\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/d2t_outputs-sampled.zip'\n",
        "!zip -r {zip_name_inter} /content/d2t_outputs-sampled\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lJ-P6XxXAMMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Create individual files for reference texts collected on AMT"
      ],
      "metadata": {
        "id": "6J6QSDw8EkD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read json and check texts (UPLOAD FIRST)\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# First upload '/content/en_references_v2-formatted.json' and '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Load json\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "# Unzip sampled output folder, which contains lists of IDs of sampled outputs\n",
        "if not os.path.exists('/content/content/d2t_outputs-sampled_grouped'):\n",
        "  ! unzip '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Get list of IDs for all sampled outputs\n",
        "list_IDs_sampled = []\n",
        "for dataset_path in glob.glob(os.path.join('/content/content/d2t_outputs-sampled_grouped', '*')):\n",
        "  head, tail = os.path.split(dataset_path)\n",
        "  # print(tail)\n",
        "  path_IDs = os.path.join(dataset_path, 'en', 'AllSizes', 'AllSizes_sampled_ids.txt')\n",
        "  # Open IDs file, read, and split the list on the first line\n",
        "  datasets_IDs_sampled = codecs.open(path_IDs, 'r', 'utf-8').readlines()[0].strip().replace('[', '').replace(']', '').replace(\"'\", '').split(', ')\n",
        "  for dataset_ID_sampled in datasets_IDs_sampled:\n",
        "    full_ID = f'en_{tail}_{dataset_ID_sampled}'\n",
        "    list_IDs_sampled.append(full_ID)\n",
        "# print(len(list_IDs_sampled))\n",
        "# print(list_IDs_sampled)\n",
        "\n",
        "ids_error = []\n",
        "all_ids_collected = []\n",
        "# Find some errors\n",
        "with codecs.open('/content/writing_problems.txt', 'w', 'utf-8') as fo1:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    all_ids_collected.append(id)\n",
        "    # Capture people who pasted some definitions\n",
        "    if re.search('\\[', text):\n",
        "      fo1.write(f'{id} (pasted definition): {text}\\n')\n",
        "      print(f'{id} (pasted definition): {text}')\n",
        "      ids_error.append(id)\n",
        "    # Capture people who pasted the input\n",
        "    elif re.search('Subject', text):\n",
        "      fo1.write(f'{id} (pasted input): {text}\\n')\n",
        "      print(f'{id} (pasted input): {text}')\n",
        "      ids_error.append(id)\n",
        "\n",
        "  # Find missing files\n",
        "  for ID_sampled in list_IDs_sampled:\n",
        "    if ID_sampled not in all_ids_collected:\n",
        "      fo1.write(f'{ID_sampled} (missing)\\n')\n",
        "      print(f'{ID_sampled} (missing)')\n",
        "\n",
        "if len(ids_error) == 0:\n",
        "  print('No errors found!')\n",
        "\n",
        "ids_seen = []\n",
        "texts_seen = []\n",
        "with codecs.open('/content/writing_duplicates.txt', 'w', 'utf-8') as fo2:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    # Check if there is a duplicate for a text with error\n",
        "    if id in ids_error:\n",
        "      print(f'Found alternative to error {id}')\n",
        "    # Capture duplicate outputs\n",
        "    if id not in ids_seen:\n",
        "      ids_seen.append(id)\n",
        "      texts_seen.append(text)\n",
        "    else:\n",
        "      index_id = ids_seen.index(id)\n",
        "      fo2.write(f'{id} (duplicate): {texts_seen[index_id]}\\n')\n",
        "      # print(f'{id} (duplicate): {texts_seen[index_id]}')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kmsoaZqBEvZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create files\n",
        "import json\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "# First upload '/content/en_references-formatted.json'\n",
        "\n",
        "# Create folders for text files\n",
        "path_collected_references = os.path.join('/content', 'texts_AMT')\n",
        "if not os.path.exists(path_collected_references):\n",
        "  os.makedirs(path_collected_references)\n",
        "\n",
        "# Load json\n",
        "ids_seen = []\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "for reference_text in references_json[\"references\"]:\n",
        "  id = reference_text[\"short_id\"]\n",
        "  # We only take the first text available\n",
        "  if id not in ids_seen:\n",
        "    ids_seen.append(id)\n",
        "    language = id.split('_', 1)[0]\n",
        "    id_nolang = id.split('_', 1)[1]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    filename = os.path.join(path_collected_references, f'[0_{language}]_{id_nolang}.txt')\n",
        "    with codecs.open(filename, 'w', 'utf-8') as fo:\n",
        "      fo.write(text)\n",
        "print(f'{str(len(ids_seen))} files were created (1,080 expected.)')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MYarTMTqapUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download texts\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "zip_name_inter = '/content/texts_AMT.zip'\n",
        "!zip -r {zip_name_inter} /content/texts_AMT\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XdJpx2r0foaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Create files for automatic evaluation by size (sizes 2 to 7 and all sizes together)"
      ],
      "metadata": {
        "id": "eBhJJ9MYeRFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Group references and system outputs by size\n",
        "from posix import write\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import shutil\n",
        "\n",
        "language_out = 'en' #['en', 'es', 'sw']\n",
        "teams_withdrawn = ['8']\n",
        "# First upload '/content/csv_sampling/d2t_inputs-sampled.zip' and /content/csv_sampling/d2t_outputs-sampled.zip\n",
        "\n",
        "folder_suffix = '-enAmtRefs' #param['-onlyWebnlgRef', '-enAmtRefs']\n",
        "outputs_sampled_folder= '/content/csv_sampling/d2t_outputs-sampled'+folder_suffix+'.zip'\n",
        "inputs_sampled_folder= '/content/csv_sampling/d2t_inputs-sampled.zip'\n",
        "\n",
        "# Unzip sampled input and output files if they have not been unzipped already:\n",
        "if not os.path.exists('/content/d2t_inputs-sampled'):\n",
        "  ! unzip {outputs_sampled_folder}\n",
        "  ! unzip {inputs_sampled_folder}\n",
        "\n",
        "# Delete /content/d2t_outputs-sampled_grouped folder\n",
        "if os.path.exists('/content/d2t_outputs-sampled'+folder_suffix+'_grouped'):\n",
        "  shutil.rmtree('/content/d2t_outputs-sampled'+folder_suffix+'_grouped')\n",
        "\n",
        "def writeFile(file_to_write, content, total_num_lines, current_num_lines):\n",
        "    file_to_write.write(content)\n",
        "    # After all lines except the last one, insert linebreak\n",
        "    if current_num_lines < total_num_lines-1:\n",
        "      file_to_write.write('\\n')\n",
        "\n",
        "# Create new output folders for storing files with grouped outputs for each team\n",
        "subfolders_out = sorted([os.path.join(f.path, language_out) for f in os.scandir('/content/d2t_outputs-sampled'+folder_suffix+'/d2t_outputs-sampled') if f.is_dir()])\n",
        "# print(subfolders_out)\n",
        "subfolders_out_grouped = []\n",
        "for subfolder_out in subfolders_out:\n",
        "  # '/content/d2t_outputs-sampled-onlyWebnlgRef/d2t_outputs-sampled/D2T-1-CFA/en'\n",
        "  new_folder_name = re.sub('d2t_outputs-sampled'+folder_suffix, 'd2t_outputs-sampled'+folder_suffix+'_grouped', subfolder_out)\n",
        "  subfolders_out_grouped.append(new_folder_name)\n",
        "  if not os.path.exists(new_folder_name):\n",
        "    os.makedirs(new_folder_name)\n",
        "\n",
        "# Read files with sampling info (we only need one file per subtask since all IDs are the same for each subtask)\n",
        "csv_files = glob.glob(os.path.join('/content/d2t_inputs-sampled/info_sampling-csv', '*-FA_samplingData.csv'))\n",
        "\n",
        "# Build a dico with the IDs grouped by size\n",
        "# {'D2T-1': {2: [46, 920, 936, 1423, 1723, 410, 1653, 1589, 762, 1471, 590, 1685, 1217, 1355, 99, 1339, 1079, 1420, 276, 771, 757, 845, 472, 1222, 1072, 582, 1043, 360, 1387, 1113], ...}}\n",
        "dico_ids_per_size = {}\n",
        "for csv_file in sorted(csv_files):\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  task = tail.rsplit('-', 1)[0]\n",
        "  df_triple_sets = pd.read_csv(csv_file)\n",
        "  dico_ids_per_size[task] = {}\n",
        "  for n in range(2, 8):\n",
        "    dico_ids_per_size[task][n] = [df_triple_sets.loc[df_triple_sets['num_triples'] == n, 'original_id'].tolist()][0]\n",
        "# print(dico_ids_per_size)\n",
        "# print(subfolders_out)\n",
        "# print(subfolders_out_grouped)\n",
        "\n",
        "# subfolder_out is something like /content/d2t_outputs-sampled/D2T-1-CFA/en\n",
        "for subfolder_out, subfolder_out_grouped in zip(subfolders_out, subfolders_out_grouped):\n",
        "  list_sampled_IDs = []\n",
        "  # Get folders inside subfolder_out\n",
        "  teams_out_folders = sorted([f.path for f in os.scandir(subfolder_out) if f.is_dir() and f.name not in teams_withdrawn])\n",
        "  # print(teams_out_folders)\n",
        "  # For each team out folder, check if the path contains one of the keys in dico_ids_per_size\n",
        "  # team_out_folder is something like /content/d2t_outputs-sampled/D2T-1-CFA/en/1\n",
        "  for team_out_folder in teams_out_folders:\n",
        "    num_of_IDs_total = 0\n",
        "    head_tf, team_id = os.path.split(team_out_folder)\n",
        "    print(team_out_folder)\n",
        "    # There are 2 subtasks in dico_ids_per_size: D2T-1 and D2T-2\n",
        "    for task in dico_ids_per_size.keys():\n",
        "      # Make sure we are in the output folder of the right subtask\n",
        "      if task in team_out_folder:\n",
        "        # print(f'  {task} found in {team_out_folder}')\n",
        "        # Within each subtask, we want to create separate files for each input size\n",
        "        for size in dico_ids_per_size[task]:\n",
        "          # print(f'  NumOfIDs: {num_of_IDs_total}')\n",
        "          print(f'    {size}: {dico_ids_per_size[task][size]}')\n",
        "          # Create a folder for each size (to run the eval later on with my notebook, I need to process all hypotheses for one (set of) reference file(s)).\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'Size'+str(size))):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'Size'+str(size)))\n",
        "          # Create a folder where we'll group all the sizes for each system in one file\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'AllSizes')):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'AllSizes'))\n",
        "\n",
        "          # Get a padded version of all IDs in dico_ids_per_size[task][size]\n",
        "          for num_of_IDs_for_a_size, id in enumerate(dico_ids_per_size[task][size]):\n",
        "            # We need to subtract 1 to the original ID to match the ID of the files, for which the numbering starts from 0\n",
        "            padded_aligned_id = str(id-1).zfill(4)\n",
        "            # Now iterate over all the output files of the current folder\n",
        "            for team_out_file_path in glob.glob(os.path.join(team_out_folder, '*.txt')):\n",
        "              head_tf, tail_tf = os.path.split(team_out_file_path)\n",
        "              # Look for files that match one of the IDs of texts of a specific size\n",
        "              if padded_aligned_id in tail_tf:\n",
        "                with codecs.open(team_out_file_path, 'r', 'utf-8') as ftext:\n",
        "                  contents_read = ftext.read()\n",
        "                  # Write separate files for all sizes\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), str(team_id)+'_Size'+str(size)+'.txt'), 'a', 'utf-8') as f_sep:\n",
        "                    writeFile(f_sep, contents_read, len(dico_ids_per_size[task][size]), num_of_IDs_for_a_size)\n",
        "                  # Write one file with all sizes grouped\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', str(team_id)+'_AllSizes.txt'), 'a', 'utf-8') as f_all:\n",
        "                    writeFile(f_all, contents_read, len(dico_ids_per_size[task][size])*len(dico_ids_per_size[task]), num_of_IDs_total)\n",
        "            num_of_IDs_total += 1\n",
        "\n",
        "          # Now gather the IDs of the inputs that will match the filenames for each dataset and size\n",
        "          padded_aligned_id_list = [str(id-1).zfill(4) for id in dico_ids_per_size[task][size]]\n",
        "          with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), 'Size'+str(size)+'_sampled_ids.txt'), 'w', 'utf-8') as f_id_sep:\n",
        "            f_id_sep.write(str(sorted(padded_aligned_id_list)))\n",
        "          # Add all the IDs into a list for all sizes\n",
        "          for padded_aligned_id2 in padded_aligned_id_list:\n",
        "            if padded_aligned_id2 not in list_sampled_IDs:\n",
        "              list_sampled_IDs.append(padded_aligned_id2)\n",
        "  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', 'AllSizes_sampled_ids.txt'), 'a', 'utf-8') as f_id_all:\n",
        "    f_id_all.write(str(sorted(list_sampled_IDs)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EC7QaRtTC2qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download files\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "folder_to_zip = '/content/d2t_outputs-sampled'+folder_suffix+'_grouped'\n",
        "zip_name_inter = folder_to_zip+'.zip'\n",
        "!zip -r {zip_name_inter} {folder_to_zip}\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IMrnK9tDjwci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - Create plots from automatic evaluation logs"
      ],
      "metadata": {
        "id": "VB_Lq9pFfRgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots per size\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "# First upload the log_eval.zip file (for 1 dataset) obtained with this notebook: https://github.com/mille-s/WebNLG-2020_Metrics\n",
        "\n",
        "dataset = 'D2T-1-FA' #@param['D2T-1-FA', 'D2T-1-CFA', 'D2T-1-FI', 'D2T-2-FA', 'D2T-2-CFA', 'D2T-2-FI']\n",
        "# Unzip log_eval folder\n",
        "# For when we zip the folders inside the content folder\n",
        "path_logEval = '/content/content/log_eval'\n",
        "path_zip = '/content/log_eval_'+dataset+'.zip'\n",
        "if os.path.exists(path_logEval):\n",
        "  shutil.rmtree(path_logEval)\n",
        "  ! unzip {path_zip}\n",
        "else:\n",
        "  ! unzip {path_zip}\n",
        "\n",
        "# dataset = 'D2T-1-FA'#@param['D2T-1-CFA', 'D2T-1-FA', 'D2T-1-FI', 'D2T-2-CFA', 'D2T-2-FA', 'D2T-2-FI']\n",
        "metrics = ['BLEU', 'METEOR', 'chrF++', 'BERT-SCORE F1']\n",
        "system_IDnames_dico = {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}\n",
        "\n",
        "# dico_plot_BLEU = {}\n",
        "# dico_plot_METEOR = {}\n",
        "# dico_plot_chrF = {}\n",
        "# dico_plot_BERT = {}\n",
        "dico_plot_per_size = {}\n",
        "dico_plot_all_sizes = {}\n",
        "eval_files_paths = glob.glob(os.path.join(path_logEval, '*.txt'))\n",
        "for eval_file_path in sorted(eval_files_paths):\n",
        "  head, tail = os.path.split(eval_file_path)\n",
        "  # values for \"size\": 2 to 7, and \"s\" for all sizes\n",
        "  size = tail.rsplit('_', 1)[1].rsplit('.', 1)[0].split('Size')[1]\n",
        "  sys_id = tail.rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  dataset = tail.rsplit('_', 3)[0].rsplit('_', 1)[1]\n",
        "  # print(f'{sys_id}  {size}')\n",
        "\n",
        "  # Make a key in dico_plot with a new system id\n",
        "  # if sys_id not in dico_plot_BLEU:\n",
        "  #   dico_plot_BLEU[sys_id] = []\n",
        "  # if sys_id not in dico_plot_METEOR:\n",
        "  #   dico_plot_METEOR[sys_id] = []\n",
        "  # if sys_id not in dico_plot_chrF:\n",
        "  #   dico_plot_chrF[sys_id] = []\n",
        "  # if sys_id not in dico_plot_BERT:\n",
        "  #   dico_plot_BERT[sys_id] = []\n",
        "\n",
        "  # The scores are on the penultimate line of the log file, the metrics names 2 lines above\n",
        "  log_lines_metrics = [line.rstrip('\\n') for line in open(eval_file_path)][-3]\n",
        "  log_lines_scores = [line.rstrip('\\n') for line in open(eval_file_path)][-1]\n",
        "  # Split lines with names and scores\n",
        "  # Metrics are separated by 4 spaces, with 2 initial spaces; some metrics have a space inside the name\n",
        "  metrics_list = [x for x in log_lines_metrics.split('  ') if not x == '']\n",
        "  # Scores are separated by an unknown number of spaces that depends on the metric name (to show metrics and scores aligned)\n",
        "  scores_list = [x for x in log_lines_scores.split(' ') if not x == '']\n",
        "  # Get the pairs metric/score for the metrics we are interested in\n",
        "  for metric_name, score in zip(metrics_list, scores_list):\n",
        "    if metric_name in metrics:\n",
        "      # print(f'{metric_name}: {score}')\n",
        "      if not size == 's':\n",
        "        if metric_name not in dico_plot_per_size:\n",
        "          dico_plot_per_size[metric_name] = {}\n",
        "        if sys_id not in dico_plot_per_size[metric_name]:\n",
        "          dico_plot_per_size[metric_name][sys_id] = []\n",
        "        dico_plot_per_size[metric_name][sys_id].append(float(score))\n",
        "      else:\n",
        "        # print(f'{sys_id} AllSizes')\n",
        "        # print(f'{metric_name}: {score}')\n",
        "        if sys_id not in dico_plot_all_sizes:\n",
        "          dico_plot_all_sizes[sys_id] = []\n",
        "        dico_plot_all_sizes[sys_id].append(float(score))\n",
        "  # print(metrics_list)\n",
        "  # print(scores_list)\n",
        "\n",
        "\n",
        "# Print the lines to put in a latex table\n",
        "print(f'\\nResults all sizes (order: BLEU, METEOR, chrF++, BERT-SCORE F1):')\n",
        "print(dico_plot_all_sizes)\n",
        "for sys_id in dico_plot_all_sizes:\n",
        "  print(system_IDnames_dico[sys_id]+' & '+' & '.join([str(x) for x in dico_plot_all_sizes[sys_id]])+' \\\\\\\\')\n",
        "\n",
        "print(f'\\nResults per size:')\n",
        "print(dico_plot_per_size)\n",
        "\n",
        "system_colors = {'1':'dodgerblue', '2':'blue', '3':'orange', '4':'green', '5':'red', '6':'purple', '7':'brown'}\n",
        "# Make and save plots in png\n",
        "for metric_name in dico_plot_per_size:\n",
        "  for sys_id in dico_plot_per_size[metric_name]:\n",
        "    plt.plot(range(2, len(dico_plot_per_size[metric_name][sys_id]) + 2), dico_plot_per_size[metric_name][sys_id], label=system_IDnames_dico[sys_id], color=system_colors[sys_id])\n",
        "  plt.title(f'{dataset}: {metric_name}')\n",
        "  plt.xlabel(\"Input size\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  plt.legend()\n",
        "  #https://blog.timodenk.com/exporting-matplotlib-plots-to-latex/\n",
        "  # I can't generate the plots inside latex\n",
        "  # plt.savefig(f'{dataset}_{metric_name}.pgf')\n",
        "  # Save the plot as a png file\n",
        "  plt.savefig(f'{dataset}_{metric_name}.png')\n",
        "  # plt.show needs to be after savefig, otherwise the image is blank\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GUt1BfdTfeum",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots for all sizes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics order in dico_plot_all_sizes: BLEU, METEOR, chrF++, BERT-SCORE F1\n",
        "metrics_order = ['BLEU', 'METEOR', 'chrF++', 'BERT']\n",
        "metrics_lower_limit = [0, 0.2, 0.4, 0.9]\n",
        "metrics_upper_limit = [40, 0.4, 0.6, 0.95]\n",
        "system_IDShortnames_dico = {'2':'DCU-PBN', '3':'DCU-Sm', '4':'DipInfo', '5':'OSU', '6':'pyrealb', '7':'Saar'}\n",
        "teams = [system_IDShortnames_dico[x] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "\n",
        "for metric_id in range(4):\n",
        "  fig, ax = plt.subplots()\n",
        "  bar_labels = teams\n",
        "  bar_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']\n",
        "  scores = [dico_plot_all_sizes[x][metric_id] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "  ax.bar(teams, scores, label=bar_labels, color=bar_colors)\n",
        "  plt.ylim(metrics_lower_limit[metric_id], metrics_upper_limit[metric_id])\n",
        "  ax.set_ylabel('Score')\n",
        "  ax.set_title(metrics_order[metric_id]+'-'+dataset)\n",
        "  # ax.legend(title='Teams')\n",
        "  plt.savefig(f'{dataset}_{metrics_order[metric_id]}_allSys.png')\n",
        "  plt.show()\n",
        "\n",
        "  # Also create a Latex table for the paper\n",
        "\n"
      ],
      "metadata": {
        "id": "4BARvDcHZPr1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate D2T score differences for each system on the different datasets\n",
        "\n",
        "system_IDShortnames = ['DCU-PBN', 'DCU-Sm', 'DipInfo', 'OSU', 'pyrealb', 'Saar']\n",
        "#BLEU (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bleuScores = [[29.08, 25.2, 26.02, 23.96, 30.34, 20.46], [27.0, 22.98, 20.85, 19.48, 24.9, 16.88], [32.31, 29.01, 28.24, 27.22, 32.01, 21.26], [30.03, 24.45, 21.44, 24.97, 27.06, 16.9], [26.37, 21.67, 21.97, 19.97, 25.05, 16.28], [29.7, 23.48, 20.76, 28.25, 26.47, 20.16]]\n",
        "#METEOR (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "meteorScores = [[0.33, 0.297, 0.322, 0.295, 0.348, 0.3], [0.314, 0.279, 0.292, 0.26, 0.3, 0.267], [0.346, 0.315, 0.342, 0.304, 0.354, 0.307], [0.335, 0.293, 0.306, 0.295, 0.334, 0.282], [0.331, 0.291, 0.31, 0.287, 0.335, 0.286], [0.347, 0.307, 0.331, 0.32, 0.359, 0.315]]\n",
        "#CHRF (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "chrfScores = [[0.555, 0.513, 0.549, 0.49, 0.581, 0.49], [0.537, 0.488, 0.507, 0.438, 0.51, 0.442], [0.58, 0.543, 0.587, 0.512, 0.592, 0.502], [0.566, 0.514, 0.537, 0.496, 0.567, 0.475], [0.551, 0.495, 0.527, 0.479, 0.561, 0.472], [0.581, 0.524, 0.557, 0.538, 0.597, 0.518]]\n",
        "#BERT (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bertScores = [[0.933, 0.923, 0.92, 0.936, 0.937, 0.924], [0.93, 0.918, 0.914, 0.925, 0.923, 0.914], [0.933, 0.926, 0.924, 0.937, 0.936, 0.923], [0.932, 0.92, 0.915, 0.934, 0.93, 0.917], [0.928, 0.918, 0.917, 0.921, 0.923, 0.916], [0.931, 0.921, 0.917, 0.934, 0.929, 0.919]]\n",
        "\n",
        "def calculateDiffs(list_scores, system_IDShortnames, metric):\n",
        "  print(metric)\n",
        "  for pos, systemScores in enumerate(list_scores):\n",
        "    system_name = system_IDShortnames[pos]\n",
        "    if system_name == 'DCU-Sm':\n",
        "      print(f'  {system_name}')\n",
        "      print(f'    1 FA to CFA: {round(systemScores[1]-systemScores[0], 2)}')\n",
        "      print(f'    1 FA to FI: {round(systemScores[2]-systemScores[0], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    2 FA to CFA: {round(systemScores[4]-systemScores[3], 2)}')\n",
        "      print(f'    2 FA to FI: {round(systemScores[5]-systemScores[3], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    FA 1 to 2: {round(systemScores[3]-systemScores[0], 2)}')\n",
        "      print(f'    CFA 1 to 2: {round(systemScores[4]-systemScores[1], 2)}')\n",
        "      print(f'    FI 1 to 2: {round(systemScores[5]-systemScores[2], 2)}')\n",
        "\n",
        "calculateDiffs(bleuScores, system_IDShortnames, 'BLEU')\n",
        "calculateDiffs(meteorScores, system_IDShortnames, 'METEOR')\n",
        "calculateDiffs(chrfScores, system_IDShortnames, 'chrF++')\n",
        "calculateDiffs(bertScores, system_IDShortnames, 'BERT')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z_I8H9TyF1aE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NpKJM2gx_ELW",
        "Jo3-O0qFO0yV",
        "HrZjTEDs_MDE",
        "DSxbJ9sfV_cH",
        "6J6QSDw8EkD0",
        "eBhJJ9MYeRFe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}