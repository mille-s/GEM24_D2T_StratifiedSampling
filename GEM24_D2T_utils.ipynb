{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_D2T_StratifiedSampling/blob/main/GEM24_D2T_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D2T data selection"
      ],
      "metadata": {
        "id": "NpKJM2gx_ELW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare repo"
      ],
      "metadata": {
        "id": "imLY7mxmOvjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install packages\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\n",
        "! pip install json2html\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DxwHcdRaLqEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hyry7e09e7P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters"
      ],
      "metadata": {
        "id": "oMZuDfUnTEA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WudT_LOiIoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Filepath definition (upload file(s) in TESTDATA folder after running!)\n",
        "\n",
        "# project_dir_path = os.path.join('/', 'content', 'drive', 'MyDrive', 'WebNLG_data_selection')\n",
        "project_dir_path = '/content'\n",
        "rdf_path = os.path.join(project_dir_path, 'testdata')\n",
        "csv_path = os.path.join(project_dir_path, 'csv_sampling')\n",
        "\n",
        "subtask = 'D2T-1'#@param['D2T-1', 'D2T-2']\n",
        "dataset = 'CFA'#@param['CFA', 'FA', 'FI']\n",
        "# seed used for GEM'24: 49\n",
        "seed = 49#@param\n",
        "seed = int(seed)\n",
        "datacode = subtask+'-'+dataset\n",
        "\n",
        "output_path = os.path.join(csv_path, datacode+'_samplingData.csv')\n",
        "\n",
        "if not os.path.exists(rdf_path):\n",
        "  os.makedirs(rdf_path)\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "  os.makedirs(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create csv file with sampling info. Run once for each file."
      ],
      "metadata": {
        "id": "Jo3-O0qFO0yV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOxH6Hr9kcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function for sampling.\n",
        "\n",
        "def extract_data(rdf_filepath, stratify_categories, exclude_size):\n",
        "\n",
        "  '''\n",
        "      This method:\n",
        "      a. extracts the required entries (RDF triple(s), number of triples, property and category) from the json file.\n",
        "      b. categorizes the triple and verbalisation pair as seen/unseen category based on its presence in the training set.\n",
        "      c. groups the required extracted entry field (in this case, number of triples and property) for stratified selection.\n",
        "  '''\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "  original_id = 1\n",
        "  for filename in os.listdir(rdf_filepath):\n",
        "    if '.xml' in filename and datacode in filename:\n",
        "      tree = ET.parse(f\"{rdf_filepath}/{filename}\")\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # extract triples\n",
        "      for entry in root.findall('./entries/entry'):\n",
        "        triples = []\n",
        "        pred = []\n",
        "        for triple in entry.find('modifiedtripleset').findall('mtriple'):\n",
        "          str_triple = triple.text\n",
        "          triples.append(str_triple)\n",
        "          only_pred = str_triple.split('|')[1]\n",
        "          pred.append(only_pred)\n",
        "        if exclude_size == 'none' or (exclude_size == '1 only' and int(entry.attrib['size']) > 1) or (exclude_size == '1 and 2' and int(entry.attrib['size']) > 2):\n",
        "          curr_entry = {\n",
        "              'id': count,\n",
        "              'original_id': original_id,\n",
        "              'triples': triples.copy(),\n",
        "              'property': pred.copy(),\n",
        "              'num_triples': int(entry.attrib['size']),\n",
        "              'category': 'unseen' if entry.attrib['category'] in ['Athlete', 'Artist', 'CelestialBody', 'MeanOfTransportation', 'Politician'] else 'seen',\n",
        "              'category_all': entry.attrib['category']\n",
        "          }\n",
        "          if stratify_categories == 'seenUnseen':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category']\n",
        "          elif stratify_categories == 'allCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category_all']\n",
        "          elif stratify_categories == 'ignoreCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])\n",
        "          data.append(curr_entry)\n",
        "          count += 1\n",
        "        original_id += 1\n",
        "\n",
        "  # Remove data points for which there is only one member in a stratify category (triggers an error when stratifying, needs 2 members min)\n",
        "  clean_data = []\n",
        "  # Make a dico with the count of instances of each strat_field\n",
        "  count_strat_field_instances = {}\n",
        "  for datapoint in data:\n",
        "    if datapoint['strat_field'] in count_strat_field_instances:\n",
        "      count_strat_field_instances[datapoint['strat_field']] += 1\n",
        "    else:\n",
        "      count_strat_field_instances[datapoint['strat_field']] = 1\n",
        "  # If a count of a strat_field is one, do no include it in the final dataset\n",
        "  for datapoint_clean in data:\n",
        "    if count_strat_field_instances[datapoint_clean['strat_field']] == 1:\n",
        "      print(f\"  Removed datapoint  {datapoint_clean['strat_field']} because there is only one member!\")\n",
        "    else:\n",
        "      clean_data.append(datapoint_clean)\n",
        "\n",
        "  return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idE1moVjJL3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling parameters\n",
        "\n",
        "stratify_categories = 'ignoreCategories'#@param['allCategories', 'seenUnseen', 'ignoreCategories']\n",
        "number_samples = \"180\"#@param[50, 100, 120, 150, 175, 180, 200, 300, 400, 500]\n",
        "num_samples = int(number_samples)\n",
        "exclude_size = '1 only'#@param['none', '1 only', '1 and 2']\n",
        "# Get data\n",
        "data=extract_data(rdf_path, stratify_categories, exclude_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN3pAE5N-ti-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Stratified selection using train_test_split\n",
        "\n",
        "# tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# X_train, X_test, = train_test_split(tset, test_size=num_samples, random_state=seed, stratify=tset['strat_field'], shuffle=True)\n",
        "# print(len(X_train), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Balanced selection using groupby\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# I found three ways, not sure what they exactly do; the second and third ones allow for specifying the random_state\n",
        "# X_test = tset.groupby(tset['strat_field']).apply(lambda s: s.sample(30))\n",
        "# X_test = tset.sample(frac = 1.0, random_state=seed, axis=0).groupby(tset['strat_field']).head(30)\n",
        "# The third one below seems more controlled; grouby uses axis=0\n",
        "X_test = tset.groupby(by=tset['strat_field']).sample(n = 30, random_state=seed)\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wjXcRashq9v2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print some numbers\n",
        "# tset['num_triples']\n",
        "# len(tset.loc[tset['category'] == 'unseen'])\n",
        "# print(X_test['num_triples'])\n",
        "\n",
        "# Show mean of column that contains triple number in each input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)\n",
        "# print(X_test.loc[:, 'num_triples'].mean())\n",
        "print(f\"{round(X_test['num_triples'].mean(), 2)} triples per input on average\")\n",
        "\n",
        "def count_num_instances(pd_column):\n",
        "  count = {}\n",
        "  for category in pd_column:\n",
        "    if category in count:\n",
        "      count[category] += 1\n",
        "    else:\n",
        "      count[category] = 1\n",
        "\n",
        "  for count_category in sorted(count):\n",
        "    print(f'{count_category}\\t{count[count_category]}')\n",
        "    # print(f'{count[count_category]}')\n",
        "  print('-----------------')\n",
        "\n",
        "count_num_instances(X_test['num_triples'])\n",
        "count_num_instances(X_test['category_all'])"
      ],
      "metadata": {
        "id": "wT8jGcydF28v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play around with groupby\n",
        "# df_test_gb = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
        "#                               'Parrot', 'Parrot'],\n",
        "#                    'Max Speed': [380., 370., 24., 26.]})\n",
        "# print(df_test_gb)\n",
        "\n",
        "# print(df_test_gb.groupby(['Animal']).mean())"
      ],
      "metadata": {
        "id": "ge6TruTBPHqt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoBNN96VZEQp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create CSV file\n",
        "X_test.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create HTML tables for inputs. Run once for each file."
      ],
      "metadata": {
        "id": "HrZjTEDs_MDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from pandas dataframe (running the code above before)\n",
        "from json2html import *\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "path_out_pd = '/content/tables_pd'\n",
        "if not os.path.exists(path_out_pd):\n",
        "  os.makedirs(path_out_pd)\n",
        "\n",
        "for ind in X_test.index:\n",
        "  triple_set = X_test['triples'][ind]\n",
        "  orig_id = X_test['original_id'][ind]\n",
        "  # orig_id starts numbering at 1, while the lists of outputs texts will start numbering at 0, so we need to remove 1 from the original ID to maintain alignment with output files lines\n",
        "  with codecs.open(os.path.join(path_out_pd, f'{datacode}_{str(orig_id-1).rjust(4, \"0\")}.html'), 'w', 'utf-8') as fo:\n",
        "    list_dico_input = []\n",
        "    for triple in triple_set:\n",
        "      dico_triples = {}\n",
        "      dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "      dico_triples['Property'] = triple.split(' | ')[1]\n",
        "      dico_triples['Object'] = triple.split(' | ')[2]\n",
        "      list_dico_input.append(dico_triples)\n",
        "    fo.write(json2html.convert(json = list_dico_input))"
      ],
      "metadata": {
        "id": "T6yqW7aCRTsd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from HuggingFace data\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# from json2html import *\n",
        "# import codecs\n",
        "# import os\n",
        "# import re\n",
        "# import json\n",
        "\n",
        "# # struct2text: common_gen, cs_restaurants, dart, e2e_nlg, totto, web_nlg_en, web_nlg_ru\n",
        "# # schema_guided_dialog\n",
        "# dataset_name = 'web_nlg_en'\n",
        "# dataset = load_dataset('gem', dataset_name)\n",
        "# # subSets = ['test', 'validation', 'train']\n",
        "# subSets = ['test']\n",
        "\n",
        "# for subSet in subSets:\n",
        "#   x = 0\n",
        "#   subSet_inputs = dataset[subSet]\n",
        "#   while x < len (subSet_inputs):\n",
        "#     # fileName_out = 'out_tables/'+dataset_name+'-'+subSet+'-'+str(x)+'.html'\n",
        "#     fileName_out = 'tables_/'+subSet_inputs[x]['gem_id']+'.html'\n",
        "#     if not os.path.exists('tables_'):\n",
        "#       os.makedirs('tables_')\n",
        "#     print('Processing '+fileName_out)\n",
        "#     fo = codecs.open(fileName_out, 'w', 'utf-8')\n",
        "#     list_dico_input = []\n",
        "#     if dataset_name == 'web_nlg_en' or dataset_name == 'web_nlg_ru':\n",
        "#       for triple in subSet_inputs[x]['input']:\n",
        "#         dico_triples = {}\n",
        "#         dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "#         dico_triples['Property'] = triple.split(' | ')[1]\n",
        "#         dico_triples['Object'] = triple.split(' | ')[2]\n",
        "#         list_dico_input.append(dico_triples)\n",
        "#     elif dataset_name == 'common_gen':\n",
        "#       dico_concepts = {}\n",
        "#       dico_concepts['Concepts'] = subSet_inputs[x]['concepts']\n",
        "#       list_dico_input.append(dico_concepts)\n",
        "#     elif dataset_name == 'cs_restaurants':\n",
        "#       dico_DAs = {}\n",
        "#       DA = subSet_inputs[x]['dialog_act'].split('(')[0]\n",
        "#       triples = subSet_inputs[x]['dialog_act'].split('(')[1].split(')')[0]\n",
        "#       dico_DAs['Dialogue Act'] = DA\n",
        "#       if re.search(',', triples):\n",
        "#         dico_DAs['Topics'] = triples.split(',')\n",
        "#       else:\n",
        "#         dico_DAs['Topic'] = triples\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     elif dataset_name == 'e2e_nlg':\n",
        "#       list_properties = subSet_inputs[x]['meaning_representation'].split(', ')\n",
        "#       for input_property in list_properties:\n",
        "#         dico_properties = {}\n",
        "#         prop_name = input_property.split('[')[0]\n",
        "#         prop_value = input_property.split('[')[1].split(']')[0]\n",
        "#         dico_properties['Property'] = prop_name\n",
        "#         dico_properties['Value'] = prop_value\n",
        "#         list_dico_input.append(dico_properties)\n",
        "#     elif dataset_name == 'schema_guided_dialog':\n",
        "#       dico_DAs = {}\n",
        "#       dico_DAs['Dialogue Acts'] = subSet_inputs[x]['dialog_acts']\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     fo.write(json2html.convert(json = list_dico_input))\n",
        "#     fo.close()\n",
        "#     x += 1"
      ],
      "metadata": {
        "id": "b25TnJS9_gWU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ],
      "metadata": {
        "id": "6nYTNyqSQ2X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download tables\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/html_tables.zip'\n",
        "!zip -r {zip_name_inter} /content/tables_pd\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "pRSvuVPY4iKG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download CSVs\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/CSVs.zip'\n",
        "!zip -r {zip_name_inter} /content/csv_sampling\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "JNnuk92B41tu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create individual files for sampled system outputs\n",
        "\n",
        "First upload all system outputs in a folder named \"sys_outputs\", and generate the corresponding csv file(s) with the code above (or upload manually in a folder called \"csv_sampling\")."
      ],
      "metadata": {
        "id": "DSxbJ9sfV_cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one text file per sampled input/output per team per language per test set\n",
        "import os\n",
        "import glob\n",
        "from pandas import *\n",
        "import codecs\n",
        "\n",
        "out_sampled_folder = '/content/d2t_outputs-sampled'\n",
        "\n",
        "# The 3 CSV files for each task have the same IDs sampled, so we can just use one file per task\n",
        "list_csv_D2T1_OIDs = read_csv('/content/csv_sampling/D2T-1-FA_samplingData.csv')['original_id'].tolist()\n",
        "list_csv_D2T2_OIDs = read_csv('/content/csv_sampling/D2T-2-FA_samplingData.csv')['original_id'].tolist()\n",
        "\n",
        "# The original IDs are numbered starting from 1, we want a number starting from 0 to aling with list indices in the system output files; make it into a dic for easy access afterwards\n",
        "list_csv_ids = {}\n",
        "list_csv_ids['D2T-1'] = [OID1-1 for OID1 in sorted(list_csv_D2T1_OIDs)]\n",
        "list_csv_ids['D2T-2'] = [OID2-1 for OID2 in sorted(list_csv_D2T2_OIDs)]\n",
        "\n",
        "for sys_output_path in glob.glob(os.path.join('/content/sys_outputs', '*.txt')):\n",
        "  head, tail = os.path.split(sys_output_path)\n",
        "  # Get parameters of every output file\n",
        "  team_ID = tail.split('-', 1)[0]\n",
        "  lang_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[1]\n",
        "  data_code_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  subtask_code_out = data_code_out.rsplit('-', 1)[0]\n",
        "  # print(tail)\n",
        "  # print(f'  {team_ID}')\n",
        "  # print(f'  {lang_out}')\n",
        "  # print(f'  {data_code_out}')\n",
        "  # print(f'  {subtask_code_out}')\n",
        "\n",
        "  # Create subfolder to store sampled system outputs\n",
        "  dest_folder_sample = os.path.join(out_sampled_folder, data_code_out, lang_out, team_ID)\n",
        "  if not os.path.exists(dest_folder_sample):\n",
        "    os.makedirs(dest_folder_sample)\n",
        "\n",
        "  # Read sys output\n",
        "  sys_output_all_lines = codecs.open(sys_output_path).readlines()\n",
        "\n",
        "  for id_sampled in list_csv_ids[subtask_code_out]:\n",
        "    # Create text files the last part of the name of which matches the name of the sampled input files\n",
        "    dest_filename_sample = os.path.join(dest_folder_sample, '['+team_ID+'_'+lang_out+']_'+data_code_out+'_'+str(id_sampled).rjust(4, \"0\")+'.txt')\n",
        "    with codecs.open(dest_filename_sample, 'w', 'utf-8') as fo:\n",
        "        fo.write(sys_output_all_lines[id_sampled].strip())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vvjhq5OkV9T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download sampled output text files\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/d2t_outputs-sampled.zip'\n",
        "!zip -r {zip_name_inter} /content/d2t_outputs-sampled\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lJ-P6XxXAMMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}