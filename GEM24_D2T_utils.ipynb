{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_D2T_StratifiedSampling/blob/main/GEM24_D2T_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - D2T data selection"
      ],
      "metadata": {
        "id": "NpKJM2gx_ELW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare repo"
      ],
      "metadata": {
        "id": "imLY7mxmOvjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install packages\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\n",
        "! pip install json2html\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DxwHcdRaLqEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hyry7e09e7P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters"
      ],
      "metadata": {
        "id": "oMZuDfUnTEA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WudT_LOiIoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Filepath definition (upload XML file(s) whose name starts with \"subtask-dataset_\" (e.g. D2T-1-FA_) in TESTDATA folder after running!)\n",
        "\n",
        "# project_dir_path = os.path.join('/', 'content', 'drive', 'MyDrive', 'WebNLG_data_selection')\n",
        "project_dir_path = '/content'\n",
        "rdf_path = os.path.join(project_dir_path, 'testdata')\n",
        "csv_path = os.path.join(project_dir_path, 'csv_sampling')\n",
        "\n",
        "subtask = 'D2T-1'#@param['D2T-1', 'D2T-2']\n",
        "dataset = 'FA'#@param['CFA', 'FA', 'FI']\n",
        "# seed used for GEM'24: 49\n",
        "seed = 49#@param\n",
        "seed = int(seed)\n",
        "datacode = subtask+'-'+dataset\n",
        "\n",
        "output_path = os.path.join(csv_path, datacode+'_samplingData.csv')\n",
        "\n",
        "if not os.path.exists(rdf_path):\n",
        "  os.makedirs(rdf_path)\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "  os.makedirs(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create csv file with sampling info. Run once for each file."
      ],
      "metadata": {
        "id": "Jo3-O0qFO0yV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOxH6Hr9kcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function for sampling.\n",
        "\n",
        "def extract_data(rdf_filepath, stratify_categories, exclude_size, project='WebNLG'):\n",
        "\n",
        "  '''\n",
        "      This method:\n",
        "      a. extracts the required entries (RDF triple(s), number of triples, property and category) from the json file.\n",
        "      b. categorizes the triple and verbalisation pair as seen/unseen category based on its presence in the training set.\n",
        "      c. groups the required extracted entry field (in this case, number of triples and property) for stratified selection.\n",
        "  '''\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "  original_id = 1\n",
        "  for filename in os.listdir(rdf_filepath):\n",
        "    if '.xml' in filename and datacode in filename:\n",
        "      tree = ET.parse(f\"{rdf_filepath}/{filename}\")\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # extract triples\n",
        "      for entry in root.findall('./entries/entry'):\n",
        "        triples = []\n",
        "        pred = []\n",
        "        for triple in entry.find('modifiedtripleset').findall('mtriple'):\n",
        "          str_triple = triple.text\n",
        "          triples.append(str_triple)\n",
        "          only_pred = str_triple.split('|')[1]\n",
        "          pred.append(only_pred)\n",
        "        if exclude_size == 'none' or (exclude_size == '1 only' and int(entry.attrib['size']) > 1) or (exclude_size == '1 and 2' and int(entry.attrib['size']) > 2):\n",
        "          curr_entry = {\n",
        "              'id': count,\n",
        "              'original_id': original_id,\n",
        "              'triples': triples.copy(),\n",
        "              'property': pred.copy(),\n",
        "              'num_triples': int(entry.attrib['size']),\n",
        "              'category': 'unseen' if entry.attrib['category'] in ['Athlete', 'Artist', 'CelestialBody', 'MeanOfTransportation', 'Politician'] else 'seen',\n",
        "              'category_all': entry.attrib['category']\n",
        "          }\n",
        "          num_to_use = 0\n",
        "          # For WebNLG and GEM, the whole size was used (between 1 and 7)\n",
        "          if project == 'WebNLG' or project == 'GEM':\n",
        "            num_to_use = curr_entry['num_triples']\n",
        "          # For long-input experiments, we only consider the tens digit\n",
        "          elif project == 'LongInput':\n",
        "            num_to_use = (str(curr_entry['num_triples'])[0] if len(str(curr_entry['num_triples'])) == 2 else 0)\n",
        "          if stratify_categories == 'seenUnseen':\n",
        "            curr_entry['strat_field'] = str(num_to_use)+curr_entry['category']\n",
        "          elif stratify_categories == 'allCategories':\n",
        "            curr_entry['strat_field'] = str(num_to_use)+curr_entry['category_all']\n",
        "          elif stratify_categories == 'ignoreCategories':\n",
        "            curr_entry['strat_field'] = str(num_to_use)\n",
        "          data.append(curr_entry)\n",
        "          count += 1\n",
        "          # print(curr_entry['strat_field'])\n",
        "        original_id += 1\n",
        "\n",
        "  # Remove data points for which there is only one member in a stratify category (triggers an error when stratifying, needs 2 members min)\n",
        "  clean_data = []\n",
        "  # Make a dico with the count of instances of each strat_field\n",
        "  count_strat_field_instances = {}\n",
        "  for datapoint in data:\n",
        "    if datapoint['strat_field'] in count_strat_field_instances:\n",
        "      count_strat_field_instances[datapoint['strat_field']] += 1\n",
        "    else:\n",
        "      count_strat_field_instances[datapoint['strat_field']] = 1\n",
        "  # If a count of a strat_field is one, do no include it in the final dataset\n",
        "  for datapoint_clean in data:\n",
        "    if count_strat_field_instances[datapoint_clean['strat_field']] == 1:\n",
        "      print(f\"  Removed datapoint  {datapoint_clean['strat_field']} because there is only one member!\")\n",
        "    else:\n",
        "      clean_data.append(datapoint_clean)\n",
        "\n",
        "  num_strat_field_values = len(count_strat_field_instances.keys())\n",
        "\n",
        "  return clean_data, count_strat_field_instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idE1moVjJL3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling parameters\n",
        "\n",
        "stratify_categories = 'ignoreCategories'#@param['allCategories', 'seenUnseen', 'ignoreCategories']\n",
        "number_samples = \"21\"#@param[14, 15, 20, 21, 50, 100, 120, 150, 175, 180, 200, 300, 400, 500]\n",
        "num_samples = int(number_samples)\n",
        "exclude_size = 'none'#@param['none', '1 only', '1 and 2']\n",
        "project = 'LongInput'#@param['WebNLG', 'GEM', 'LongInput']\n",
        "# Get data\n",
        "data, count_strat_field_instances = extract_data(rdf_path, stratify_categories, exclude_size, project)\n",
        "\n",
        "print(f'Will stratify using {len(count_strat_field_instances.keys())} categories.')\n",
        "if project == 'WebNLG' or project == 'GEM':\n",
        "  print(sorted(count_strat_field_instances.keys()))\n",
        "elif project == 'LongInput':\n",
        "  print(sorted([str(x)+'0-'+str(x)+'9' for x in count_strat_field_instances.keys()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN3pAE5N-ti-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Stratified selection using train_test_split\n",
        "\n",
        "# tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# X_train, X_test, = train_test_split(tset, test_size=num_samples, random_state=seed, stratify=tset['strat_field'], shuffle=True)\n",
        "# print(len(X_train), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Balanced selection using groupby\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# I found three ways, not sure what they exactly do; the second and third ones allow for specifying the random_state\n",
        "# X_test = tset.groupby(tset['strat_field']).apply(lambda s: s.sample(30))\n",
        "# X_test = tset.sample(frac = 1.0, random_state=seed, axis=0).groupby(tset['strat_field']).head(30)\n",
        "# The third one below seems more controlled; grouby uses axis=0\n",
        "num_samples_per_category = int(num_samples / len(count_strat_field_instances.keys()))\n",
        "X_test = tset.groupby(by=tset['strat_field']).sample(n = num_samples_per_category, random_state=seed)\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wjXcRashq9v2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print some numbers\n",
        "# tset['num_triples']\n",
        "# len(tset.loc[tset['category'] == 'unseen'])\n",
        "# print(X_test['num_triples'])\n",
        "\n",
        "# Show mean of column that contains triple number in each input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)\n",
        "# print(X_test.loc[:, 'num_triples'].mean())\n",
        "print(f\"{round(X_test['num_triples'].mean(), 2)} triples per input on average\")\n",
        "\n",
        "def count_num_instances(pd_column):\n",
        "  count = {}\n",
        "  for category in pd_column:\n",
        "    if category in count:\n",
        "      count[category] += 1\n",
        "    else:\n",
        "      count[category] = 1\n",
        "\n",
        "  for count_category in sorted(count):\n",
        "    print(f'{count_category}\\t{count[count_category]}')\n",
        "    # print(f'{count[count_category]}')\n",
        "  print('-----------------')\n",
        "\n",
        "count_num_instances(X_test['num_triples'])\n",
        "count_num_instances(X_test['category_all'])"
      ],
      "metadata": {
        "id": "wT8jGcydF28v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play around with groupby\n",
        "# df_test_gb = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
        "#                               'Parrot', 'Parrot'],\n",
        "#                    'Max Speed': [380., 370., 24., 26.]})\n",
        "# print(df_test_gb)\n",
        "\n",
        "# print(df_test_gb.groupby(['Animal']).mean())"
      ],
      "metadata": {
        "id": "ge6TruTBPHqt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoBNN96VZEQp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create CSV file\n",
        "X_test.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Create HTML tables for inputs. Run once for each file."
      ],
      "metadata": {
        "id": "HrZjTEDs_MDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from pandas dataframe (running the code above before)\n",
        "from json2html import *\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "path_out_pd = '/content/tables_pd'\n",
        "if not os.path.exists(path_out_pd):\n",
        "  os.makedirs(path_out_pd)\n",
        "\n",
        "for ind in X_test.index:\n",
        "  triple_set = X_test['triples'][ind]\n",
        "  orig_id = X_test['original_id'][ind]\n",
        "  # orig_id starts numbering at 1, while the lists of outputs texts will start numbering at 0, so we need to remove 1 from the original ID to maintain alignment with output files lines\n",
        "  with codecs.open(os.path.join(path_out_pd, f'{datacode}_{str(orig_id-1).rjust(4, \"0\")}.html'), 'w', 'utf-8') as fo:\n",
        "    list_dico_input = []\n",
        "    for triple in triple_set:\n",
        "      dico_triples = {}\n",
        "      dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "      dico_triples['Property'] = triple.split(' | ')[1]\n",
        "      dico_triples['Object'] = triple.split(' | ')[2]\n",
        "      list_dico_input.append(dico_triples)\n",
        "    fo.write(json2html.convert(json = list_dico_input))"
      ],
      "metadata": {
        "id": "T6yqW7aCRTsd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from HuggingFace data\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# from json2html import *\n",
        "# import codecs\n",
        "# import os\n",
        "# import re\n",
        "# import json\n",
        "\n",
        "# # struct2text: common_gen, cs_restaurants, dart, e2e_nlg, totto, web_nlg_en, web_nlg_ru\n",
        "# # schema_guided_dialog\n",
        "# dataset_name = 'web_nlg_en'\n",
        "# dataset = load_dataset('gem', dataset_name)\n",
        "# # subSets = ['test', 'validation', 'train']\n",
        "# subSets = ['test']\n",
        "\n",
        "# for subSet in subSets:\n",
        "#   x = 0\n",
        "#   subSet_inputs = dataset[subSet]\n",
        "#   while x < len (subSet_inputs):\n",
        "#     # fileName_out = 'out_tables/'+dataset_name+'-'+subSet+'-'+str(x)+'.html'\n",
        "#     fileName_out = 'tables_/'+subSet_inputs[x]['gem_id']+'.html'\n",
        "#     if not os.path.exists('tables_'):\n",
        "#       os.makedirs('tables_')\n",
        "#     print('Processing '+fileName_out)\n",
        "#     fo = codecs.open(fileName_out, 'w', 'utf-8')\n",
        "#     list_dico_input = []\n",
        "#     if dataset_name == 'web_nlg_en' or dataset_name == 'web_nlg_ru':\n",
        "#       for triple in subSet_inputs[x]['input']:\n",
        "#         dico_triples = {}\n",
        "#         dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "#         dico_triples['Property'] = triple.split(' | ')[1]\n",
        "#         dico_triples['Object'] = triple.split(' | ')[2]\n",
        "#         list_dico_input.append(dico_triples)\n",
        "#     elif dataset_name == 'common_gen':\n",
        "#       dico_concepts = {}\n",
        "#       dico_concepts['Concepts'] = subSet_inputs[x]['concepts']\n",
        "#       list_dico_input.append(dico_concepts)\n",
        "#     elif dataset_name == 'cs_restaurants':\n",
        "#       dico_DAs = {}\n",
        "#       DA = subSet_inputs[x]['dialog_act'].split('(')[0]\n",
        "#       triples = subSet_inputs[x]['dialog_act'].split('(')[1].split(')')[0]\n",
        "#       dico_DAs['Dialogue Act'] = DA\n",
        "#       if re.search(',', triples):\n",
        "#         dico_DAs['Topics'] = triples.split(',')\n",
        "#       else:\n",
        "#         dico_DAs['Topic'] = triples\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     elif dataset_name == 'e2e_nlg':\n",
        "#       list_properties = subSet_inputs[x]['meaning_representation'].split(', ')\n",
        "#       for input_property in list_properties:\n",
        "#         dico_properties = {}\n",
        "#         prop_name = input_property.split('[')[0]\n",
        "#         prop_value = input_property.split('[')[1].split(']')[0]\n",
        "#         dico_properties['Property'] = prop_name\n",
        "#         dico_properties['Value'] = prop_value\n",
        "#         list_dico_input.append(dico_properties)\n",
        "#     elif dataset_name == 'schema_guided_dialog':\n",
        "#       dico_DAs = {}\n",
        "#       dico_DAs['Dialogue Acts'] = subSet_inputs[x]['dialog_acts']\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     fo.write(json2html.convert(json = list_dico_input))\n",
        "#     fo.close()\n",
        "#     x += 1"
      ],
      "metadata": {
        "id": "b25TnJS9_gWU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ],
      "metadata": {
        "id": "6nYTNyqSQ2X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download tables\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/html_tables.zip'\n",
        "!zip -r {zip_name_inter} /content/tables_pd\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "pRSvuVPY4iKG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download CSVs\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/CSVs.zip'\n",
        "!zip -r {zip_name_inter} /content/csv_sampling\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "JNnuk92B41tu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check the contents of the created CSVs\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob(os.path.join(csv_path, '*.csv'))\n",
        "\n",
        "dico_properties = {}\n",
        "\n",
        "for csv_file in sorted(csv_files):\n",
        "  props_unique = []\n",
        "  count_props_all = 0\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  # print(tail)\n",
        "  df_triple_sets = pd.read_csv(csv_file)['triples']\n",
        "  # print(df_triple_sets)\n",
        "  for triple_set in df_triple_sets:\n",
        "    triples_list = triple_set.replace('\"', \"'\").replace(\"['\", \"\").replace(\"']\", \"\").split(\"', '\")\n",
        "    properties_list = [triple.split(' | ')[1] for triple in triples_list]\n",
        "    for triple_property in properties_list:\n",
        "      count_props_all += 1\n",
        "      if triple_property not in props_unique:\n",
        "        props_unique.append(triple_property)\n",
        "  dico_properties[tail] = props_unique\n",
        "  # print(f'  {count_props_all} properties found (810 expected).')\n",
        "\n",
        "for dataset in dico_properties:\n",
        "  print(dataset, ':', len(dico_properties[dataset]), 'different properties found:')\n",
        "  print('  ', sorted(dico_properties[dataset]))"
      ],
      "metadata": {
        "id": "fKc0BNTSNX-Q",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Create individual files for sampled system outputs and format for LLM eval\n",
        "\n",
        "First upload all system outputs in a folder named \"sys_outputs\", and generate the corresponding csv file(s) with the code above (or upload manually in a folder called \"csv_sampling\")."
      ],
      "metadata": {
        "id": "DSxbJ9sfV_cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Map JSON format to one text per line format\n",
        "# Used to normalise the format of some raw LLM outputs\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "zip_file_path = '/content/LLMs_out_raw.zip'\n",
        "out_folder = '/content/normalised_outputs'\n",
        "download_outfolder = True\n",
        "\n",
        "if not os.path.exists(out_folder):\n",
        "  os.makedirs(out_folder)\n",
        "\n",
        "if os.path.exists(zip_file_path):\n",
        "  if not os.path.exists('/content/LLMs'):\n",
        "    ! unzip {zip_file_path}\n",
        "  for sys_id, sys_texts_path in enumerate(glob.glob(os.path.join('/content/LLMs', '*.json'))):\n",
        "    head, tail = os.path.split(sys_texts_path)\n",
        "    language = tail.rsplit('_', 1)[1].rsplit('.', 1)[0].lower()\n",
        "    filename = ''\n",
        "    if re.search('-', tail):\n",
        "      filename = str(sys_id+1)+'-'+tail.split('_', 1)[1].split('-', 1)[0]+'_D2T-1-FA_'+language+'.txt'\n",
        "    else:\n",
        "      filename = str(sys_id+1)+'-'+tail.split('_', 1)[1].split('_', 1)[0]+'_D2T-1-FA_'+language+'.txt'\n",
        "    with codecs.open(os.path.join(out_folder, filename), 'w', 'utf-8') as fo:\n",
        "      # print(tail)\n",
        "      data_llm = None\n",
        "      with codecs.open(sys_texts_path, 'r', 'utf-8') as finput:\n",
        "        data_llm = json.load(finput)\n",
        "      for count, datapoint in enumerate(data_llm):\n",
        "        # Remove the linebreaks in the outputs\n",
        "        single_line_output = datapoint['output'].replace('\\n', ' ')\n",
        "        while re.search('  ', single_line_output):\n",
        "          single_line_output = single_line_output.replace('  ', ' ')\n",
        "        # print(f\"{count} - {single_line_output}\")\n",
        "        fo.write(single_line_output+'\\n')\n",
        "\n",
        "  if download_outfolder:\n",
        "    from google.colab import files\n",
        "    zip_name_inter = '/content/LLMs_normalised_outputs.zip'\n",
        "    !zip -r {zip_name_inter} /content/normalised_outputs\n",
        "    files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "96HfxobaFbAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one text file per sampled input/output per team per language per test set\n",
        "import os\n",
        "import glob\n",
        "from pandas import *\n",
        "import codecs\n",
        "\n",
        "out_sampled_folder = '/content/d2t_outputs-sampled'\n",
        "\n",
        "# The 3 CSV files for each task have the same IDs sampled, so we can just use one file per task\n",
        "list_csv_D2T1_OIDs = None\n",
        "list_csv_D2T2_OIDs = None\n",
        "# The original IDs are numbered starting from 1, we want a number starting from 0 to aling with list indices in the system output files; make it into a dic for easy access afterwards\n",
        "list_csv_ids = {}\n",
        "if os.path.exists('/content/csv_sampling/D2T-1-FA_samplingData.csv'):\n",
        "  list_csv_D2T1_OIDs = read_csv('/content/csv_sampling/D2T-1-FA_samplingData.csv')['original_id'].tolist()\n",
        "  list_csv_ids['D2T-1'] = [OID1-1 for OID1 in sorted(list_csv_D2T1_OIDs)]\n",
        "if os.path.exists('/content/csv_sampling/D2T-2-FA_samplingData.csv'):\n",
        "  list_csv_D2T2_OIDs = read_csv('/content/csv_sampling/D2T-2-FA_samplingData.csv')['original_id'].tolist()\n",
        "  list_csv_ids['D2T-2'] = [OID2-1 for OID2 in sorted(list_csv_D2T2_OIDs)]\n",
        "\n",
        "\n",
        "for sys_output_path in glob.glob(os.path.join('/content/sys_outputs', '*.txt')):\n",
        "  head, tail = os.path.split(sys_output_path)\n",
        "  # Get parameters of every output file\n",
        "  team_ID = tail.split('-', 1)[0]\n",
        "  lang_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[1]\n",
        "  data_code_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  subtask_code_out = data_code_out.rsplit('-', 1)[0]\n",
        "  # print(tail)\n",
        "  # print(f'  {team_ID}')\n",
        "  # print(f'  {lang_out}')\n",
        "  # print(f'  {data_code_out}')\n",
        "  # print(f'  {subtask_code_out}')\n",
        "\n",
        "  # Create subfolder to store sampled system outputs\n",
        "  dest_folder_sample = os.path.join(out_sampled_folder, data_code_out, lang_out, team_ID)\n",
        "  if not os.path.exists(dest_folder_sample):\n",
        "    os.makedirs(dest_folder_sample)\n",
        "\n",
        "  # Read sys output\n",
        "  sys_output_all_lines = codecs.open(sys_output_path).readlines()\n",
        "\n",
        "  for id_sampled in list_csv_ids[subtask_code_out]:\n",
        "    # Create text files the last part of the name of which matches the name of the sampled input files\n",
        "    dest_filename_sample = os.path.join(dest_folder_sample, '['+team_ID+'_'+lang_out+']_'+data_code_out+'_'+str(id_sampled).rjust(4, \"0\")+'.txt')\n",
        "    with codecs.open(dest_filename_sample, 'w', 'utf-8') as fo:\n",
        "        fo.write(sys_output_all_lines[id_sampled].strip())\n"
      ],
      "metadata": {
        "id": "vvjhq5OkV9T4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download sampled output text files\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/d2t_outputs-sampled.zip'\n",
        "!zip -r {zip_name_inter} /content/d2t_outputs-sampled\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lJ-P6XxXAMMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Group output files in json files in the same format as the files packaged for human evaluation\n",
        "\n",
        "# None will create one single file per language, while specifying a number will split the files to satisfy the max number indicated\n",
        "len_package = None\n",
        "outfolder = '/content/json4evalHumLLM'\n",
        "outfileName = 'longInputD2T_LLMtexts'\n",
        "\n",
        "path_zip_sampled_outputs = '/content/d2t_outputs-sampled.zip'\n",
        "path_html_tables = '/content/html_tables.zip'\n",
        "\n",
        "if os.path.exists(path_zip_sampled_outputs) and os.path.exists(path_html_tables):\n",
        "  if not os.path.exists(outfolder):\n",
        "    os.makedirs(outfolder)\n",
        "\n",
        "  # Extract zip files\n",
        "  if not os.path.exists('/content/content/d2t_outputs-sampled'):\n",
        "    !unzip -q $path_zip_sampled_outputs\n",
        "  if not os.path.exists('/content/content/tables_pd'):\n",
        "    !unzip -q $path_html_tables\n",
        "\n",
        "  # Make dico with HTML inputs where keys are input ID (e.g. D2T-1-FA_0015)\n",
        "  dico_html_inputs = {}\n",
        "  for HTML_table in glob.glob('/content/content/tables_pd/*.html'):\n",
        "    filename = os.path.basename(HTML_table).rsplit('.', 1)[0]\n",
        "    # print(filename)\n",
        "    with codecs.open(HTML_table, 'r', 'utf-8') as f:\n",
        "      html_content = f.read()\n",
        "      # print((html_content))\n",
        "      dico_html_inputs[filename] = html_content\n",
        "  # print(dico_html_inputs)\n",
        "\n",
        "  # Make dico with all systems outputs and their input\n",
        "    # {\n",
        "    #   \"id\": \"sw_D2T-2-FA_0362_2\",\n",
        "    #   \"input\": \"<table border=\\\"1\\\"><thead><tr><th>Subject</th><th>Property</th><th>Object</th></tr></thead><tbody><tr><td>Samir_Nasri</td><td>ReligionOrWorldview</td><td>Islam</td></tr><tr><td>Samir_Nasri</td><td>DateOfBirth</td><td>1987-06-26</td></tr><tr><td>Samir_Nasri</td><td>ParticipantIn</td><td>UEFA_Euro_2008</td></tr></tbody></table>\",\n",
        "    #   \"output\": \"Samir Nasri, aliyezaliwa tarehe 26 Juni 1987, ni Muislamu aliyechezea UEFA Euro 2008.\"\n",
        "    # },\n",
        "\n",
        "  # Lists and dico to count how many different datasets, languages, systems, inputs we have, to check the numbers at the end\n",
        "  datasets_list = []\n",
        "  # There can be different systems for different languages, or systems that have outputs for several languages\n",
        "  # So we need to store how many different systems we have for each language\n",
        "  languages_systems_dico = {}\n",
        "  inputs_list = []\n",
        "\n",
        "  dico_datapoints = {}\n",
        "  for dataset_path in glob.glob('/content/content/d2t_outputs-sampled/*'):\n",
        "    datset_name = dataset_path.rsplit('/', 1)[1]\n",
        "    if datset_name not in datasets_list:\n",
        "      datasets_list.append(datset_name)\n",
        "    for language_path in sorted(glob.glob(dataset_path + '/*')):\n",
        "      language_name = language_path.rsplit('/', 1)[1]\n",
        "      if language_name not in languages_systems_dico.keys():\n",
        "        languages_systems_dico[language_name] = []\n",
        "      if language_name not in dico_datapoints.keys():\n",
        "        dico_datapoints[language_name] = []\n",
        "      for system_path in sorted(glob.glob(language_path + '/*')):\n",
        "        system_name = system_path.rsplit('/', 1)[1]\n",
        "        if system_name not in languages_systems_dico[language_name]:\n",
        "          languages_systems_dico[language_name].append(system_name)\n",
        "        for output_text_file in sorted(glob.glob(system_path + '/*')):\n",
        "          filename = os.path.basename(output_text_file).rsplit('.', 1)[0]\n",
        "          language_fromfilename = filename.split(']', 1)[0].split('[', 1)[1].split('_', 1)[1]\n",
        "          assert language_fromfilename == language_name\n",
        "          system_fromfilename = filename.split(']', 1)[0].split('[', 1)[1].split('_', 1)[0]\n",
        "          assert system_fromfilename == system_name\n",
        "          input_id = filename.split(']_', 1)[1].rsplit('.', 1)[0]\n",
        "          output_text = codecs.open(output_text_file, 'r', 'utf-8').read().strip()\n",
        "          input_html = dico_html_inputs[input_id]\n",
        "          if input_id not in inputs_list:\n",
        "            inputs_list.append(input_id)\n",
        "          datapoint = {\n",
        "              \"id\": f\"{language_name}_{input_id}_{system_name}\",\n",
        "              \"input\": input_html,\n",
        "              \"output\": output_text\n",
        "          }\n",
        "          dico_datapoints[language_name].append(datapoint)\n",
        "\n",
        "  # print(f'Found {len(datasets_list)} datasets, {len(languages_list)} languages, {len(systems_list)} systems, {len(inputs_list)} input files.')\n",
        "  # print(list_datapoints)\n",
        "  assert sum(len(w) for w in dico_datapoints.values()) == len(datasets_list) * sum(len(v) for v in languages_systems_dico.values()) * len(inputs_list), f'Expected {len(datasets_list) * sum(len(v) for v in languages_systems_dico.values()) * len(systems_list) * len(inputs_list)} datapoints, found {sum(len(w) for w in dico_datapoints.values())}.'\n",
        "  print(f'Found {sum(len(w) for w in dico_datapoints.values())} datapoints (looks good).')\n",
        "\n",
        "  for language_key in dico_datapoints:\n",
        "    # Shuffle list of datapoints\n",
        "    shuffled_list_datapoints = random.sample(dico_datapoints[language_key], len(dico_datapoints[language_key]))\n",
        "    if len_package == None:\n",
        "      with codecs.open(os.path.join(outfolder, f'{language_key}_{outfileName}.json'), 'w', 'utf-8') as f:\n",
        "        json.dump(shuffled_list_datapoints, f, indent=4)\n",
        "        print(f'Created {language_key} json in {outfolder}')\n",
        "    else:\n",
        "      # Make packages of a given size\n",
        "      packages = [shuffled_list_datapoints[i:i+len_package] for i in range(0, len(shuffled_list_datapoints), len_package)]\n",
        "      # Save list packages as JSON\n",
        "      for i, package in enumerate(packages):\n",
        "        path_outfile = os.path.join(outfolder, f'{language_key}_{outfileName}_{i}.json')\n",
        "        with open(path_outfile, 'w') as f:\n",
        "          json.dump(package, f, indent=4)\n",
        "          print(f'Created {language_key}-{i} json in {outfolder}')\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "tD_EQKYTiLT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Create individual files for reference texts collected on AMT"
      ],
      "metadata": {
        "id": "6J6QSDw8EkD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read json and check texts (UPLOAD FIRST)\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# First upload '/content/en_references_v2-formatted.json' and '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Load json\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "# Unzip sampled output folder, which contains lists of IDs of sampled outputs\n",
        "if not os.path.exists('/content/content/d2t_outputs-sampled_grouped'):\n",
        "  ! unzip '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Get list of IDs for all sampled outputs\n",
        "list_IDs_sampled = []\n",
        "for dataset_path in glob.glob(os.path.join('/content/content/d2t_outputs-sampled_grouped', '*')):\n",
        "  head, tail = os.path.split(dataset_path)\n",
        "  # print(tail)\n",
        "  path_IDs = os.path.join(dataset_path, 'en', 'AllSizes', 'AllSizes_sampled_ids.txt')\n",
        "  # Open IDs file, read, and split the list on the first line\n",
        "  datasets_IDs_sampled = codecs.open(path_IDs, 'r', 'utf-8').readlines()[0].strip().replace('[', '').replace(']', '').replace(\"'\", '').split(', ')\n",
        "  for dataset_ID_sampled in datasets_IDs_sampled:\n",
        "    full_ID = f'en_{tail}_{dataset_ID_sampled}'\n",
        "    list_IDs_sampled.append(full_ID)\n",
        "# print(len(list_IDs_sampled))\n",
        "# print(list_IDs_sampled)\n",
        "\n",
        "ids_error = []\n",
        "all_ids_collected = []\n",
        "# Find some errors\n",
        "with codecs.open('/content/writing_problems.txt', 'w', 'utf-8') as fo1:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    all_ids_collected.append(id)\n",
        "    # Capture people who pasted some definitions\n",
        "    if re.search('\\[', text):\n",
        "      fo1.write(f'{id} (pasted definition): {text}\\n')\n",
        "      print(f'{id} (pasted definition): {text}')\n",
        "      ids_error.append(id)\n",
        "    # Capture people who pasted the input\n",
        "    elif re.search('Subject', text):\n",
        "      fo1.write(f'{id} (pasted input): {text}\\n')\n",
        "      print(f'{id} (pasted input): {text}')\n",
        "      ids_error.append(id)\n",
        "\n",
        "  # Find missing files\n",
        "  for ID_sampled in list_IDs_sampled:\n",
        "    if ID_sampled not in all_ids_collected:\n",
        "      fo1.write(f'{ID_sampled} (missing)\\n')\n",
        "      print(f'{ID_sampled} (missing)')\n",
        "\n",
        "if len(ids_error) == 0:\n",
        "  print('No errors found!')\n",
        "\n",
        "ids_seen = []\n",
        "texts_seen = []\n",
        "with codecs.open('/content/writing_duplicates.txt', 'w', 'utf-8') as fo2:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    # Check if there is a duplicate for a text with error\n",
        "    if id in ids_error:\n",
        "      print(f'Found alternative to error {id}')\n",
        "    # Capture duplicate outputs\n",
        "    if id not in ids_seen:\n",
        "      ids_seen.append(id)\n",
        "      texts_seen.append(text)\n",
        "    else:\n",
        "      index_id = ids_seen.index(id)\n",
        "      fo2.write(f'{id} (duplicate): {texts_seen[index_id]}\\n')\n",
        "      # print(f'{id} (duplicate): {texts_seen[index_id]}')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kmsoaZqBEvZP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create files\n",
        "import json\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "# First upload '/content/en_references-formatted.json'\n",
        "\n",
        "# Create folders for text files\n",
        "path_collected_references = os.path.join('/content', 'texts_AMT')\n",
        "if not os.path.exists(path_collected_references):\n",
        "  os.makedirs(path_collected_references)\n",
        "\n",
        "# Load json\n",
        "ids_seen = []\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "for reference_text in references_json[\"references\"]:\n",
        "  id = reference_text[\"short_id\"]\n",
        "  # We only take the first text available\n",
        "  if id not in ids_seen:\n",
        "    ids_seen.append(id)\n",
        "    language = id.split('_', 1)[0]\n",
        "    id_nolang = id.split('_', 1)[1]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    filename = os.path.join(path_collected_references, f'[0_{language}]_{id_nolang}.txt')\n",
        "    with codecs.open(filename, 'w', 'utf-8') as fo:\n",
        "      fo.write(text)\n",
        "print(f'{str(len(ids_seen))} files were created (1,080 expected.)')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MYarTMTqapUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download texts\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "zip_name_inter = '/content/texts_AMT.zip'\n",
        "!zip -r {zip_name_inter} /content/texts_AMT\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XdJpx2r0foaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Create files for automatic evaluation by size (sizes 2 to 7 and all sizes together)"
      ],
      "metadata": {
        "id": "eBhJJ9MYeRFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Group references and system outputs by size\n",
        "from posix import write\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import shutil\n",
        "\n",
        "language_out = 'en' #['en', 'es', 'sw']\n",
        "teams_withdrawn = ['8']\n",
        "# First upload '/content/csv_sampling/d2t_inputs-sampled.zip' and /content/csv_sampling/d2t_outputs-sampled.zip\n",
        "\n",
        "folder_suffix = '-enAmtRefs' #param['-onlyWebnlgRef', '-enAmtRefs', '-enAmtRefs_secondary']\n",
        "outputs_sampled_folder= '/content/csv_sampling/d2t_outputs-sampled'+folder_suffix+'.zip'\n",
        "inputs_sampled_folder= '/content/csv_sampling/d2t_inputs-sampled.zip'\n",
        "\n",
        "# Unzip sampled input and output files if they have not been unzipped already:\n",
        "if not os.path.exists('/content/d2t_inputs-sampled'):\n",
        "  ! unzip {outputs_sampled_folder}\n",
        "  ! unzip {inputs_sampled_folder}\n",
        "\n",
        "# Delete /content/d2t_outputs-sampled_grouped folder\n",
        "if os.path.exists('/content/d2t_outputs-sampled'+folder_suffix+'_grouped'):\n",
        "  shutil.rmtree('/content/d2t_outputs-sampled'+folder_suffix+'_grouped')\n",
        "\n",
        "def writeFile(file_to_write, content, total_num_lines, current_num_lines):\n",
        "    file_to_write.write(content)\n",
        "    # After all lines except the last one, insert linebreak\n",
        "    if current_num_lines < total_num_lines-1:\n",
        "      file_to_write.write('\\n')\n",
        "\n",
        "# Create new output folders for storing files with grouped outputs for each team\n",
        "subfolders_out = sorted([os.path.join(f.path, language_out) for f in os.scandir('/content/d2t_outputs-sampled'+folder_suffix+'/d2t_outputs-sampled') if f.is_dir()])\n",
        "# print(subfolders_out)\n",
        "subfolders_out_grouped = []\n",
        "for subfolder_out in subfolders_out:\n",
        "  # '/content/d2t_outputs-sampled-onlyWebnlgRef/d2t_outputs-sampled/D2T-1-CFA/en'\n",
        "  new_folder_name = re.sub('d2t_outputs-sampled'+folder_suffix, 'd2t_outputs-sampled'+folder_suffix+'_grouped', subfolder_out)\n",
        "  subfolders_out_grouped.append(new_folder_name)\n",
        "  if not os.path.exists(new_folder_name):\n",
        "    os.makedirs(new_folder_name)\n",
        "\n",
        "# Read files with sampling info (we only need one file per subtask since all IDs are the same for each subtask)\n",
        "csv_files = glob.glob(os.path.join('/content/d2t_inputs-sampled/info_sampling-csv', '*-FA_samplingData.csv'))\n",
        "\n",
        "# Build a dico with the IDs grouped by size\n",
        "# {'D2T-1': {2: [46, 920, 936, 1423, 1723, 410, 1653, 1589, 762, 1471, 590, 1685, 1217, 1355, 99, 1339, 1079, 1420, 276, 771, 757, 845, 472, 1222, 1072, 582, 1043, 360, 1387, 1113], ...}}\n",
        "dico_ids_per_size = {}\n",
        "for csv_file in sorted(csv_files):\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  task = tail.rsplit('-', 1)[0]\n",
        "  df_triple_sets = pd.read_csv(csv_file)\n",
        "  dico_ids_per_size[task] = {}\n",
        "  for n in range(2, 8):\n",
        "    dico_ids_per_size[task][n] = [df_triple_sets.loc[df_triple_sets['num_triples'] == n, 'original_id'].tolist()][0]\n",
        "# print(dico_ids_per_size)\n",
        "# print(subfolders_out)\n",
        "# print(subfolders_out_grouped)\n",
        "\n",
        "# subfolder_out is something like /content/d2t_outputs-sampled/D2T-1-CFA/en\n",
        "for subfolder_out, subfolder_out_grouped in zip(subfolders_out, subfolders_out_grouped):\n",
        "  list_sampled_IDs = []\n",
        "  # Get folders inside subfolder_out\n",
        "  teams_out_folders = sorted([f.path for f in os.scandir(subfolder_out) if f.is_dir() and f.name not in teams_withdrawn])\n",
        "  # print(teams_out_folders)\n",
        "  # For each team out folder, check if the path contains one of the keys in dico_ids_per_size\n",
        "  # team_out_folder is something like /content/d2t_outputs-sampled/D2T-1-CFA/en/1\n",
        "  for team_out_folder in teams_out_folders:\n",
        "    num_of_IDs_total = 0\n",
        "    head_tf, team_id = os.path.split(team_out_folder)\n",
        "    print(team_out_folder)\n",
        "    # There are 2 subtasks in dico_ids_per_size: D2T-1 and D2T-2\n",
        "    for task in dico_ids_per_size.keys():\n",
        "      # Make sure we are in the output folder of the right subtask\n",
        "      if task in team_out_folder:\n",
        "        # print(f'  {task} found in {team_out_folder}')\n",
        "        # Within each subtask, we want to create separate files for each input size\n",
        "        for size in dico_ids_per_size[task]:\n",
        "          # print(f'  NumOfIDs: {num_of_IDs_total}')\n",
        "          print(f'    {size}: {dico_ids_per_size[task][size]}')\n",
        "          # Create a folder for each size (to run the eval later on with my notebook, I need to process all hypotheses for one (set of) reference file(s)).\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'Size'+str(size))):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'Size'+str(size)))\n",
        "          # Create a folder where we'll group all the sizes for each system in one file\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'AllSizes')):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'AllSizes'))\n",
        "\n",
        "          # Get a padded version of all IDs in dico_ids_per_size[task][size]\n",
        "          for num_of_IDs_for_a_size, id in enumerate(dico_ids_per_size[task][size]):\n",
        "            # We need to subtract 1 to the original ID to match the ID of the files, for which the numbering starts from 0\n",
        "            padded_aligned_id = str(id-1).zfill(4)\n",
        "            # Now iterate over all the output files of the current folder\n",
        "            for team_out_file_path in glob.glob(os.path.join(team_out_folder, '*.txt')):\n",
        "              head_tf, tail_tf = os.path.split(team_out_file_path)\n",
        "              # Look for files that match one of the IDs of texts of a specific size\n",
        "              if padded_aligned_id in tail_tf:\n",
        "                with codecs.open(team_out_file_path, 'r', 'utf-8') as ftext:\n",
        "                  contents_read = ftext.read()\n",
        "                  # Write separate files for all sizes\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), str(team_id)+'_Size'+str(size)+'.txt'), 'a', 'utf-8') as f_sep:\n",
        "                    writeFile(f_sep, contents_read, len(dico_ids_per_size[task][size]), num_of_IDs_for_a_size)\n",
        "                  # Write one file with all sizes grouped\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', str(team_id)+'_AllSizes.txt'), 'a', 'utf-8') as f_all:\n",
        "                    writeFile(f_all, contents_read, len(dico_ids_per_size[task][size])*len(dico_ids_per_size[task]), num_of_IDs_total)\n",
        "            num_of_IDs_total += 1\n",
        "\n",
        "          # Now gather the IDs of the inputs that will match the filenames for each dataset and size\n",
        "          padded_aligned_id_list = [str(id-1).zfill(4) for id in dico_ids_per_size[task][size]]\n",
        "          with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), 'Size'+str(size)+'_sampled_ids.txt'), 'w', 'utf-8') as f_id_sep:\n",
        "            f_id_sep.write(str(sorted(padded_aligned_id_list)))\n",
        "          # Add all the IDs into a list for all sizes\n",
        "          for padded_aligned_id2 in padded_aligned_id_list:\n",
        "            if padded_aligned_id2 not in list_sampled_IDs:\n",
        "              list_sampled_IDs.append(padded_aligned_id2)\n",
        "  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', 'AllSizes_sampled_ids.txt'), 'a', 'utf-8') as f_id_all:\n",
        "    f_id_all.write(str(sorted(list_sampled_IDs)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EC7QaRtTC2qS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download files\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "folder_to_zip = '/content/d2t_outputs-sampled'+folder_suffix+'_grouped'\n",
        "zip_name_inter = folder_to_zip+'.zip'\n",
        "!zip -r {zip_name_inter} {folder_to_zip}\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IMrnK9tDjwci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - Create plots from automatic evaluation logs"
      ],
      "metadata": {
        "id": "VB_Lq9pFfRgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots per size\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "# First upload the log_eval.zip file (for 1 dataset) obtained with this notebook: https://github.com/mille-s/WebNLG-2020_Metrics\n",
        "\n",
        "dataset = 'D2T-1-CFA' #@param['D2T-1-CFA', 'D2T-1-FA', 'D2T-1-FI', 'D2T-2-CFA', 'D2T-2-FA', 'D2T-2-FI']\n",
        "# Unzip log_eval folder\n",
        "# For when we zip the folders inside the content folder\n",
        "path_logEval = '/content/content/log_eval'\n",
        "path_zip = '/content/log_eval_'+dataset+'.zip'\n",
        "if os.path.exists(path_logEval):\n",
        "  shutil.rmtree(path_logEval)\n",
        "  ! unzip {path_zip}\n",
        "else:\n",
        "  ! unzip {path_zip}\n",
        "\n",
        "# dataset = 'D2T-1-FA'#@param['D2T-1-CFA', 'D2T-1-FA', 'D2T-1-FI', 'D2T-2-CFA', 'D2T-2-FA', 'D2T-2-FI']\n",
        "metrics = ['BLEU', 'METEOR', 'chrF++', 'BERT-SCORE F1']\n",
        "system_IDnames_dico = {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST', '8':'DCU-NLG-Small-noT5', '9':'DCU-NLG-PBN-2nd'}\n",
        "\n",
        "# dico_plot_BLEU = {}\n",
        "# dico_plot_METEOR = {}\n",
        "# dico_plot_chrF = {}\n",
        "# dico_plot_BERT = {}\n",
        "dico_plot_per_size = {}\n",
        "dico_plot_all_sizes = {}\n",
        "eval_files_paths = glob.glob(os.path.join(path_logEval, '*.txt'))\n",
        "for eval_file_path in sorted(eval_files_paths):\n",
        "  head, tail = os.path.split(eval_file_path)\n",
        "  # values for \"size\": 2 to 7, and \"s\" for all sizes\n",
        "  size = tail.rsplit('_', 1)[1].rsplit('.', 1)[0].split('Size')[1]\n",
        "  sys_id = tail.rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  dataset = tail.rsplit('_', 3)[0].rsplit('_', 1)[1]\n",
        "  # print(f'{sys_id}  {size}')\n",
        "\n",
        "  # Make a key in dico_plot with a new system id\n",
        "  # if sys_id not in dico_plot_BLEU:\n",
        "  #   dico_plot_BLEU[sys_id] = []\n",
        "  # if sys_id not in dico_plot_METEOR:\n",
        "  #   dico_plot_METEOR[sys_id] = []\n",
        "  # if sys_id not in dico_plot_chrF:\n",
        "  #   dico_plot_chrF[sys_id] = []\n",
        "  # if sys_id not in dico_plot_BERT:\n",
        "  #   dico_plot_BERT[sys_id] = []\n",
        "\n",
        "  # The scores are on the penultimate line of the log file, the metrics names 2 lines above\n",
        "  log_lines_metrics = [line.rstrip('\\n') for line in open(eval_file_path)][-3]\n",
        "  log_lines_scores = [line.rstrip('\\n') for line in open(eval_file_path)][-1]\n",
        "  # Split lines with names and scores\n",
        "  # Metrics are separated by 4 spaces, with 2 initial spaces; some metrics have a space inside the name\n",
        "  metrics_list = [x for x in log_lines_metrics.split('  ') if not x == '']\n",
        "  # Scores are separated by an unknown number of spaces that depends on the metric name (to show metrics and scores aligned)\n",
        "  scores_list = [x for x in log_lines_scores.split(' ') if not x == '']\n",
        "  # Get the pairs metric/score for the metrics we are interested in\n",
        "  for metric_name, score in zip(metrics_list, scores_list):\n",
        "    if metric_name in metrics:\n",
        "      # print(f'{metric_name}: {score}')\n",
        "      if not size == 's':\n",
        "        if metric_name not in dico_plot_per_size:\n",
        "          dico_plot_per_size[metric_name] = {}\n",
        "        if sys_id not in dico_plot_per_size[metric_name]:\n",
        "          dico_plot_per_size[metric_name][sys_id] = []\n",
        "        dico_plot_per_size[metric_name][sys_id].append(float(score))\n",
        "      else:\n",
        "        # print(f'{sys_id} AllSizes')\n",
        "        # print(f'{metric_name}: {score}')\n",
        "        if sys_id not in dico_plot_all_sizes:\n",
        "          dico_plot_all_sizes[sys_id] = []\n",
        "        dico_plot_all_sizes[sys_id].append(float(score))\n",
        "  # print(metrics_list)\n",
        "  # print(scores_list)\n",
        "\n",
        "\n",
        "# Print the lines to put in a latex table\n",
        "print(f'\\nResults all sizes (order: BLEU, METEOR, chrF++, BERT-SCORE F1):')\n",
        "print(dico_plot_all_sizes)\n",
        "for sys_id in dico_plot_all_sizes:\n",
        "  print(system_IDnames_dico[sys_id]+' & '+' & '.join([str(x) for x in dico_plot_all_sizes[sys_id]])+' \\\\\\\\')\n",
        "\n",
        "print(f'\\nResults per size:')\n",
        "print(dico_plot_per_size)\n",
        "\n",
        "system_colors = {'1':'dodgerblue', '2':'blue', '3':'orange', '4':'green', '5':'red', '6':'purple', '7':'brown', '8':'purple', '9':'purple'}\n",
        "# Make and save plots in png\n",
        "for metric_name in dico_plot_per_size:\n",
        "  for sys_id in dico_plot_per_size[metric_name]:\n",
        "    plt.plot(range(2, len(dico_plot_per_size[metric_name][sys_id]) + 2), dico_plot_per_size[metric_name][sys_id], label=system_IDnames_dico[sys_id], color=system_colors[sys_id])\n",
        "  plt.title(f'{dataset}: {metric_name}')\n",
        "  plt.xlabel(\"Input size\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  plt.legend()\n",
        "  #https://blog.timodenk.com/exporting-matplotlib-plots-to-latex/\n",
        "  # I can't generate the plots inside latex\n",
        "  # plt.savefig(f'{dataset}_{metric_name}.pgf')\n",
        "  # Save the plot as a png file\n",
        "  plt.savefig(f'{dataset}_{metric_name}.png')\n",
        "  # plt.show needs to be after savefig, otherwise the image is blank\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GUt1BfdTfeum",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots for all sizes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics order in dico_plot_all_sizes: BLEU, METEOR, chrF++, BERT-SCORE F1\n",
        "metrics_order = ['BLEU', 'METEOR', 'chrF++', 'BERT']\n",
        "metrics_lower_limit = [0, 0.2, 0.4, 0.9]\n",
        "metrics_upper_limit = [40, 0.4, 0.6, 0.95]\n",
        "system_IDShortnames_dico = {'2':'DCU-PBN', '3':'DCU-Sm', '4':'DipInfo', '5':'OSU', '6':'pyrealb', '7':'Saar', '8':'DCU-Sm-noT5', '9':'DCU-PBN-2nd'}\n",
        "teams = [system_IDShortnames_dico[x] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "\n",
        "for metric_id in range(4):\n",
        "  fig, ax = plt.subplots()\n",
        "  bar_labels = teams\n",
        "  bar_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']\n",
        "  scores = [dico_plot_all_sizes[x][metric_id] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "  ax.bar(teams, scores, label=bar_labels, color=bar_colors)\n",
        "  plt.ylim(metrics_lower_limit[metric_id], metrics_upper_limit[metric_id])\n",
        "  ax.set_ylabel('Score')\n",
        "  ax.set_title(metrics_order[metric_id]+'-'+dataset)\n",
        "  # ax.legend(title='Teams')\n",
        "  plt.savefig(f'{dataset}_{metrics_order[metric_id]}_allSys.png')\n",
        "  plt.show()\n",
        "\n",
        "  # Also create a Latex table for the paper\n",
        "\n"
      ],
      "metadata": {
        "id": "4BARvDcHZPr1",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate D2T score differences for each system on the different datasets\n",
        "\n",
        "system_IDShortnames = ['DCU-PBN', 'DCU-Sm', 'DipInfo', 'OSU', 'pyrealb', 'Saar']\n",
        "#BLEU (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bleuScores = [[29.08, 25.2, 26.02, 23.96, 30.34, 20.46], [27.0, 22.98, 20.85, 19.48, 24.9, 16.88], [32.31, 29.01, 28.24, 27.22, 32.01, 21.26], [30.03, 24.45, 21.44, 24.97, 27.06, 16.9], [26.37, 21.67, 21.97, 19.97, 25.05, 16.28], [29.7, 23.48, 20.76, 28.25, 26.47, 20.16]]\n",
        "#METEOR (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "meteorScores = [[0.33, 0.297, 0.322, 0.295, 0.348, 0.3], [0.314, 0.279, 0.292, 0.26, 0.3, 0.267], [0.346, 0.315, 0.342, 0.304, 0.354, 0.307], [0.335, 0.293, 0.306, 0.295, 0.334, 0.282], [0.331, 0.291, 0.31, 0.287, 0.335, 0.286], [0.347, 0.307, 0.331, 0.32, 0.359, 0.315]]\n",
        "#CHRF (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "chrfScores = [[0.555, 0.513, 0.549, 0.49, 0.581, 0.49], [0.537, 0.488, 0.507, 0.438, 0.51, 0.442], [0.58, 0.543, 0.587, 0.512, 0.592, 0.502], [0.566, 0.514, 0.537, 0.496, 0.567, 0.475], [0.551, 0.495, 0.527, 0.479, 0.561, 0.472], [0.581, 0.524, 0.557, 0.538, 0.597, 0.518]]\n",
        "#BERT (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bertScores = [[0.933, 0.923, 0.92, 0.936, 0.937, 0.924], [0.93, 0.918, 0.914, 0.925, 0.923, 0.914], [0.933, 0.926, 0.924, 0.937, 0.936, 0.923], [0.932, 0.92, 0.915, 0.934, 0.93, 0.917], [0.928, 0.918, 0.917, 0.921, 0.923, 0.916], [0.931, 0.921, 0.917, 0.934, 0.929, 0.919]]\n",
        "\n",
        "def calculateDiffs(list_scores, system_IDShortnames, metric):\n",
        "  print(metric)\n",
        "  for pos, systemScores in enumerate(list_scores):\n",
        "    system_name = system_IDShortnames[pos]\n",
        "    if system_name == 'DCU-Sm':\n",
        "      print(f'  {system_name}')\n",
        "      print(f'    1 FA to CFA: {round(systemScores[1]-systemScores[0], 2)}')\n",
        "      print(f'    1 FA to FI: {round(systemScores[2]-systemScores[0], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    2 FA to CFA: {round(systemScores[4]-systemScores[3], 2)}')\n",
        "      print(f'    2 FA to FI: {round(systemScores[5]-systemScores[3], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    FA 1 to 2: {round(systemScores[3]-systemScores[0], 2)}')\n",
        "      print(f'    CFA 1 to 2: {round(systemScores[4]-systemScores[1], 2)}')\n",
        "      print(f'    FI 1 to 2: {round(systemScores[5]-systemScores[2], 2)}')\n",
        "\n",
        "calculateDiffs(bleuScores, system_IDShortnames, 'BLEU')\n",
        "calculateDiffs(meteorScores, system_IDShortnames, 'METEOR')\n",
        "calculateDiffs(chrfScores, system_IDShortnames, 'chrF++')\n",
        "calculateDiffs(bertScores, system_IDShortnames, 'BERT')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z_I8H9TyF1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "FORGe_input_folder = '/content/wiki-gen-demo/FORGe/DCU_TCD_FORGe_WebNLG23/code'\n",
        "\n",
        "\n",
        "print(FORGe_input_folder.rsplit('/', 1)[0])\n",
        "print(os.path.split(FORGe_input_folder))"
      ],
      "metadata": {
        "id": "I520bR6uDmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 - Qualitative evaluation\n",
        "\n",
        "Choose eval_method and run all cells in this group.\n",
        "This group creates csv files with the following system-level scores for each language (en, es, sw) for each of the 6 D2T subtasks (180 input/output pairs each):\n",
        "\n",
        "With eval_method == 'LLM':\n",
        "\n",
        "(1) Scores of each of the 4 indivual LLMs (1 rating per LLM per input/output pair): /content/csv_sysLevel_scores/sysLevel_scores_perLLM.csv.\n",
        "\n",
        "(2) Average scores of the 4 LLMs (4 ratings per input/output pair): /content/csv_sysLevel_scores/sysLevel_scores_avgLLM.csv.\n",
        "\n",
        "With eval_method == 'Human':\n",
        "\n",
        "(3) Average human scores (2 human ratings per input/output pair): /content/csv_sysLevel_scores/sysLevel_scores_Humans.csv.\n",
        "\n",
        "For both it also creates files with 180 scores for each language\\*dataset\\*system\\*evaluator combination, where \"evaluator\" is either \"human\" (averaging the ratings for each data point) or one of the LLMs: /content/csv_for_correls/"
      ],
      "metadata": {
        "id": "N4Pc-zkSRkYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare repo (before running, upload zip with jsons that contain evaluations)\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "eval_method = 'Human'#@param['LLM', 'Human']\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  # Unzip evaluations from Gemini and GPT\n",
        "  if not os.path.exists('/content/LLM-eval_final_jsons'):\n",
        "    assert os.path.exists('/content/LLM-eval_final_jsons.zip'), 'You need to upload LLM-eval_final_jsons.zip'\n",
        "    ! unzip /content/LLM-eval_final_jsons.zip\n",
        "\n",
        "  # Unzip evaluations from DeepSeek\n",
        "  if not os.path.exists('/content/r1_llama_70b_en-es-sw'):\n",
        "    assert os.path.exists('r1_llama_70b_en-es-sw.zip'), 'You need to upload r1_llama_70b_en-es-sw.zip'\n",
        "    ! unzip r1_llama_70b_en-es-sw.zip\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Unzip evaluations from DeepSeek\n",
        "  if not os.path.exists('/content/human-eval_final_jsons'):\n",
        "    assert os.path.exists('all_human_annotations.zip'), 'You need to upload en_all_annotations.zip'\n",
        "    ! unzip all_human_annotations.zip -d /content/human-eval_final_jsons\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KzMtQ7hiRlmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format DeepSeek outputs and save jsons in LLM-eval_final_jsons folder\n",
        "# DeepSeek outputs came out with a slightly different format from the Gemini and GPT outputs.\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# If no score is found for a datapoint, ignore it ('no') or assisng a default score (between 1 and 7)\n",
        "assign_default_score = 'no'#@param['no', '1', '2', '3', '4', '5', '6', '7']\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  llama_jsons = glob.glob(os.path.join('/content/r1_llama_70b_en-es-sw', '*.json'))\n",
        "\n",
        "  languages = ['en', 'es', 'sw']\n",
        "  allowed_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "  criteria = {'No-Omissions':'no-omissions', 'No-Additions':'no-additions', 'Grammaticality':'grammaticality', 'Fluency':'fluency'}\n",
        "\n",
        "  counter_all_datapoints = 0\n",
        "  for language in languages:\n",
        "    print(f'\\n{language}\\n--------')\n",
        "    list_dico_new_json = []\n",
        "    counter_lng_datapoints = 0\n",
        "    for llama_json in llama_jsons:\n",
        "      dico_new_json = {}\n",
        "      data_r1 = ''\n",
        "      with open(llama_json, 'r') as f:\n",
        "        data_r1 = json.load(f)\n",
        "      # print(data_r1['id'])\n",
        "      language_json = data_r1['id'].split('_', 1)[0]\n",
        "      if language_json == language:\n",
        "        dico_new_json['eid'] = data_r1['id']\n",
        "        dico_new_json['annotator_id'] = 'R1-Llama-70B'\n",
        "\n",
        "        # Check data\n",
        "        if data_r1['parsed_response']:\n",
        "          for criterion in sorted(criteria.keys(), reverse=True):\n",
        "            if data_r1['parsed_response'][criterion]['Score']:\n",
        "              if int(data_r1['parsed_response'][criterion]['Score']) in allowed_scores:\n",
        "                key_name = criteria[criterion]\n",
        "                dico_new_json[key_name] = int(data_r1['parsed_response'][criterion]['Score'])\n",
        "              else:\n",
        "                # This does not happen in the data\n",
        "                print(f'{data_r1[\"id\"]} - Criterion score out of range: {criterion}')\n",
        "            else:\n",
        "              # This does not happen either in the data\n",
        "              print(f'{data_r1[\"id\"]} - Criterion score missing: {criterion}')\n",
        "          counter_lng_datapoints += 1\n",
        "          # Add eval point to the main dico\n",
        "          list_dico_new_json.append(dico_new_json)\n",
        "        else:\n",
        "          # We have 11 cases of no answer from the model. Excluding data points for now.\n",
        "          print('  Missing scores for', data_r1['id'], ':', data_r1['parsed_response'])\n",
        "          if not assign_default_score == 'no':\n",
        "            for criterion in sorted(criteria.keys(), reverse=True):\n",
        "              # Assign median score for missing evals\n",
        "              key_name = criteria[criterion]\n",
        "              dico_new_json[key_name] = int(assign_default_score)\n",
        "              print(f'    Assigned {assign_default_score} for {criterion}')\n",
        "            list_dico_new_json.append(dico_new_json)\n",
        "\n",
        "    counter_all_datapoints += counter_lng_datapoints\n",
        "    print(f'Language {language}: {counter_lng_datapoints} data points')\n",
        "\n",
        "    # Save list_dico_new_json in a json file\n",
        "    with open(f'/content/LLM-eval_final_jsons/{language.upper()}_R1-Llama-70B_scores.json', 'w') as f:\n",
        "      json.dump(list_dico_new_json, f, indent=4)\n",
        "\n",
        "    print(f'All datapoints: {counter_all_datapoints}')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "xWYS5pZluSFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate system_level scores D2T assigned by each LLM or by the set of human annotators and create CSVs\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "csv_sys_level_scores = '/content/csv_sysLevel_scores'\n",
        "if not os.path.exists(csv_sys_level_scores):\n",
        "  os.makedirs(csv_sys_level_scores)\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "allowed_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "def check_scores(score_list, allowed_scores):\n",
        "  for score in score_list:\n",
        "    assert score in allowed_scores, f'Score out of range: {score}'\n",
        "\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "json_files = ''\n",
        "if eval_method == 'LLM':\n",
        "  # Load the files one at a time; there is one file per LLM-language combination (4*3 = 12 files in total)\n",
        "  json_folder = '/content/LLM-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Load the files one at a time; there is one file per language (since human annotators did not each do all annotations, unlike LLMs)\n",
        "  json_folder = '/content/human-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "pd_list = [pd.read_json(file) for file in sorted(json_files)]\n",
        "\n",
        "# Create dataframes to store system-level scores\n",
        "sysLevel_scores_perLLM = pd.DataFrame(columns=['language', 'task', 'system', eval_method, 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "sysLevel_scores_Humans = pd.DataFrame(columns=['language', 'task', 'system', eval_method, 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "system_task_counter = 0\n",
        "for df in pd_list:\n",
        "  if DEBUG:\n",
        "    print(df)\n",
        "  # LLM and Human files use different IDs (eid VS id)\n",
        "  if eval_method == 'LLM':\n",
        "    # split eid column using underscores and create corresponding columns in the dataframe\n",
        "    df[['language', 'task', 'id', 'system']] = df['eid'].str.split('_', expand=True)\n",
        "  elif eval_method == 'Human':\n",
        "    # split id column using underscores and create corresponding columns in the dataframe\n",
        "    df[['language', 'task', 'id', 'system']] = df['id'].str.split('_', expand=True)\n",
        "\n",
        "  if eval_method == 'LLM':\n",
        "    print('Getting scores for', df['annotator_id'][0], df['language'][0], '...')\n",
        "  elif eval_method == 'Human':\n",
        "    print('Getting human scores for', df['language'][0])\n",
        "\n",
        "  # Get all possible values for task and system column\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  unique_systems = sorted(df['system'].unique())\n",
        "  # print('', unique_tasks)\n",
        "  # print('', unique_systems)\n",
        "\n",
        "  # Get the average scores for each system for each task\n",
        "  for task in unique_tasks:\n",
        "    if DEBUG:\n",
        "      print(f'Task: {task}\\n===========')\n",
        "    for system in unique_systems:\n",
        "      if DEBUG:\n",
        "        print(f'\\nSystem: {system}')\n",
        "      # get scores in each of the 4 score columns\n",
        "      no_omissions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no-omissions']\n",
        "      check_scores(no_omissions_scores, allowed_scores)\n",
        "      no_additions_scores = ''\n",
        "      # LLM and human files have different criterion names (hyphen VS underscore)\n",
        "      if eval_method == 'LLM':\n",
        "        no_additions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no-additions']\n",
        "      elif eval_method == 'Human':\n",
        "        no_additions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no_additions']\n",
        "      check_scores(no_additions_scores, allowed_scores)\n",
        "      grammaticality_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'grammaticality']\n",
        "      check_scores(grammaticality_scores, allowed_scores)\n",
        "      fluency_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'fluency']\n",
        "      check_scores(fluency_scores, allowed_scores)\n",
        "      #Get average scores\n",
        "      average_no_omissions_score = no_omissions_scores.mean()\n",
        "      average_no_additions_score = no_additions_scores.mean()\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "      if DEBUG:\n",
        "        if len(no_omissions_scores) > 0:\n",
        "          print(f'  Average no-omissions score: {average_no_omissions_score}')\n",
        "          print(f'  Average no-additions score: {average_no_additions_score}')\n",
        "          print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "          print(f'  Average fluency score: {average_fluency_score}')\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "\n",
        "        if eval_method == 'LLM':\n",
        "          # There are scores missing for DeepSeek\n",
        "          if not df['annotator_id'][0] == 'R1-Llama-70B':\n",
        "            assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 180, 'There should be 180 ratings for each criterion!'\n",
        "          else:\n",
        "            if no_omissions_scores.count() == 180:\n",
        "              assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 180, 'There should be 180 ratings for each criterion!'\n",
        "            else:\n",
        "              print(f\"  Less than 180 scores found: {df['language'][0]}-{task}-{system}\", no_omissions_scores.count(), no_additions_scores.count(), grammaticality_scores.count(), fluency_scores.count())\n",
        "          # Add a row to sysLevel_scores_perLLM dataframe\n",
        "          sysLevel_scores_perLLM.loc[system_task_counter] = [df['language'][0], task, system, df['annotator_id'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        elif eval_method == 'Human':\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() , 'There should be the same number of ratings for each criterion!'\n",
        "          if not no_omissions_scores.count() >= 388:\n",
        "            # There should be 2 annotators per datapoint (2*180) + several datapoints with additional annotators for IAA (apparently at least 30 additional scores in EN, 28 in ES)\n",
        "            print('??? ERROR??? Less than 388 scores:', df['language'][0], task, system, '|| Found', str(no_omissions_scores.count()))\n",
        "          # print(f'Number of no-omissions scores: {no_omissions_scores.count()}')\n",
        "          # print(f'Number of no-additions scores: {no_additions_scores.count()}')\n",
        "          # print(f'Number of grammaticality scores: {grammaticality_scores.count()}')\n",
        "          # print(f'Number of fluency scores: {fluency_scores.count()}')\n",
        "          # Add row to sysLevel_scores_Humans dataframe\n",
        "          sysLevel_scores_Humans.loc[system_task_counter] = [df['language'][0], task, system, eval_method, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "\n",
        "        system_task_counter += 1\n",
        "\n",
        "print()\n",
        "# At this point, all submissions, including the withdrawn one, were evaluated\n",
        "# EN = 46 rows: 7 systems * 6 datasets + 1 system * 3 datasets + human *1 dataset\n",
        "# ES = 18 rows: 3 systems * 6 datasets\n",
        "# SW = 15 rows: 2 systems * 6 datasets + 1 system * 3 datasets\n",
        "# All languages = 79 rows; 4 LLMS-> 316 rows\n",
        "if eval_method == 'LLM':\n",
        "  assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "  print('Number of rows OK (316).')\n",
        "  print(sysLevel_scores_perLLM)\n",
        "  # Dump dataframe in a CSV file\n",
        "  sysLevel_scores_perLLM.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_perLLM.csv'), index=False)\n",
        "elif eval_method == 'Human':\n",
        "  assert len(sysLevel_scores_Humans) == 64, 'There should be 64 rows (EN=46, ES=18) with scores for the human evals!'\n",
        "  print('Number of rows OK (64).')\n",
        "  print(sysLevel_scores_Humans)\n",
        "  # Dump dataframe in a CSV file\n",
        "  sysLevel_scores_Humans.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_Humans.csv'), index=False)"
      ],
      "metadata": {
        "id": "ob48zv9sS_b8",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get average (across LLMs) system-level scores D2T and create CSV\n",
        "# There are 4 LLMs which gave evaluations for all datapoints; we need this extra cell for LLMs to get an average LLM score for each system\n",
        "\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  # Create dataframe to store average LLM-assigned system-level scores\n",
        "  avg_sysLevel_scores_LLMs = pd.DataFrame(columns=['language', 'task', 'system', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "  # Now add a row that averages all LLM scores for each system for each task\n",
        "  unique_languages = sorted(sysLevel_scores_perLLM['language'].unique())\n",
        "  unique_tasks_avg = sorted(sysLevel_scores_perLLM['task'].unique())\n",
        "  unique_systems_avg = sorted(sysLevel_scores_perLLM['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  # print(unique_tasks_avg)\n",
        "  # print(unique_systems_avg)\n",
        "\n",
        "  language_system_task_counter = 0\n",
        "  for language in unique_languages:\n",
        "    if DEBUG:\n",
        "      print(f'Language: {language}')\n",
        "    for task in unique_tasks_avg:\n",
        "      if DEBUG:\n",
        "        print(f'Task: {task}')\n",
        "      for system in unique_systems_avg:\n",
        "        if DEBUG:\n",
        "          print(f'System: {system}')\n",
        "        # Get the average scores for each system for each task\n",
        "        no_omissions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_no-omissions']\n",
        "        no_additions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_no-additions']\n",
        "        grammaticality_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_grammaticality']\n",
        "        fluency_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_fluency']\n",
        "        # Get average\n",
        "        average_no_omissions_score = no_omissions_scores.mean()\n",
        "        average_no_additions_score = no_additions_scores.mean()\n",
        "        average_grammaticality_score = grammaticality_scores.mean()\n",
        "        average_fluency_score = fluency_scores.mean()\n",
        "        if DEBUG:\n",
        "          if len(no_omissions_scores) > 0:\n",
        "            print(f'  Average no-omissions score: {average_no_omissions_score}')\n",
        "            print(f'  Average no-additions score: {average_no_additions_score}')\n",
        "            print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "            print(f'  Average fluency score: {average_fluency_score}')\n",
        "\n",
        "        # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "        if not math.isnan(average_fluency_score):\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 4, 'There should be 4 ratings for each criterion!'\n",
        "          # Add a row to avg_scores dataframe\n",
        "          avg_sysLevel_scores_LLMs.loc[language_system_task_counter] = [language, task, system, 'Average_LLM', average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "          language_system_task_counter += 1\n",
        "          # Dump dataframe in a CSV file\n",
        "          avg_sysLevel_scores_LLMs.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_avgLLM.csv'), index=False)\n",
        "\n",
        "  pd.set_option('display.max_rows', None)\n",
        "  print()\n",
        "  assert len(avg_sysLevel_scores_LLMs) == 79, 'There should be 79 rows with scores!'\n",
        "  print(avg_sysLevel_scores_LLMs)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zIucLwGOx0EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one file with all scores for each language+task+system+evaluator combination\n",
        "#These are the files for calculating correlations\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if os.path.exists('/content/csv_for_correls.zip'):\n",
        "  ! rm -rf /content/csv_for_correls.zip\n",
        "\n",
        "if not os.path.exists('/content/csv_for_correls'):\n",
        "  os.mkdir('/content/csv_for_correls')\n",
        "\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "# List here all systems that have scores from all evaluators (hum+LLMS) on at least one dataset. Exclude withdrawn system (system 8)\n",
        "system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "\n",
        "json_files = ''\n",
        "# LLM and Human files use different IDs (eid VS id)\n",
        "gram_label = 'grammaticality'\n",
        "flu_label = 'fluency'\n",
        "no_om_label = 'no-omissions'\n",
        "no_add_label = ''\n",
        "id_label = ''\n",
        "if eval_method == 'LLM':\n",
        "  # Load the files one at a time; there is one file per LLM-language combination (4*3 = 12 files in total)\n",
        "  json_folder = '/content/LLM-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "  id_label = 'eid'\n",
        "  no_add_label = 'no-additions'\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Load the files one at a time; there is one file per language (since human annotators did not each do all annotations, unlike LLMs)\n",
        "  json_folder = '/content/human-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "  id_label = 'id'\n",
        "  no_add_label = 'no_additions'\n",
        "\n",
        "pd_list = [pd.read_json(file) for file in sorted(json_files)]\n",
        "\n",
        "count_systems_no_score = 0\n",
        "# The beginning of the following is the same as in the cell that calculates system-level scores\n",
        "for df in pd_list:\n",
        "  # if DEBUG:\n",
        "  #   print(df)\n",
        "  # split eid column using underscores and create corresponding columns in the dataframe\n",
        "  df[['language', 'task', 'input_id', 'system']] = df[id_label].str.split('_', expand=True)\n",
        "\n",
        "  unique_languages = sorted(df['language'].unique())\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  # id contains the whole info: language, task, input_id, system\n",
        "  # unique_ids = sorted(df['id'].unique())\n",
        "  # Exclude withdrawn system\n",
        "  unique_systems = [x for x in sorted(df['system'].unique()) if x in system_IDnames_dico[df['language'][0]].keys()]\n",
        "  if DEBUG:\n",
        "    if eval_method == 'LLM':\n",
        "      print(df['annotator_id'][0])\n",
        "    else:\n",
        "      print('Human')\n",
        "    print(unique_languages)\n",
        "    print(unique_tasks)\n",
        "    print(unique_systems)\n",
        "  for language in unique_languages:\n",
        "    if eval_method == 'LLM':\n",
        "      print(f'Processing {language} {df[\"annotator_id\"][0]}...')\n",
        "    else:\n",
        "      print(f'Processing {language} Human...')\n",
        "    for task in unique_tasks:\n",
        "      # We need to count unique_inputs at a task level, since in D2T-1 and D2T-2 the sampled inputs are not the same\n",
        "      unique_input_ids = sorted(df.loc[(df['language'] == language) & (df['task'] == task), 'input_id'].unique())\n",
        "      print(f'  {task}: # unique input IDs = {len(unique_input_ids)}')\n",
        "      for system in unique_systems:\n",
        "        filename = ''\n",
        "        if eval_method == 'LLM':\n",
        "          filename = f'{language}_{task}_{system}_{df[\"annotator_id\"][0]}.csv'\n",
        "        else:\n",
        "          filename = f'{language}_{task}_{system}_Human.csv'\n",
        "\n",
        "        # Check if there are results for the combination of language, task and system\n",
        "        if df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system)].empty:\n",
        "          if DEBUG:\n",
        "            print(f'No scores found for {language}-{task}-{system}')\n",
        "          count_systems_no_score += 1\n",
        "        else:\n",
        "          if DEBUG:\n",
        "            print(f'{language}-{task}-{system} OK')\n",
        "          # Make a new dataframe with the columns id, no_om_label, no_add_label, gram_label, flu_label and add rows that correspond to language, task and system\n",
        "          scores_df = pd.DataFrame(columns=['id', 'no-omissions', 'no-additions', gram_label, flu_label, 'num_scores'])\n",
        "          if eval_method == 'LLM':\n",
        "            # For LLM evals, we have on file per LLM/language in which there is one rating for each of the 180 datapoints for all datasets and all systems (except for the few missing ratings from R1)\n",
        "            # Thus we simply need to put select the 180 ratings of one system on one dataset (the language condition below is not needed but left just in case)\n",
        "            scores_df['id'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'input_id']\n",
        "            scores_df['no-omissions'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), no_om_label]\n",
        "            scores_df['no-additions'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), no_add_label]\n",
        "            scores_df[gram_label] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), gram_label]\n",
        "            scores_df[flu_label] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), flu_label]\n",
        "            scores_df['num_scores'] = 1\n",
        "            # Actually, I'd like the LLM and Human files to be aligned for easier processing, so let's sort the rows\n",
        "            # I could use the same code as for the human files, but the latter is much slower (I checked, the outputs are the same in both cases)\n",
        "            scores_df = scores_df.sort_values(by=['id']).reset_index(drop=True)\n",
        "          else:\n",
        "            # For human evals, we have one file per language with 2 to n annotation for each of the 180 datapoints for all datasets and all systems.\n",
        "            # We need to average these 2 to n annotations so as to get 180 scores per dataset per system per language, as for LLMs\n",
        "            # For each language/task/system, get the 2 to n scores for each input\n",
        "            for i, input_id in enumerate(unique_input_ids):\n",
        "              scores_no_om = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), no_om_label]\n",
        "              scores_no_add = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), no_add_label]\n",
        "              scores_gram = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), gram_label]\n",
        "              scores_flu = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), flu_label]\n",
        "              scores_df.loc[i] = [input_id, scores_no_om.mean(), scores_no_add.mean(), scores_gram.mean(), scores_flu.mean(), scores_no_om.count()]\n",
        "\n",
        "          if DEBUG:\n",
        "            print(scores_df)\n",
        "\n",
        "          # Save scores_df in csv_for_correls\n",
        "          scores_df.to_csv(os.path.join('/content/csv_for_correls', filename), index=False)\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  print(f'Number of combinations of language/dataset/system with no score: {str(count_systems_no_score)} (expected: 44)')\n",
        "else:\n",
        "  print(f'Number of combinations of language/dataset/system with no score: {str(count_systems_no_score)} (expected: 8)')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zbJZRY1GzA_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download csv folders\n",
        "from google.colab import files\n",
        "\n",
        "! zip -r /content/csv_for_correls.zip /content/csv_for_correls\n",
        "files.download('/content/csv_for_correls.zip')\n",
        "\n",
        "! zip -r /content/csv_sysLevel_scores.zip /content/csv_sysLevel_scores\n",
        "files.download('/content/csv_sysLevel_scores.zip')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zddv3L8YFWkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 - Create plots and LaTeX tables from scores aggregated in \"Qualitative evaluation\" above."
      ],
      "metadata": {
        "id": "FacsCjD-5JzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "fig_path = '/content/figures'\n",
        "\n",
        "def dictionarise_scores_all_datasets(df_scores, name_evaluator_column):\n",
        "  \"\"\"\n",
        "  To get a dico like the following: {'en': {'D2T-1-CFA': {'avg_no-omissions': {'GPT-4o-mini': [None, 6.06, 6.16, 5.37, 6.32, 5.83, 6.44, 6.38, 2.57], 'GPT-o3-mini': [None, 6.13, 6.37, 5.55, 6.46, 6.05, 6.97, 6.77, 2.72], ...} ...} ...} ...}\n",
        "  For plotting scores of an evaluation method across all systems for each criterion, for each dataset, for each language.\n",
        "  \"\"\"\n",
        "  unique_languages = sorted(df_scores['language'].unique())\n",
        "  unique_evaluator = sorted(df_scores[name_evaluator_column].unique())\n",
        "  unique_tasks = sorted(df_scores['task'].unique())\n",
        "  unique_systems = sorted(df_scores['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  # print(unique_LLMs)\n",
        "  # print(unique_tasks)\n",
        "  # print(unique_systems)\n",
        "\n",
        "  # system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  eval_criteria = ['avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency']\n",
        "\n",
        "  no_scores_found = []\n",
        "  counter_row_with_score = 0\n",
        "  # Let's plot how each LLM rates each system, and make a plot for each language*task*criterion\n",
        "  dico_plots = {}\n",
        "  for language in unique_languages:\n",
        "    if not language in dico_plots:\n",
        "      dico_plots[language] = {}\n",
        "    for task in unique_tasks:\n",
        "      if not ('All' in unique_tasks and not task == 'All'):\n",
        "        if not task in dico_plots[language]:\n",
        "          dico_plots[language][task] = {}\n",
        "        for eval_criterion in eval_criteria:\n",
        "          if not eval_criterion in dico_plots[language][task]:\n",
        "            dico_plots[language][task][eval_criterion] = {}\n",
        "          for evaluator in unique_evaluator:\n",
        "            if not evaluator in dico_plots[language][task][eval_criterion]:\n",
        "              dico_plots[language][task][eval_criterion][evaluator] = []\n",
        "            for system in unique_systems:\n",
        "              # Filter out withdrawn system\n",
        "              # if system in system_IDnames_dico[language].keys(): # Actually no, let's filter that later on\n",
        "              # Check that a system has scores for the task\n",
        "              if df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores[name_evaluator_column] == evaluator) & (df_scores['system'] == system)].empty:\n",
        "                if f'{language}-{task}-{system}-{evaluator}-{eval_criterion}' not in no_scores_found:\n",
        "                  no_scores_found.append(f'{language}-{task}-{system}-{evaluator}-{eval_criterion}')\n",
        "                dico_plots[language][task][eval_criterion][evaluator].append(None)\n",
        "              else:\n",
        "                # Append the score for the system found in the column with the header eval_criterion\n",
        "                dico_plots[language][task][eval_criterion][evaluator].append(float(round(df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores[name_evaluator_column] == evaluator) & (df_scores['system'] == system), eval_criterion].values[0], 2)))\n",
        "                counter_row_with_score += 1\n",
        "  return dico_plots, counter_row_with_score\n",
        "\n",
        "def plot_dicoPlots_lng_task_criterion_evalMethod_line (dico_plots, fig_path):\n",
        "  if not os.path.exists(fig_path):\n",
        "    os.mkdir(fig_path)\n",
        "  # system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  # We list here only actually submitted systems (i.e. not system 8, who withdrew) that have all scores on ALL datasets, which excludes WebNLG-Human (only D2T-1-FA) and DCU-ADAPT-modPB (only D2T-1).\n",
        "  system_IDnames_dico = {'en' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  # system_colors = {'0':'dodgerblue', '1':'purple', '2':'blue', '3':'orange', '4':'green', '5':'red', '6':'purple', '7':'brown'}\n",
        "  # plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "  # plt.xlabel(\"System\")\n",
        "  # plt.ylabel(\"Score\")\n",
        "  # plt.legend()\n",
        "  for language in dico_plots.keys():\n",
        "    # print(language)\n",
        "    for task in dico_plots[language].keys():\n",
        "      # print(f'  {task}')\n",
        "      for eval_criterion in dico_plots[language][task]:\n",
        "        # print(f'    {eval_criterion}')\n",
        "        for LLM in dico_plots[language][task][eval_criterion]:\n",
        "          # print(f'      {LLM}')\n",
        "          list_scores = [dico_plots[language][task][eval_criterion][LLM][int(x)] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "          # print(f'        {list_scores}')\n",
        "          # plt.plot(range(1, len(list_scores) + 1), list_scores, label=LLM)\n",
        "          plt.plot([system_IDnames_dico[language][x] for x in sorted(system_IDnames_dico[language].keys())], list_scores, label=LLM)\n",
        "        plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "        if len(system_IDnames_dico[language].keys()) > 4:\n",
        "          plt.xticks(rotation=15)\n",
        "        plt.ylim(1, 7)\n",
        "        plt.xlabel(\"System\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(os.path.join(fig_path, f'{language}-{task}-{eval_criterion}_line.png'))\n",
        "        plt.show()\n",
        "\n",
        "def plot_dicoPlots_lng_task_criterion_evalMethod_bars (dico_plots, fig_path):\n",
        "  if not os.path.exists(fig_path):\n",
        "    os.mkdir(fig_path)\n",
        "  system_IDnames_dico = {'en' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  for language in dico_plots.keys():\n",
        "    for task in dico_plots[language].keys():\n",
        "      for eval_criterion in dico_plots[language][task]:\n",
        "        # Get list of systems for this language\n",
        "        systems = [system_IDnames_dico[language][x] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "\n",
        "        # Get list of scores for each LLM for this language, task, and criterion\n",
        "        llm_scores = {}\n",
        "        for LLM in dico_plots[language][task][eval_criterion]:\n",
        "          llm_scores[LLM] = [dico_plots[language][task][eval_criterion][LLM][int(x)] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "\n",
        "        # Create bar plot\n",
        "        bar_width = 0.15  # Adjust as needed\n",
        "        x_pos = np.arange(len(systems))\n",
        "        # print(f'XPOS = {x_pos}')\n",
        "\n",
        "        # Plotting bars for each LLM with offset for readability\n",
        "        for i, (LLM, scores) in enumerate(llm_scores.items()):\n",
        "          scores_wo_None = [0 if x==None else x for x in scores]\n",
        "          # print(f'{x_pos + i * bar_width}, {scores_wo_None}, {bar_width}, {LLM}')\n",
        "          plt.bar(x_pos + i * bar_width, scores_wo_None, bar_width, label=LLM)\n",
        "\n",
        "        plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "        plt.xticks(x_pos + bar_width * (len(llm_scores) -1)/2 , systems, rotation=15 if len(systems) > 4 else 0)  # Center x-axis ticks\n",
        "        plt.ylim(1, 7)\n",
        "        plt.xlabel(\"System\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(os.path.join(fig_path, f'{language}-{task}-{eval_criterion}_bar.png'))\n",
        "        plt.show()\n",
        "\n",
        "def plot_dicoPlots_lng_task_criterion_evalMethod_dots(dicoPlots, path_out_plots):\n",
        "  \"\"\"\n",
        "  Generates plots for different combinations of language, task, and criterion.\n",
        "\n",
        "  Args:\n",
        "    dicoPlots: A dictionary containing scores for different systems, evaluation modes,\n",
        "               languages, tasks, and criteria.\n",
        "    path_out_plots: The path to the folder where the plots will be saved.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define markers and colors for the plots\n",
        "  markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', '*']\n",
        "  colors = {'R1-Llama-70B': 'purple', 'Gemini-1dot5-flash': 'green', 'GPT-4o-mini': 'blue', 'GPT-o3-mini': 'orange'}\n",
        "\n",
        "  # System names dictionary\n",
        "  system_IDnames_dico = {'en': {'2': 'DCU-NLG-PBN', '3': 'DCU-NLG-Small', '4': 'DipInfo-UniTo', '5': 'OSU-CompLing', '6': 'RDFpyrealb', '7': 'SaarLST'},\n",
        "                        'es': {'2': 'DCU-NLG-PBN', '3': 'DCU-NLG-Small', '5': 'OSU-CompLing'},\n",
        "                        'sw': {'2': 'DCU-NLG-PBN', '3': 'DCU-NLG-Small'}}\n",
        "\n",
        "  # Iterate over languages, tasks, and criteria\n",
        "  for language in dicoPlots:\n",
        "    for task in dicoPlots[language]:\n",
        "      for criterion in dicoPlots[language][task]:\n",
        "        # Proceed only if we have human ratings\n",
        "        if \"Human\" in dicoPlots[language][task][criterion] and not (len(set(dicoPlots[language][task][criterion]['Human'])) == 1 and None in set(dicoPlots[language][task][criterion]['Human'])):\n",
        "          # Create a figure and axis object\n",
        "          fig, ax = plt.subplots()\n",
        "\n",
        "          human_scores = dicoPlots[language][task][criterion][\"Human\"]\n",
        "\n",
        "          # Legend entries for systems (using grey color)\n",
        "          system_legend_entries = []\n",
        "          for system_index in sorted(system_IDnames_dico[language].keys()):\n",
        "            system_name = system_IDnames_dico[language][system_index]\n",
        "            system_legend_entries.append(ax.scatter([], [], marker=markers[int(system_index)], color='grey', label=system_name))\n",
        "          # Legend entries for LLMs (using colors from the dictionary)\n",
        "          llm_legend_entries = []\n",
        "          for eval_mode in sorted(colors.keys()):\n",
        "              llm_legend_entries.append(ax.scatter([], [], marker='o', color=colors[eval_mode], label=eval_mode))\n",
        "          # Combine system and LLM legend entries\n",
        "          all_legend_entries = system_legend_entries + llm_legend_entries\n",
        "\n",
        "          # Plot data points (using LLM colors)\n",
        "          for system_index in sorted(system_IDnames_dico[language].keys()):\n",
        "            # Get system name using system ID and language\n",
        "            system_name = system_IDnames_dico[language][system_index]\n",
        "\n",
        "            for eval_mode in dicoPlots[language][task][criterion]:\n",
        "              if eval_mode != \"Human\":\n",
        "                llm_score = dicoPlots[language][task][criterion][eval_mode][int(system_index)]\n",
        "                # Update label to use system name and LLM name without \"System\"\n",
        "                ax.scatter(llm_score, human_scores[int(system_index)], marker=markers[int(system_index)], color=colors[eval_mode]) #, label=f\"{system_name} ({eval_mode})\" if int(system_index) == 0 else f\"{system_name}\")\n",
        "\n",
        "          ax.set_xlabel(\"LLM score\")\n",
        "          ax.set_ylabel(\"Human score\")\n",
        "          ax.set_title(f\"{language}-{task}: {criterion}\")\n",
        "\n",
        "          handles, labels = ax.get_legend_handles_labels()\n",
        "          by_label = dict(zip(labels, handles))\n",
        "          # ax.legend(by_label.values(), by_label.keys(), loc='best', bbox_to_anchor=(1.05, 1))\n",
        "          ax.legend(handles=all_legend_entries, loc='upper left', bbox_to_anchor=(0, 1))\n",
        "\n",
        "          ax.set_xlim([4, 7])\n",
        "          ax.set_ylim([4, 7])\n",
        "\n",
        "          filename = os.path.join(path_out_plots, f\"{language}-{task}-{criterion}_dots.png\")\n",
        "          plt.savefig(filename, bbox_inches='tight')\n",
        "          plt.show()\n",
        "          plt.close(fig)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T6njsi2VScJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot scores of each LLM independently, for each criterion, for each dataset"
      ],
      "metadata": {
        "id": "E6IxvYJf7Edx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Organise data into a dico and make plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "llm_csv = '/content/csv_sysLevel_scores/sysLevel_scores_perLLM.csv'\n",
        "assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "sysLevel_scores_perLLM = pd.read_csv(llm_csv)\n",
        "assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "\n",
        "dicoPlots_lng_task_criterion_evalMethod, counter_row_with_score = dictionarise_scores_all_datasets(sysLevel_scores_perLLM, 'LLM')\n",
        "assert counter_row_with_score == 1264, f'There should be 1264 rows with scores (with the 8 systems: 4 criteria * 316 rows - 184-en, 72-es, 60-sw), found {counter_row_with_score}!'\n",
        "# print(sorted(no_scores_found))\n",
        "print(dicoPlots_lng_task_criterion_evalMethod)\n",
        "\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_line(dicoPlots_lng_task_criterion_evalMethod, fig_path)\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_bars(dicoPlots_lng_task_criterion_evalMethod, fig_path)"
      ],
      "metadata": {
        "id": "J6xjI9455xW3",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot average scores across all datasets of all evaluations independently (4 LLMs and 1 human) for each criterion"
      ],
      "metadata": {
        "id": "_aJWR6ogjxCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine human and LLM eval dataframes and get averages across tasks for each system\n",
        "import os\n",
        "\n",
        "hum_csv = '/content/csv_sysLevel_scores/sysLevel_scores_Humans.csv'\n",
        "llm_csv = '/content/csv_sysLevel_scores/sysLevel_scores_perLLM.csv'\n",
        "assert os.path.exists(hum_csv), 'sysLevel_scores_Humans.csv file missing! Run cells in 7 above with \"Human\" eval_method'\n",
        "assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "\n",
        "# Load csv files into pandas dataframes\n",
        "df_hum = pd.read_csv(hum_csv)\n",
        "assert len(df_hum) == 64, 'There should be 64 rows (EN+ES) with scores for the human evals!'\n",
        "print('There are 64 rows as expected in the human eval CSV!')\n",
        "df_llm = pd.read_csv(llm_csv)\n",
        "assert len(df_llm) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "print('There are 316 rows as expected in the LLM eval CSV!')\n",
        "# print(df_hum)\n",
        "# print(df_llm)\n",
        "\n",
        "# Combine both dataframes\n",
        "df_all = pd.DataFrame(columns=['language', 'task', 'system', 'evaluator', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "# index is used to store the current index in the dataframe\n",
        "index = 0\n",
        "# Put the human eval data first\n",
        "for index, row in df_hum.iterrows():\n",
        "  df_all.loc[index] = [row['language'], row['task'], row['system'], row['Human'], row['avg_no-omissions'], row['avg_no-additions'], row['avg_grammaticality'], row['avg_fluency']]\n",
        "# Then add the llm eval data on the following rows\n",
        "for _ , row in df_llm.iterrows():\n",
        "  index = index + 1\n",
        "  df_all.loc[index] = [row['language'], row['task'], row['system'], row['LLM'], row['avg_no-omissions'], row['avg_no-additions'], row['avg_grammaticality'], row['avg_fluency']]\n",
        "assert len(df_all) == (len(df_hum) + len(df_llm)), 'Error! The number of rows does not equal the sum of the number of rows in human eval df and llm df.'\n",
        "# print(df_all)\n",
        "\n",
        "# Create dataframe to store average LLM-assigned system-level scores\n",
        "avg_overall_sysLevel_scores_allEvals = pd.DataFrame(columns=['language', 'task', 'system', 'evaluator', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "# Get average scores across all tasks\n",
        "unique_languages = sorted(df_all['language'].unique())\n",
        "unique_evaluators = sorted(df_all['evaluator'].unique())\n",
        "unique_systems = sorted([int(system) for system in df_all['system'].unique()])\n",
        "print(unique_languages)\n",
        "print(unique_evaluators)\n",
        "print(unique_systems)\n",
        "\n",
        "language_system_evaluator_counter = 0\n",
        "for language in unique_languages:\n",
        "  for system in unique_systems:\n",
        "    for evaluator in unique_evaluators:\n",
        "      # Get average scores across all tasks\n",
        "      # print(f'{language}-{system}-{evaluator}')\n",
        "      no_omissions_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_no-omissions']\n",
        "      no_additions_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_no-additions']\n",
        "      grammaticality_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_grammaticality']\n",
        "      fluency_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_fluency']\n",
        "      average_no_omissions_score = no_omissions_scores.mean()\n",
        "      average_no_additions_score = no_additions_scores.mean()\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "        if system == 0:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 1, 'There should be 1 ratings for each criterion for system 0!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'D2T-1-FA', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        elif system == 1:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 3, 'There should be 3 ratings for each criterion for system 1!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'D2T-1', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        else:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 6, 'There should be 6 ratings for each criterion for systems 2-8!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'All', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        # Add a row to avg_scores dataframe\n",
        "        language_system_evaluator_counter += 1\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "print()\n",
        "print(avg_overall_sysLevel_scores_allEvals)\n",
        "assert len(avg_overall_sysLevel_scores_allEvals) == 72, 'There should be 75 rows with scores (en=45, es=15, sw=15)! Or 72 without SW-human evals.'\n",
        "print('There are 72/75 rows as expected in the output dataframe!')\n",
        "\n",
        "# Now only keep a version of the dataframe with only the \"task=All\" column. Not sure it's a good idea, breaks the next funciton to be applied because not all systems have values\n",
        "# avg_overall_sysLevel_scores_allEvals_allTasks = avg_overall_sysLevel_scores_allEvals.loc[avg_overall_sysLevel_scores_allEvals['task'] == 'All']\n",
        "# print()\n",
        "# print(avg_overall_sysLevel_scores_allEvals_allTasks)\n",
        "# assert len(avg_overall_sysLevel_scores_allEvals_allTasks) == 55, 'There should be 60 rows with scores (en=35, es=10, sw=15)! Or 55 without ES and SW human evals.'\n",
        "# print('There are 55 rows as expected in the output dataframe!')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "mfwjM49n5k5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Organise data into a dico and make plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make plots for all tasks separately (just in case we need it)\n",
        "# dicoPlots_lng_task_criterion_evalMethod_all, counter_row_with_score = dictionarise_scores_all_datasets(df_all, 'evaluator')\n",
        "# plot_dicoPlots_lng_task_criterion_evalMethod(dicoPlots_lng_task_criterion_evalMethod_all)\n",
        "\n",
        "# Put all LLM and human system-level evaluations in a dico for plotting\n",
        "dicoPlots_lng_criterion_evalMethod_all, counter_row_with_score = dictionarise_scores_all_datasets(avg_overall_sysLevel_scores_allEvals, 'evaluator')\n",
        "print(dicoPlots_lng_criterion_evalMethod_all)\n",
        "\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_line(dicoPlots_lng_criterion_evalMethod_all, fig_path)\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_bars(dicoPlots_lng_criterion_evalMethod_all, fig_path)\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_dots(dicoPlots_lng_criterion_evalMethod_all, fig_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BZOtb-dxj0HB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download figures folder\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "folder_to_zip = '/content/figures'\n",
        "zip_name_inter = folder_to_zip+'.zip'\n",
        "# if os.path.exists(folder_to_zip):\n",
        "#   ! rm zip_name_inter\n",
        "!zip -r {zip_name_inter} {folder_to_zip}\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XWxy_N_-96sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create LaTeX tables"
      ],
      "metadata": {
        "id": "MpML5-DM7SWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create LaTeX table rows with average scores\n",
        "\n",
        "#Get tables for averaged LLM scores (True), or for each LLM separately (False)\n",
        "eval_method_4table = 'Human'#@param['Human', 'LLM-avg', 'LLM-each']\n",
        "\n",
        "def create_LaTeX_table_parts(dico_avg_scores, eval_method, LLM=None):\n",
        "  unique_languages = sorted(dico_avg_scores['language'].unique())\n",
        "  # unique_tasks = sorted(avg_avg_scores['task'].unique())\n",
        "  unique_tasks = ['D2T-1-FA', 'D2T-1-CFA', 'D2T-1-FI', 'D2T-2-FA', 'D2T-2-CFA', 'D2T-2-FI']\n",
        "  unique_systems = sorted(dico_avg_scores['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  print(f'Results printed in the following order of tasks: {unique_tasks}')\n",
        "  # print(unique_systems)\n",
        "\n",
        "  system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  eval_criteria = {'avg_no-omissions':'No-Omissions', 'avg_no-additions':'No-Additions', 'avg_grammaticality':'Grammaticality', 'avg_fluency':'Fluency'}\n",
        "\n",
        "  # Since the table mixes human and LLM scores, which come from different files, I only print here the LLM scores for each criterion separately, and copy/paste it in the paper by hand (4 copy/paste per language)\n",
        "  counter_scores = 0\n",
        "  dico_plots = {}\n",
        "  for language in unique_languages:\n",
        "    num_systems_for_lng = len(system_IDnames_dico[language])\n",
        "    for eval_criterion in eval_criteria.keys():\n",
        "      if eval_method_4table == 'Human':\n",
        "        line = '\\\\multirow{'+str(num_systems_for_lng*2)+'}{*}{\\\\textbf{'+eval_criteria[eval_criterion]+'}} & \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{Avg. '+str(eval_method_4table)+'} $\\\\uparrow$} & '\n",
        "      elif eval_method_4table == 'LLM-avg':\n",
        "        line = ' & \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{Avg. '+str(eval_method_4table)+'s} $\\\\uparrow$} & '\n",
        "      elif eval_method_4table == 'LLM-each':\n",
        "        line = ' \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{'+str(LLM)+'} $\\\\uparrow$} & '\n",
        "      # Keep track of how many systems we see, because for the first system, the table line is different\n",
        "      count_systems = 0\n",
        "      for system in unique_systems:\n",
        "        # Exclude system that withdrew\n",
        "        if str(system) in system_IDnames_dico[language].keys():\n",
        "          avg_scores_system_across_tasks = []\n",
        "          # Keep track of if a system has an n/a score, to exclude from general average at the end\n",
        "          has_na = False\n",
        "          if count_systems > 0:\n",
        "            # For lines that are not that of the first system, start with two empty columns\n",
        "            if eval_method_4table == 'LLM-avg' or eval_method_4table == 'Human':\n",
        "              line = line + ' & & '\n",
        "            elif eval_method_4table == 'LLM-each':\n",
        "              line = line + ' & '\n",
        "          line = line + system_IDnames_dico[language][str(system)]\n",
        "          for task in unique_tasks:\n",
        "            # Print the score for this criterion\n",
        "            # Check that a system has scores for the task it is expected to\n",
        "            if dico_avg_scores.loc[(dico_avg_scores['language'] == language) & (dico_avg_scores['task'] == task) & (dico_avg_scores['system'] == system)].empty:\n",
        "              # print(f'  {language}-{eval_criterion}-{system}-{task}: n/a')\n",
        "              line = line + ' & ' + 'n/a'\n",
        "              has_na = True\n",
        "            else:\n",
        "              # print(f'  {language}-{eval_criterion}-{system}-{task}: {round(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]), 2)}')\n",
        "              # Add scores for one system across task to list for calculating the average across tasks\n",
        "              avg_scores_system_across_tasks.append(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]))\n",
        "              # Write the score for the current criterion for each of the 6 subtasks\n",
        "              line = line + ' & ' + str(round(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]), 2))\n",
        "              counter_scores += 1\n",
        "\n",
        "          # print(f'  {language}-{eval_criterion}-{system}-Avg: {round(sum(avg_scores_system_across_tasks)/len(avg_scores_system_across_tasks), 2)}')\n",
        "          # Only calculate overall average if a system has scores for all datasets\n",
        "          if has_na:\n",
        "            line = line + ' & ' + 'n/a'\n",
        "          else:\n",
        "            line = line + ' & ' + str(round(sum(avg_scores_system_across_tasks)/len(avg_scores_system_across_tasks), 2))\n",
        "          ## Add the latex linebreak at the end of the line, and a console linebreak for human readibility\n",
        "          line = line + '\\\\\\\\\\n'\n",
        "          count_systems += 1\n",
        "\n",
        "      if eval_method_4table == 'Human':\n",
        "        line = line + '  \\cline{2-10}\\n'\n",
        "      elif eval_method_4table == 'LLM':\n",
        "        line = line + '  \\hline\\n'\n",
        "      if LLM == None:\n",
        "        print(f'{language}-{eval_criterion}-{str(eval_method_4table)}\\n------------------------')\n",
        "      else:\n",
        "        print(f'{language}-{eval_criterion}-{LLM}\\n------------------------')\n",
        "      print(line)\n",
        "\n",
        "  assert counter_scores == 292, f'There should be 292 average scores (excluding system 8: 160-en, 72-es, 60-sw), found {counter_scores}!'\n",
        "\n",
        "\n",
        "hum_csv = '/content/csv_sysLevel_scores/sysLevel_scores_Humans.csv'\n",
        "llm_csv = '/content/csv_sysLevel_scores/sysLevel_scores_perLLM.csv'\n",
        "llm_avg_csv = '/content/csv_sysLevel_scores/sysLevel_scores_avgLLM.csv'\n",
        "\n",
        "# Tables for LLM scores\n",
        "if eval_method_4table == 'Human':\n",
        "  assert os.path.exists(hum_csv), 'sysLevel_scores_Humans.csv file missing! Run cells in 7 above with \"Human\" eval_method'\n",
        "  sysLevel_scores_Humans = pd.read_csv(hum_csv)\n",
        "  assert len(sysLevel_scores_Humans) == 64, 'There should be 64 rows (EN+ES) with scores for the human evals!'\n",
        "  # print(sysLevel_scores_Humans)\n",
        "  create_LaTeX_table_parts(sysLevel_scores_Humans, eval_method_4table)\n",
        "elif eval_method_4table == 'LLM-avg':\n",
        "  assert os.path.exists(hum_csv), 'sysLevel_scores_avgLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method and then run cell for getting average LLM score.'\n",
        "  avg_sysLevel_scores_LLMs = pd.read_csv(llm_avg_csv)\n",
        "  assert len(avg_sysLevel_scores_LLMs) == 79, 'There should be 79 rows with average LLM scores!'\n",
        "  create_LaTeX_table_parts(avg_sysLevel_scores_LLMs, eval_method_4table)\n",
        "elif eval_method_4table == 'LLM-each':\n",
        "  assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "  sysLevel_scores_perLLM = pd.read_csv(llm_csv)\n",
        "  assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "  unique_LLMs = sorted(sysLevel_scores_perLLM['LLM'].unique())\n",
        "  for LLM in unique_LLMs:\n",
        "    # Use only the part of the dataframe with the corresponding LLM\n",
        "    sysLevel_scores_perLLM_LLM = sysLevel_scores_perLLM.loc[sysLevel_scores_perLLM['LLM'] == LLM]\n",
        "    create_LaTeX_table_parts(sysLevel_scores_perLLM_LLM, eval_method_4table, LLM=LLM)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "so5idx2P4znQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9 - Processing of Swahili spreadsheets"
      ],
      "metadata": {
        "id": "-x21GGvkcyM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create folder (once run, upload the CSVs in the /content/SW-evals folder); the name of the CSVs shold be like \"IAA Annotations for D2T - 1 - Annotation.csv\", where \"1\" is the annotator ID\n",
        "import os\n",
        "\n",
        "path_SW_eval_folder = '/content/SW-evals'\n",
        "if not os.path.exists(path_SW_eval_folder):\n",
        "  os.mkdir(path_SW_eval_folder)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3UyAepFRv8U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert SW human eval spreadsheets to normalised json format\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "#======PARAMS=======\n",
        "path_SW_eval_folder = '/content/SW-evals'\n",
        "# first_score_row counter starts at 0\n",
        "first_score_row = 1\n",
        "gap_rows_consecutive_scores = 10\n",
        "annot_id_separator_in_filename = ' - '\n",
        "file_extension = 'csv'\n",
        "completion_marker = '100.0%'\n",
        "allowed_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "#======PARAMS=======\n",
        "\n",
        "def check_scores(score_list, allowed_scores):\n",
        "  for score in score_list:\n",
        "    # print(score, allowed_scores)\n",
        "    assert score in allowed_scores, f'Score out of range: {score}'\n",
        "\n",
        "paths_SW_eval_files = glob.glob(os.path.join(path_SW_eval_folder, '*.'+file_extension))\n",
        "\n",
        "list_human_eval_datapoints = []\n",
        "# Load csvs into dataframes. In each CSV, scores and IDs are found every 10 rows\n",
        "for path_SW_eval_file in paths_SW_eval_files:\n",
        "  annot_id = path_SW_eval_file.replace(' - Annotation', '').rsplit(annot_id_separator_in_filename, 1)[1].rsplit('.', 1)[0]\n",
        "  # print(annot_id)\n",
        "  df = pd.read_csv(path_SW_eval_file)\n",
        "  header_values = list(df.columns.values)\n",
        "  # The top left cell should contain a percentage completion counter, which should be at max\n",
        "  assert header_values[0] == completion_marker, f'Completion counter not at 100%: {header_values[0]}'\n",
        "  x = 0\n",
        "  while x < len(df):\n",
        "    # Get only rows that have the scores and the IDs, i.e. the row numbers for which if you subtract first_score_row and divide by gap_rows_consecutive_scores, the remainder of the division is 0 .\n",
        "    # If the scores are on row 1 and every 10 rows, the \"if\" below will only match rows 1, 11, 21, 31, etc.,\n",
        "    if (x - first_score_row)%gap_rows_consecutive_scores == 0:\n",
        "      dico_eval_datapoint = {}\n",
        "      # get the contents of the cell on row \"x\", column \"no_omissions\"\n",
        "      # no_omissions = df.loc[x, 'no_omissions']\n",
        "      no_omissions_score = df.at[x, 'no_omissions']\n",
        "      no_additions_score = df.at[x, 'no_additions']\n",
        "      grammaticality_score = df.at[x, 'grammaticality']\n",
        "      fluency_score = df.at[x, 'fluency']\n",
        "      # In the final sheets, the ID is on the row before the ratings\n",
        "      id = df.at[x-1, 'ID']\n",
        "      # if id.startswith('ID: '):\n",
        "      #   id = id.replace('ID: ', '')\n",
        "      output_text = df.iloc[x-1, 0]\n",
        "      print(f'{no_omissions_score}, {no_additions_score}, {grammaticality_score}, {fluency_score}, {id}')\n",
        "      # Typo in no-omissions is intentional, so it's the same key as for other languages (en, es)\n",
        "      dico_eval_datapoint['id'] = id\n",
        "      dico_eval_datapoint['input'] = ''\n",
        "      dico_eval_datapoint['output'] = output_text\n",
        "      dico_eval_datapoint['no-omissions'] = no_omissions_score\n",
        "      dico_eval_datapoint['no_additions'] = no_additions_score\n",
        "      dico_eval_datapoint['grammaticality'] = grammaticality_score\n",
        "      dico_eval_datapoint['fluency'] = fluency_score\n",
        "      # The check function expects a list of scores to check\n",
        "      check_scores([no_omissions_score, no_additions_score, grammaticality_score, fluency_score], allowed_scores)\n",
        "      dico_eval_datapoint['annotator_id'] = int(annot_id)\n",
        "      list_human_eval_datapoints.append(dico_eval_datapoint)\n",
        "    x += 1\n",
        "\n",
        "# Save list_human_eval_datapoints as json file\n",
        "with open(os.path.join('/content', 'sw_iaa_annotations.json'), 'w') as f:\n",
        "  json.dump(list_human_eval_datapoints, f)"
      ],
      "metadata": {
        "id": "0O1K2Ru-c58r",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate Fleiss's Kappa\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "path_json = '/content/sw_iaa_annotations.json'#@param\n",
        "# num_agg_categories can be 2 or 3 for now (into how many categories should the 7 scores aggregated)\n",
        "num_agg_categories = 3#@param{'type':'integer'}\n",
        "\n",
        "'''\n",
        "Created on Aug 1, 2016\n",
        "@author: skarumbaiah\n",
        "\n",
        "Computes Fleiss' Kappa\n",
        "Joseph L. Fleiss, Measuring Nominal Scale Agreement Among Many Raters, 1971.\n",
        "'''\n",
        "\n",
        "def checkInput(rate, n):\n",
        "    \"\"\"\n",
        "    Check correctness of the input matrix\n",
        "    @param rate - ratings matrix\n",
        "    @return n - number of raters\n",
        "    @throws AssertionError\n",
        "    \"\"\"\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    assert all(len(rate[i]) == k for i in range(k)), \"Row length != #categories)\"\n",
        "    assert all(isinstance(rate[i][j], int) for i in range(N) for j in range(k)), \"Element not integer\"\n",
        "    assert all(sum(row) == n for row in rate), \"Sum of ratings != #raters)\"\n",
        "\n",
        "def fleissKappa(rate,n):\n",
        "    \"\"\"\n",
        "    Computes the Kappa value\n",
        "    @param rate - ratings matrix containing number of ratings for each subject per category\n",
        "    [size - N X k where N = #subjects and k = #categories]\n",
        "    @param n - number of raters\n",
        "    @return fleiss' kappa\n",
        "    \"\"\"\n",
        "\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    print(\"#raters = \", n, \", #subjects = \", N, \", #categories = \", k)\n",
        "    checkInput(rate, n)\n",
        "\n",
        "    #mean of the extent to which raters agree for the ith subject\n",
        "    PA = sum([(sum([i**2 for i in row])- n) / (n * (n - 1)) for row in rate])/N\n",
        "    print(\"PA = \", PA)\n",
        "\n",
        "    # mean of squares of proportion of all assignments which were to jth category\n",
        "    PE = sum([j**2 for j in [sum([rows[i] for rows in rate])/(N*n) for i in range(k)]])\n",
        "    print(\"PE =\", PE)\n",
        "\n",
        "    kappa = -float(\"inf\")\n",
        "    try:\n",
        "        kappa = (PA - PE) / (1 - PE)\n",
        "        kappa = float(\"{:.3f}\".format(kappa))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Expected agreement = 1\")\n",
        "\n",
        "    print(\"Fleiss' Kappa =\", kappa)\n",
        "\n",
        "    return kappa\n",
        "\n",
        "# Code above expects a matrix of d rows and s columns where:\n",
        "# d is the number of data points (also called \"subjects\")\n",
        "# s is the number of categories, i.e. possible scores (in our case, for one system, that's 7: 1 to 7).\n",
        "# Then at each position in the matrix, there should be the number n of annotators who chose the score s for datapoint d.\n",
        "# rate = [\n",
        "#     [2,0,0,0],\n",
        "#     [0,2,0,0],\n",
        "#     [0,0,2,0],\n",
        "#     [0,0,0,2]\n",
        "# ]\n",
        "# kappa = fleissKappa(rate,2)\n",
        "# assert(kappa==1)\n",
        "\n",
        "def count_ratings_exact(ratings_list):\n",
        "  # Create a list with 7 zeros, each index representing counts for 1-7\n",
        "  counts = [0] * 7\n",
        "  for rating in ratings_list:\n",
        "    if 1 <= rating <= 7:\n",
        "      counts[rating - 1] += 1\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid rating: {rating}. Ratings must be between 1 and 7.\")\n",
        "  return counts\n",
        "\n",
        "def count_ratings_aggregate(ratings_list, num_categories):\n",
        "  # Create a list with 3 zeros, each index representing counts for 1-7\n",
        "  counts = [0] * num_categories\n",
        "  for rating in ratings_list:\n",
        "    if 1 <= rating <= 3:\n",
        "      counts[0] += 1\n",
        "    if num_categories == 2:\n",
        "      if 4 <= rating <= 7:\n",
        "        counts[1] += 1\n",
        "    elif num_categories == 3:\n",
        "      if rating == 4:\n",
        "        counts[1] += 1\n",
        "      elif 5 <= rating <= 7:\n",
        "        counts[2] += 1\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid rating: {rating}. Ratings must be between 1 and 7.\")\n",
        "  return counts\n",
        "\n",
        "def makeFleissMatrix(json_filepath, num_categories):\n",
        "  # for IAA with all 7 scores\n",
        "  scores_matrix_dico_7scores = {'omissions':[], 'additions':[], 'grammaticality':[], 'fluency':[]}\n",
        "  # fro IAA with 3 scores, aggregating 1-2-3 and 5-6-7\n",
        "  scores_matrix_dico_Aggscores = {'omissions':[], 'additions':[], 'grammaticality':[], 'fluency':[]}\n",
        "  num_annot = 0\n",
        "  with open(json_filepath) as f:\n",
        "    json_data = json.load(f)\n",
        "  df_scores = pd.read_json(json_filepath)\n",
        "  df_scores_per_item = df_scores.groupby(['id'])\n",
        "  # key is the full ID\n",
        "  for key, item in df_scores_per_item:\n",
        "    # print(key)\n",
        "    no_om_scores = []\n",
        "    no_ad_scores = []\n",
        "    gram_scores = []\n",
        "    flu_scores = []\n",
        "    # Collect scores\n",
        "    for index, row in item.iterrows():\n",
        "      no_om_scores.append(row['no-omissions'])\n",
        "      no_ad_scores.append(row['no_additions'])\n",
        "      gram_scores.append(row['grammaticality'])\n",
        "      flu_scores.append(row['fluency'])\n",
        "    assert len(no_om_scores) == len(no_ad_scores) == len(gram_scores) == len(flu_scores)\n",
        "    if num_annot == 0:\n",
        "      num_annot = len(no_om_scores)\n",
        "    assert len(no_om_scores) == num_annot\n",
        "    # Convert list of scores into a list of count of each score, e.g. [3, 3, 4, 2, 5] -> [0, 1, 2, 1, 1, 0, 0] (7 scores) or  [3, 3, 4, 2, 5] -> [3, 2] (2 scores)\n",
        "    scores_matrix_dico_7scores['omissions'].append(count_ratings_exact(no_om_scores))\n",
        "    scores_matrix_dico_7scores['additions'].append(count_ratings_exact(no_ad_scores))\n",
        "    scores_matrix_dico_7scores['grammaticality'].append(count_ratings_exact(gram_scores))\n",
        "    scores_matrix_dico_7scores['fluency'].append(count_ratings_exact(flu_scores))\n",
        "    scores_matrix_dico_Aggscores['omissions'].append(count_ratings_aggregate(no_om_scores, num_categories))\n",
        "    scores_matrix_dico_Aggscores['additions'].append(count_ratings_aggregate(no_ad_scores, num_categories))\n",
        "    scores_matrix_dico_Aggscores['grammaticality'].append(count_ratings_aggregate(gram_scores, num_categories))\n",
        "    scores_matrix_dico_Aggscores['fluency'].append(count_ratings_aggregate(flu_scores, num_categories))\n",
        "  return scores_matrix_dico_7scores, scores_matrix_dico_Aggscores, num_annot\n",
        "\n",
        "# How many categories do we want for the aggregated scores?\n",
        "scores_matrix_dico_7scores, scores_matrix_dico_Aggscores, num_annot = makeFleissMatrix(path_json, num_agg_categories)\n",
        "\n",
        "print('Fleiss with 7 scores\\n----------------')\n",
        "for qualityCriterion in scores_matrix_dico_7scores:\n",
        "  print(qualityCriterion)\n",
        "  fleissKappa(scores_matrix_dico_7scores[qualityCriterion], num_annot)\n",
        "\n",
        "print('\\nFleiss with aggregated scores\\n----------------')\n",
        "for qualityCriterion in scores_matrix_dico_Aggscores:\n",
        "  print(qualityCriterion)\n",
        "  fleissKappa(scores_matrix_dico_Aggscores[qualityCriterion], num_annot)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BruYhh-FhPDe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6J6QSDw8EkD0",
        "eBhJJ9MYeRFe",
        "VB_Lq9pFfRgN",
        "N4Pc-zkSRkYa",
        "FacsCjD-5JzP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}