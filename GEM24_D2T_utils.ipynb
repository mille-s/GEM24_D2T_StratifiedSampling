{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_D2T_StratifiedSampling/blob/main/GEM24_D2T_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D2T data selection"
      ],
      "metadata": {
        "id": "NpKJM2gx_ELW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hyry7e09e7P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Importing required libraries\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WudT_LOiIoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Filepath definition (upload file in TESTDATA folder after running!)\n",
        "\n",
        "# project_dir_path = os.path.join('/', 'content', 'drive', 'MyDrive', 'WebNLG_data_selection')\n",
        "project_dir_path = '/content'\n",
        "rdf_path = os.path.join(project_dir_path, 'testdata')\n",
        "output_path = os.path.join(rdf_path, 'output_data.csv')\n",
        "\n",
        "if not os.path.exists(rdf_path):\n",
        "  os.makedirs(rdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhuUak3C3Arj"
      },
      "outputs": [],
      "source": [
        "seed = 49"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOxH6Hr9kcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function for sampling\n",
        "\n",
        "def extract_data(rdf_filepath, stratify_categories, exclude_size):\n",
        "\n",
        "  '''\n",
        "      This method:\n",
        "      a. extracts the required entries (RDF triple(s), number of triples, property and category) from the json file.\n",
        "      b. categorizes the triple and verbalisation pair as seen/unseen category based on its presence in the training set.\n",
        "      c. groups the required extracted entry field (in this case, number of triples and property) for stratified selection.\n",
        "  '''\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "  for filename in os.listdir(rdf_filepath):\n",
        "    if '.xml' in filename:\n",
        "      tree = ET.parse(f\"{rdf_filepath}/{filename}\")\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # extract triples\n",
        "      for entry in root.findall('./entries/entry'):\n",
        "        triples = []\n",
        "        pred = []\n",
        "        for triple in entry.find('modifiedtripleset').findall('mtriple'):\n",
        "          str_triple = triple.text\n",
        "          triples.append(str_triple)\n",
        "          only_pred = str_triple.split('|')[1]\n",
        "          pred.append(only_pred)\n",
        "        if exclude_size == 'none' or (exclude_size == '1 only' and int(entry.attrib['size']) > 1) or (exclude_size == '1 and 2' and int(entry.attrib['size']) > 2):\n",
        "          curr_entry = {\n",
        "              'id': count,\n",
        "              'triples': triples.copy(),\n",
        "              'property': pred.copy(),\n",
        "              'num_triples': int(entry.attrib['size']),\n",
        "              'category': 'unseen' if entry.attrib['category'] in ['Athlete', 'Artist', 'CelestialBody', 'MeanOfTransportation', 'Politician'] else 'seen',\n",
        "              'category_all': entry.attrib['category']\n",
        "          }\n",
        "          if stratify_categories == 'seenUnseen':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category']\n",
        "          elif stratify_categories == 'allCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category_all']\n",
        "          elif stratify_categories == 'ignoreCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])\n",
        "          data.append(curr_entry)\n",
        "          count += 1\n",
        "\n",
        "  # Remove data points for which there is only one member in a stratify category (triggers an error when stratifying, needs 2 members min)\n",
        "  clean_data = []\n",
        "  # Make a dico with the count of instances of each strat_field\n",
        "  count_strat_field_instances = {}\n",
        "  for datapoint in data:\n",
        "    if datapoint['strat_field'] in count_strat_field_instances:\n",
        "      count_strat_field_instances[datapoint['strat_field']] += 1\n",
        "    else:\n",
        "      count_strat_field_instances[datapoint['strat_field']] = 1\n",
        "  # If a count of a strat_field is one, do no include it in the final dataset\n",
        "  for datapoint_clean in data:\n",
        "    if count_strat_field_instances[datapoint_clean['strat_field']] == 1:\n",
        "      print(f\"  Removed datapoint  {datapoint_clean['strat_field']} because there is only one member!\")\n",
        "    else:\n",
        "      clean_data.append(datapoint_clean)\n",
        "\n",
        "  return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idE1moVjJL3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling parameters\n",
        "\n",
        "stratify_categories = 'ignoreCategories'#@param['allCategories', 'seenUnseen', 'ignoreCategories']\n",
        "number_samples = \"180\"#@param[50, 100, 120, 150, 175, 180, 200, 300, 400, 500]\n",
        "num_samples = int(number_samples)\n",
        "exclude_size = '1 only'#@param['none', '1 only', '1 and 2']\n",
        "# Get data\n",
        "data=extract_data(rdf_path, stratify_categories, exclude_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN3pAE5N-ti-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Stratified selection using train_test_split\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "X_train, X_test, = train_test_split(tset, test_size=num_samples, random_state=seed, stratify=tset['strat_field'], shuffle=True)\n",
        "print(len(X_train), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Balanced selection using groupby\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# I found three ways, not sure what they exactly do; the second and third ones allow for specifying the random_state\n",
        "# X_test = tset.groupby(tset['strat_field']).apply(lambda s: s.sample(30))\n",
        "# X_test = tset.sample(frac = 1.0, random_state=seed, axis=0).groupby(tset['strat_field']).head(30)\n",
        "# The third one below seems more controlled; grouby uses axis=0\n",
        "X_test = tset.groupby(by=tset['strat_field']).sample(n = 30, random_state=seed)\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "wjXcRashq9v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print some numbers\n",
        "# tset['num_triples']\n",
        "# len(tset.loc[tset['category'] == 'unseen'])\n",
        "# print(X_test['num_triples'])\n",
        "\n",
        "# Show mean of column that contains triple number in each input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)\n",
        "# print(X_test.loc[:, 'num_triples'].mean())\n",
        "print(f\"{round(X_test['num_triples'].mean(), 2)} triples per input on average\")\n",
        "\n",
        "def count_num_instances(pd_column):\n",
        "  count = {}\n",
        "  for category in pd_column:\n",
        "    if category in count:\n",
        "      count[category] += 1\n",
        "    else:\n",
        "      count[category] = 1\n",
        "\n",
        "  for count_category in sorted(count):\n",
        "    print(f'{count_category}\\t{count[count_category]}')\n",
        "    # print(f'{count[count_category]}')\n",
        "  print('-----------------')\n",
        "\n",
        "count_num_instances(X_test['num_triples'])\n",
        "count_num_instances(X_test['category_all'])"
      ],
      "metadata": {
        "id": "wT8jGcydF28v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play around with groupby\n",
        "df_test_gb = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
        "                              'Parrot', 'Parrot'],\n",
        "                   'Max Speed': [380., 370., 24., 26.]})\n",
        "print(df_test_gb)\n",
        "\n",
        "print(df_test_gb.groupby(['Animal']).mean())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ge6TruTBPHqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoBNN96VZEQp"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTML tables"
      ],
      "metadata": {
        "id": "HrZjTEDs_MDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install packages\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\n",
        "! pip install json2html\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2mKsmkS__4ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from pandas dataframe\n",
        "from json2html import *\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "subtask = 'D2T-1'#@param['D2T-1', 'D2T-2']\n",
        "dataset = 'FI'#@param['FA', 'FI', 'CFA']\n",
        "\n",
        "\n",
        "path_out_pd = '/content/tables_pd'\n",
        "if not os.path.exists(path_out_pd):\n",
        "  os.makedirs(path_out_pd)\n",
        "\n",
        "for ind in X_test.index:\n",
        "  triple_set = X_test['triples'][ind]\n",
        "  with codecs.open(os.path.join(path_out_pd, f'{subtask}-{dataset}_{str(ind).rjust(4, \"0\")}.html'), 'w', 'utf-8') as fo:\n",
        "    list_dico_input = []\n",
        "    for triple in triple_set:\n",
        "      dico_triples = {}\n",
        "      dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "      dico_triples['Property'] = triple.split(' | ')[1]\n",
        "      dico_triples['Object'] = triple.split(' | ')[2]\n",
        "      list_dico_input.append(dico_triples)\n",
        "    fo.write(json2html.convert(json = list_dico_input))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T6yqW7aCRTsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from HuggingFace data\n",
        "from datasets import load_dataset\n",
        "from json2html import *\n",
        "import codecs\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "# struct2text: common_gen, cs_restaurants, dart, e2e_nlg, totto, web_nlg_en, web_nlg_ru\n",
        "# schema_guided_dialog\n",
        "dataset_name = 'web_nlg_en'\n",
        "dataset = load_dataset('gem', dataset_name)\n",
        "# subSets = ['test', 'validation', 'train']\n",
        "subSets = ['test']\n",
        "\n",
        "for subSet in subSets:\n",
        "  x = 0\n",
        "  subSet_inputs = dataset[subSet]\n",
        "  while x < len (subSet_inputs):\n",
        "    # fileName_out = 'out_tables/'+dataset_name+'-'+subSet+'-'+str(x)+'.html'\n",
        "    fileName_out = 'tables_/'+subSet_inputs[x]['gem_id']+'.html'\n",
        "    if not os.path.exists('tables_'):\n",
        "      os.makedirs('tables_')\n",
        "    print('Processing '+fileName_out)\n",
        "    fo = codecs.open(fileName_out, 'w', 'utf-8')\n",
        "    list_dico_input = []\n",
        "    if dataset_name == 'web_nlg_en' or dataset_name == 'web_nlg_ru':\n",
        "      for triple in subSet_inputs[x]['input']:\n",
        "        dico_triples = {}\n",
        "        dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "        dico_triples['Property'] = triple.split(' | ')[1]\n",
        "        dico_triples['Object'] = triple.split(' | ')[2]\n",
        "        list_dico_input.append(dico_triples)\n",
        "    elif dataset_name == 'common_gen':\n",
        "      dico_concepts = {}\n",
        "      dico_concepts['Concepts'] = subSet_inputs[x]['concepts']\n",
        "      list_dico_input.append(dico_concepts)\n",
        "    elif dataset_name == 'cs_restaurants':\n",
        "      dico_DAs = {}\n",
        "      DA = subSet_inputs[x]['dialog_act'].split('(')[0]\n",
        "      triples = subSet_inputs[x]['dialog_act'].split('(')[1].split(')')[0]\n",
        "      dico_DAs['Dialogue Act'] = DA\n",
        "      if re.search(',', triples):\n",
        "        dico_DAs['Topics'] = triples.split(',')\n",
        "      else:\n",
        "        dico_DAs['Topic'] = triples\n",
        "      list_dico_input.append(dico_DAs)\n",
        "    elif dataset_name == 'e2e_nlg':\n",
        "      list_properties = subSet_inputs[x]['meaning_representation'].split(', ')\n",
        "      for input_property in list_properties:\n",
        "        dico_properties = {}\n",
        "        prop_name = input_property.split('[')[0]\n",
        "        prop_value = input_property.split('[')[1].split(']')[0]\n",
        "        dico_properties['Property'] = prop_name\n",
        "        dico_properties['Value'] = prop_value\n",
        "        list_dico_input.append(dico_properties)\n",
        "    elif dataset_name == 'schema_guided_dialog':\n",
        "      dico_DAs = {}\n",
        "      dico_DAs['Dialogue Acts'] = subSet_inputs[x]['dialog_acts']\n",
        "      list_dico_input.append(dico_DAs)\n",
        "    fo.write(json2html.convert(json = list_dico_input))\n",
        "    fo.close()\n",
        "    x += 1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "b25TnJS9_gWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}