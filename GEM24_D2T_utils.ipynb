{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/GEM24_D2T_StratifiedSampling/blob/main/GEM24_D2T_utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - D2T data selection"
      ],
      "metadata": {
        "id": "NpKJM2gx_ELW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare repo"
      ],
      "metadata": {
        "id": "imLY7mxmOvjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install packages\n",
        "\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\n",
        "! pip install json2html\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DxwHcdRaLqEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hyry7e09e7P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set parameters"
      ],
      "metadata": {
        "id": "oMZuDfUnTEA9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WudT_LOiIoN",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Filepath definition (upload file(s) in TESTDATA folder after running!)\n",
        "\n",
        "# project_dir_path = os.path.join('/', 'content', 'drive', 'MyDrive', 'WebNLG_data_selection')\n",
        "project_dir_path = '/content'\n",
        "rdf_path = os.path.join(project_dir_path, 'testdata')\n",
        "csv_path = os.path.join(project_dir_path, 'csv_sampling')\n",
        "\n",
        "subtask = 'D2T-1'#@param['D2T-1', 'D2T-2']\n",
        "dataset = 'CFA'#@param['CFA', 'FA', 'FI']\n",
        "# seed used for GEM'24: 49\n",
        "seed = 49#@param\n",
        "seed = int(seed)\n",
        "datacode = subtask+'-'+dataset\n",
        "\n",
        "output_path = os.path.join(csv_path, datacode+'_samplingData.csv')\n",
        "\n",
        "if not os.path.exists(rdf_path):\n",
        "  os.makedirs(rdf_path)\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "  os.makedirs(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create csv file with sampling info. Run once for each file."
      ],
      "metadata": {
        "id": "Jo3-O0qFO0yV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfOxH6Hr9kcf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Function for sampling.\n",
        "\n",
        "def extract_data(rdf_filepath, stratify_categories, exclude_size):\n",
        "\n",
        "  '''\n",
        "      This method:\n",
        "      a. extracts the required entries (RDF triple(s), number of triples, property and category) from the json file.\n",
        "      b. categorizes the triple and verbalisation pair as seen/unseen category based on its presence in the training set.\n",
        "      c. groups the required extracted entry field (in this case, number of triples and property) for stratified selection.\n",
        "  '''\n",
        "\n",
        "  data = []\n",
        "  count = 0\n",
        "  original_id = 1\n",
        "  for filename in os.listdir(rdf_filepath):\n",
        "    if '.xml' in filename and datacode in filename:\n",
        "      tree = ET.parse(f\"{rdf_filepath}/{filename}\")\n",
        "      root = tree.getroot()\n",
        "\n",
        "      # extract triples\n",
        "      for entry in root.findall('./entries/entry'):\n",
        "        triples = []\n",
        "        pred = []\n",
        "        for triple in entry.find('modifiedtripleset').findall('mtriple'):\n",
        "          str_triple = triple.text\n",
        "          triples.append(str_triple)\n",
        "          only_pred = str_triple.split('|')[1]\n",
        "          pred.append(only_pred)\n",
        "        if exclude_size == 'none' or (exclude_size == '1 only' and int(entry.attrib['size']) > 1) or (exclude_size == '1 and 2' and int(entry.attrib['size']) > 2):\n",
        "          curr_entry = {\n",
        "              'id': count,\n",
        "              'original_id': original_id,\n",
        "              'triples': triples.copy(),\n",
        "              'property': pred.copy(),\n",
        "              'num_triples': int(entry.attrib['size']),\n",
        "              'category': 'unseen' if entry.attrib['category'] in ['Athlete', 'Artist', 'CelestialBody', 'MeanOfTransportation', 'Politician'] else 'seen',\n",
        "              'category_all': entry.attrib['category']\n",
        "          }\n",
        "          if stratify_categories == 'seenUnseen':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category']\n",
        "          elif stratify_categories == 'allCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])+curr_entry['category_all']\n",
        "          elif stratify_categories == 'ignoreCategories':\n",
        "            curr_entry['strat_field'] = str(curr_entry['num_triples'])\n",
        "          data.append(curr_entry)\n",
        "          count += 1\n",
        "        original_id += 1\n",
        "\n",
        "  # Remove data points for which there is only one member in a stratify category (triggers an error when stratifying, needs 2 members min)\n",
        "  clean_data = []\n",
        "  # Make a dico with the count of instances of each strat_field\n",
        "  count_strat_field_instances = {}\n",
        "  for datapoint in data:\n",
        "    if datapoint['strat_field'] in count_strat_field_instances:\n",
        "      count_strat_field_instances[datapoint['strat_field']] += 1\n",
        "    else:\n",
        "      count_strat_field_instances[datapoint['strat_field']] = 1\n",
        "  # If a count of a strat_field is one, do no include it in the final dataset\n",
        "  for datapoint_clean in data:\n",
        "    if count_strat_field_instances[datapoint_clean['strat_field']] == 1:\n",
        "      print(f\"  Removed datapoint  {datapoint_clean['strat_field']} because there is only one member!\")\n",
        "    else:\n",
        "      clean_data.append(datapoint_clean)\n",
        "\n",
        "  return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8idE1moVjJL3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling parameters\n",
        "\n",
        "stratify_categories = 'ignoreCategories'#@param['allCategories', 'seenUnseen', 'ignoreCategories']\n",
        "number_samples = \"180\"#@param[50, 100, 120, 150, 175, 180, 200, 300, 400, 500]\n",
        "num_samples = int(number_samples)\n",
        "exclude_size = '1 only'#@param['none', '1 only', '1 and 2']\n",
        "# Get data\n",
        "data=extract_data(rdf_path, stratify_categories, exclude_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN3pAE5N-ti-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Stratified selection using train_test_split\n",
        "\n",
        "# tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# X_train, X_test, = train_test_split(tset, test_size=num_samples, random_state=seed, stratify=tset['strat_field'], shuffle=True)\n",
        "# print(len(X_train), len(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Balanced selection using groupby\n",
        "\n",
        "tset = pd.DataFrame.from_dict(data)\n",
        "\n",
        "# I found three ways, not sure what they exactly do; the second and third ones allow for specifying the random_state\n",
        "# X_test = tset.groupby(tset['strat_field']).apply(lambda s: s.sample(30))\n",
        "# X_test = tset.sample(frac = 1.0, random_state=seed, axis=0).groupby(tset['strat_field']).head(30)\n",
        "# The third one below seems more controlled; grouby uses axis=0\n",
        "X_test = tset.groupby(by=tset['strat_field']).sample(n = 30, random_state=seed)\n",
        "\n",
        "print(X_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wjXcRashq9v2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print some numbers\n",
        "# tset['num_triples']\n",
        "# len(tset.loc[tset['category'] == 'unseen'])\n",
        "# print(X_test['num_triples'])\n",
        "\n",
        "# Show mean of column that contains triple number in each input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)\n",
        "# print(X_test.loc[:, 'num_triples'].mean())\n",
        "print(f\"{round(X_test['num_triples'].mean(), 2)} triples per input on average\")\n",
        "\n",
        "def count_num_instances(pd_column):\n",
        "  count = {}\n",
        "  for category in pd_column:\n",
        "    if category in count:\n",
        "      count[category] += 1\n",
        "    else:\n",
        "      count[category] = 1\n",
        "\n",
        "  for count_category in sorted(count):\n",
        "    print(f'{count_category}\\t{count[count_category]}')\n",
        "    # print(f'{count[count_category]}')\n",
        "  print('-----------------')\n",
        "\n",
        "count_num_instances(X_test['num_triples'])\n",
        "count_num_instances(X_test['category_all'])"
      ],
      "metadata": {
        "id": "wT8jGcydF28v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Play around with groupby\n",
        "# df_test_gb = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
        "#                               'Parrot', 'Parrot'],\n",
        "#                    'Max Speed': [380., 370., 24., 26.]})\n",
        "# print(df_test_gb)\n",
        "\n",
        "# print(df_test_gb.groupby(['Animal']).mean())"
      ],
      "metadata": {
        "id": "ge6TruTBPHqt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoBNN96VZEQp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create CSV file\n",
        "X_test.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Create HTML tables for inputs. Run once for each file."
      ],
      "metadata": {
        "id": "HrZjTEDs_MDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from pandas dataframe (running the code above before)\n",
        "from json2html import *\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "path_out_pd = '/content/tables_pd'\n",
        "if not os.path.exists(path_out_pd):\n",
        "  os.makedirs(path_out_pd)\n",
        "\n",
        "for ind in X_test.index:\n",
        "  triple_set = X_test['triples'][ind]\n",
        "  orig_id = X_test['original_id'][ind]\n",
        "  # orig_id starts numbering at 1, while the lists of outputs texts will start numbering at 0, so we need to remove 1 from the original ID to maintain alignment with output files lines\n",
        "  with codecs.open(os.path.join(path_out_pd, f'{datacode}_{str(orig_id-1).rjust(4, \"0\")}.html'), 'w', 'utf-8') as fo:\n",
        "    list_dico_input = []\n",
        "    for triple in triple_set:\n",
        "      dico_triples = {}\n",
        "      dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "      dico_triples['Property'] = triple.split(' | ')[1]\n",
        "      dico_triples['Object'] = triple.split(' | ')[2]\n",
        "      list_dico_input.append(dico_triples)\n",
        "    fo.write(json2html.convert(json = list_dico_input))"
      ],
      "metadata": {
        "id": "T6yqW7aCRTsd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate HTML tables from HuggingFace data\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# from json2html import *\n",
        "# import codecs\n",
        "# import os\n",
        "# import re\n",
        "# import json\n",
        "\n",
        "# # struct2text: common_gen, cs_restaurants, dart, e2e_nlg, totto, web_nlg_en, web_nlg_ru\n",
        "# # schema_guided_dialog\n",
        "# dataset_name = 'web_nlg_en'\n",
        "# dataset = load_dataset('gem', dataset_name)\n",
        "# # subSets = ['test', 'validation', 'train']\n",
        "# subSets = ['test']\n",
        "\n",
        "# for subSet in subSets:\n",
        "#   x = 0\n",
        "#   subSet_inputs = dataset[subSet]\n",
        "#   while x < len (subSet_inputs):\n",
        "#     # fileName_out = 'out_tables/'+dataset_name+'-'+subSet+'-'+str(x)+'.html'\n",
        "#     fileName_out = 'tables_/'+subSet_inputs[x]['gem_id']+'.html'\n",
        "#     if not os.path.exists('tables_'):\n",
        "#       os.makedirs('tables_')\n",
        "#     print('Processing '+fileName_out)\n",
        "#     fo = codecs.open(fileName_out, 'w', 'utf-8')\n",
        "#     list_dico_input = []\n",
        "#     if dataset_name == 'web_nlg_en' or dataset_name == 'web_nlg_ru':\n",
        "#       for triple in subSet_inputs[x]['input']:\n",
        "#         dico_triples = {}\n",
        "#         dico_triples['Subject'] = triple.split(' | ')[0]\n",
        "#         dico_triples['Property'] = triple.split(' | ')[1]\n",
        "#         dico_triples['Object'] = triple.split(' | ')[2]\n",
        "#         list_dico_input.append(dico_triples)\n",
        "#     elif dataset_name == 'common_gen':\n",
        "#       dico_concepts = {}\n",
        "#       dico_concepts['Concepts'] = subSet_inputs[x]['concepts']\n",
        "#       list_dico_input.append(dico_concepts)\n",
        "#     elif dataset_name == 'cs_restaurants':\n",
        "#       dico_DAs = {}\n",
        "#       DA = subSet_inputs[x]['dialog_act'].split('(')[0]\n",
        "#       triples = subSet_inputs[x]['dialog_act'].split('(')[1].split(')')[0]\n",
        "#       dico_DAs['Dialogue Act'] = DA\n",
        "#       if re.search(',', triples):\n",
        "#         dico_DAs['Topics'] = triples.split(',')\n",
        "#       else:\n",
        "#         dico_DAs['Topic'] = triples\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     elif dataset_name == 'e2e_nlg':\n",
        "#       list_properties = subSet_inputs[x]['meaning_representation'].split(', ')\n",
        "#       for input_property in list_properties:\n",
        "#         dico_properties = {}\n",
        "#         prop_name = input_property.split('[')[0]\n",
        "#         prop_value = input_property.split('[')[1].split(']')[0]\n",
        "#         dico_properties['Property'] = prop_name\n",
        "#         dico_properties['Value'] = prop_value\n",
        "#         list_dico_input.append(dico_properties)\n",
        "#     elif dataset_name == 'schema_guided_dialog':\n",
        "#       dico_DAs = {}\n",
        "#       dico_DAs['Dialogue Acts'] = subSet_inputs[x]['dialog_acts']\n",
        "#       list_dico_input.append(dico_DAs)\n",
        "#     fo.write(json2html.convert(json = list_dico_input))\n",
        "#     fo.close()\n",
        "#     x += 1"
      ],
      "metadata": {
        "id": "b25TnJS9_gWU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download files"
      ],
      "metadata": {
        "id": "6nYTNyqSQ2X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download tables\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/html_tables.zip'\n",
        "!zip -r {zip_name_inter} /content/tables_pd\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "pRSvuVPY4iKG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download CSVs\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/CSVs.zip'\n",
        "!zip -r {zip_name_inter} /content/csv_sampling\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "id": "JNnuk92B41tu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check the contents of the created CSVs\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob(os.path.join(csv_path, '*.csv'))\n",
        "\n",
        "dico_properties = {}\n",
        "\n",
        "for csv_file in sorted(csv_files):\n",
        "  props_unique = []\n",
        "  count_props_all = 0\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  # print(tail)\n",
        "  df_triple_sets = pd.read_csv(csv_file)['triples']\n",
        "  # print(df_triple_sets)\n",
        "  for triple_set in df_triple_sets:\n",
        "    triples_list = triple_set.replace('\"', \"'\").replace(\"['\", \"\").replace(\"']\", \"\").split(\"', '\")\n",
        "    properties_list = [triple.split(' | ')[1] for triple in triples_list]\n",
        "    for triple_property in properties_list:\n",
        "      count_props_all += 1\n",
        "      if triple_property not in props_unique:\n",
        "        props_unique.append(triple_property)\n",
        "  dico_properties[tail] = props_unique\n",
        "  # print(f'  {count_props_all} properties found (810 expected).')\n",
        "\n",
        "for dataset in dico_properties:\n",
        "  print(dataset, ':', len(dico_properties[dataset]), 'different properties found:')\n",
        "  print('  ', sorted(dico_properties[dataset]))"
      ],
      "metadata": {
        "id": "fKc0BNTSNX-Q",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Create individual files for sampled system outputs\n",
        "\n",
        "First upload all system outputs in a folder named \"sys_outputs\", and generate the corresponding csv file(s) with the code above (or upload manually in a folder called \"csv_sampling\")."
      ],
      "metadata": {
        "id": "DSxbJ9sfV_cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one text file per sampled input/output per team per language per test set\n",
        "import os\n",
        "import glob\n",
        "from pandas import *\n",
        "import codecs\n",
        "\n",
        "out_sampled_folder = '/content/d2t_outputs-sampled'\n",
        "\n",
        "# The 3 CSV files for each task have the same IDs sampled, so we can just use one file per task\n",
        "list_csv_D2T1_OIDs = read_csv('/content/csv_sampling/D2T-1-FA_samplingData.csv')['original_id'].tolist()\n",
        "list_csv_D2T2_OIDs = read_csv('/content/csv_sampling/D2T-2-FA_samplingData.csv')['original_id'].tolist()\n",
        "\n",
        "# The original IDs are numbered starting from 1, we want a number starting from 0 to aling with list indices in the system output files; make it into a dic for easy access afterwards\n",
        "list_csv_ids = {}\n",
        "list_csv_ids['D2T-1'] = [OID1-1 for OID1 in sorted(list_csv_D2T1_OIDs)]\n",
        "list_csv_ids['D2T-2'] = [OID2-1 for OID2 in sorted(list_csv_D2T2_OIDs)]\n",
        "\n",
        "for sys_output_path in glob.glob(os.path.join('/content/sys_outputs', '*.txt')):\n",
        "  head, tail = os.path.split(sys_output_path)\n",
        "  # Get parameters of every output file\n",
        "  team_ID = tail.split('-', 1)[0]\n",
        "  lang_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[1]\n",
        "  data_code_out = tail.rsplit('.', 1)[0].rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  subtask_code_out = data_code_out.rsplit('-', 1)[0]\n",
        "  # print(tail)\n",
        "  # print(f'  {team_ID}')\n",
        "  # print(f'  {lang_out}')\n",
        "  # print(f'  {data_code_out}')\n",
        "  # print(f'  {subtask_code_out}')\n",
        "\n",
        "  # Create subfolder to store sampled system outputs\n",
        "  dest_folder_sample = os.path.join(out_sampled_folder, data_code_out, lang_out, team_ID)\n",
        "  if not os.path.exists(dest_folder_sample):\n",
        "    os.makedirs(dest_folder_sample)\n",
        "\n",
        "  # Read sys output\n",
        "  sys_output_all_lines = codecs.open(sys_output_path).readlines()\n",
        "\n",
        "  for id_sampled in list_csv_ids[subtask_code_out]:\n",
        "    # Create text files the last part of the name of which matches the name of the sampled input files\n",
        "    dest_filename_sample = os.path.join(dest_folder_sample, '['+team_ID+'_'+lang_out+']_'+data_code_out+'_'+str(id_sampled).rjust(4, \"0\")+'.txt')\n",
        "    with codecs.open(dest_filename_sample, 'w', 'utf-8') as fo:\n",
        "        fo.write(sys_output_all_lines[id_sampled].strip())\n"
      ],
      "metadata": {
        "id": "vvjhq5OkV9T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download sampled output text files\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "zip_name_inter = '/content/d2t_outputs-sampled.zip'\n",
        "!zip -r {zip_name_inter} /content/d2t_outputs-sampled\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lJ-P6XxXAMMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Create individual files for reference texts collected on AMT"
      ],
      "metadata": {
        "id": "6J6QSDw8EkD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read json and check texts (UPLOAD FIRST)\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# First upload '/content/en_references_v2-formatted.json' and '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Load json\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "# Unzip sampled output folder, which contains lists of IDs of sampled outputs\n",
        "if not os.path.exists('/content/content/d2t_outputs-sampled_grouped'):\n",
        "  ! unzip '/content/csv_sampling/d2t_outputs-sampled-onlyWebnlgRef_grouped_checked.zip'\n",
        "\n",
        "# Get list of IDs for all sampled outputs\n",
        "list_IDs_sampled = []\n",
        "for dataset_path in glob.glob(os.path.join('/content/content/d2t_outputs-sampled_grouped', '*')):\n",
        "  head, tail = os.path.split(dataset_path)\n",
        "  # print(tail)\n",
        "  path_IDs = os.path.join(dataset_path, 'en', 'AllSizes', 'AllSizes_sampled_ids.txt')\n",
        "  # Open IDs file, read, and split the list on the first line\n",
        "  datasets_IDs_sampled = codecs.open(path_IDs, 'r', 'utf-8').readlines()[0].strip().replace('[', '').replace(']', '').replace(\"'\", '').split(', ')\n",
        "  for dataset_ID_sampled in datasets_IDs_sampled:\n",
        "    full_ID = f'en_{tail}_{dataset_ID_sampled}'\n",
        "    list_IDs_sampled.append(full_ID)\n",
        "# print(len(list_IDs_sampled))\n",
        "# print(list_IDs_sampled)\n",
        "\n",
        "ids_error = []\n",
        "all_ids_collected = []\n",
        "# Find some errors\n",
        "with codecs.open('/content/writing_problems.txt', 'w', 'utf-8') as fo1:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    all_ids_collected.append(id)\n",
        "    # Capture people who pasted some definitions\n",
        "    if re.search('\\[', text):\n",
        "      fo1.write(f'{id} (pasted definition): {text}\\n')\n",
        "      print(f'{id} (pasted definition): {text}')\n",
        "      ids_error.append(id)\n",
        "    # Capture people who pasted the input\n",
        "    elif re.search('Subject', text):\n",
        "      fo1.write(f'{id} (pasted input): {text}\\n')\n",
        "      print(f'{id} (pasted input): {text}')\n",
        "      ids_error.append(id)\n",
        "\n",
        "  # Find missing files\n",
        "  for ID_sampled in list_IDs_sampled:\n",
        "    if ID_sampled not in all_ids_collected:\n",
        "      fo1.write(f'{ID_sampled} (missing)\\n')\n",
        "      print(f'{ID_sampled} (missing)')\n",
        "\n",
        "if len(ids_error) == 0:\n",
        "  print('No errors found!')\n",
        "\n",
        "ids_seen = []\n",
        "texts_seen = []\n",
        "with codecs.open('/content/writing_duplicates.txt', 'w', 'utf-8') as fo2:\n",
        "  for reference_text in references_json[\"references\"]:\n",
        "    id = reference_text[\"short_id\"]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    # Check if there is a duplicate for a text with error\n",
        "    if id in ids_error:\n",
        "      print(f'Found alternative to error {id}')\n",
        "    # Capture duplicate outputs\n",
        "    if id not in ids_seen:\n",
        "      ids_seen.append(id)\n",
        "      texts_seen.append(text)\n",
        "    else:\n",
        "      index_id = ids_seen.index(id)\n",
        "      fo2.write(f'{id} (duplicate): {texts_seen[index_id]}\\n')\n",
        "      # print(f'{id} (duplicate): {texts_seen[index_id]}')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kmsoaZqBEvZP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create files\n",
        "import json\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "# First upload '/content/en_references-formatted.json'\n",
        "\n",
        "# Create folders for text files\n",
        "path_collected_references = os.path.join('/content', 'texts_AMT')\n",
        "if not os.path.exists(path_collected_references):\n",
        "  os.makedirs(path_collected_references)\n",
        "\n",
        "# Load json\n",
        "ids_seen = []\n",
        "references_json = json.load(open('/content/en_references_v2-formatted.json'))\n",
        "for reference_text in references_json[\"references\"]:\n",
        "  id = reference_text[\"short_id\"]\n",
        "  # We only take the first text available\n",
        "  if id not in ids_seen:\n",
        "    ids_seen.append(id)\n",
        "    language = id.split('_', 1)[0]\n",
        "    id_nolang = id.split('_', 1)[1]\n",
        "    text = reference_text[\"english_output\"].strip().replace('\\r\\n', ' ')\n",
        "    filename = os.path.join(path_collected_references, f'[0_{language}]_{id_nolang}.txt')\n",
        "    with codecs.open(filename, 'w', 'utf-8') as fo:\n",
        "      fo.write(text)\n",
        "print(f'{str(len(ids_seen))} files were created (1,080 expected.)')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MYarTMTqapUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download texts\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "zip_name_inter = '/content/texts_AMT.zip'\n",
        "!zip -r {zip_name_inter} /content/texts_AMT\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XdJpx2r0foaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Create files for automatic evaluation by size (sizes 2 to 7 and all sizes together)"
      ],
      "metadata": {
        "id": "eBhJJ9MYeRFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Group references and system outputs by size\n",
        "from posix import write\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import shutil\n",
        "\n",
        "language_out = 'en' #['en', 'es', 'sw']\n",
        "teams_withdrawn = ['8']\n",
        "# First upload '/content/csv_sampling/d2t_inputs-sampled.zip' and /content/csv_sampling/d2t_outputs-sampled.zip\n",
        "\n",
        "folder_suffix = '-enAmtRefs' #param['-onlyWebnlgRef', '-enAmtRefs', '-enAmtRefs_secondary']\n",
        "outputs_sampled_folder= '/content/csv_sampling/d2t_outputs-sampled'+folder_suffix+'.zip'\n",
        "inputs_sampled_folder= '/content/csv_sampling/d2t_inputs-sampled.zip'\n",
        "\n",
        "# Unzip sampled input and output files if they have not been unzipped already:\n",
        "if not os.path.exists('/content/d2t_inputs-sampled'):\n",
        "  ! unzip {outputs_sampled_folder}\n",
        "  ! unzip {inputs_sampled_folder}\n",
        "\n",
        "# Delete /content/d2t_outputs-sampled_grouped folder\n",
        "if os.path.exists('/content/d2t_outputs-sampled'+folder_suffix+'_grouped'):\n",
        "  shutil.rmtree('/content/d2t_outputs-sampled'+folder_suffix+'_grouped')\n",
        "\n",
        "def writeFile(file_to_write, content, total_num_lines, current_num_lines):\n",
        "    file_to_write.write(content)\n",
        "    # After all lines except the last one, insert linebreak\n",
        "    if current_num_lines < total_num_lines-1:\n",
        "      file_to_write.write('\\n')\n",
        "\n",
        "# Create new output folders for storing files with grouped outputs for each team\n",
        "subfolders_out = sorted([os.path.join(f.path, language_out) for f in os.scandir('/content/d2t_outputs-sampled'+folder_suffix+'/d2t_outputs-sampled') if f.is_dir()])\n",
        "# print(subfolders_out)\n",
        "subfolders_out_grouped = []\n",
        "for subfolder_out in subfolders_out:\n",
        "  # '/content/d2t_outputs-sampled-onlyWebnlgRef/d2t_outputs-sampled/D2T-1-CFA/en'\n",
        "  new_folder_name = re.sub('d2t_outputs-sampled'+folder_suffix, 'd2t_outputs-sampled'+folder_suffix+'_grouped', subfolder_out)\n",
        "  subfolders_out_grouped.append(new_folder_name)\n",
        "  if not os.path.exists(new_folder_name):\n",
        "    os.makedirs(new_folder_name)\n",
        "\n",
        "# Read files with sampling info (we only need one file per subtask since all IDs are the same for each subtask)\n",
        "csv_files = glob.glob(os.path.join('/content/d2t_inputs-sampled/info_sampling-csv', '*-FA_samplingData.csv'))\n",
        "\n",
        "# Build a dico with the IDs grouped by size\n",
        "# {'D2T-1': {2: [46, 920, 936, 1423, 1723, 410, 1653, 1589, 762, 1471, 590, 1685, 1217, 1355, 99, 1339, 1079, 1420, 276, 771, 757, 845, 472, 1222, 1072, 582, 1043, 360, 1387, 1113], ...}}\n",
        "dico_ids_per_size = {}\n",
        "for csv_file in sorted(csv_files):\n",
        "  head, tail = os.path.split(csv_file)\n",
        "  task = tail.rsplit('-', 1)[0]\n",
        "  df_triple_sets = pd.read_csv(csv_file)\n",
        "  dico_ids_per_size[task] = {}\n",
        "  for n in range(2, 8):\n",
        "    dico_ids_per_size[task][n] = [df_triple_sets.loc[df_triple_sets['num_triples'] == n, 'original_id'].tolist()][0]\n",
        "# print(dico_ids_per_size)\n",
        "# print(subfolders_out)\n",
        "# print(subfolders_out_grouped)\n",
        "\n",
        "# subfolder_out is something like /content/d2t_outputs-sampled/D2T-1-CFA/en\n",
        "for subfolder_out, subfolder_out_grouped in zip(subfolders_out, subfolders_out_grouped):\n",
        "  list_sampled_IDs = []\n",
        "  # Get folders inside subfolder_out\n",
        "  teams_out_folders = sorted([f.path for f in os.scandir(subfolder_out) if f.is_dir() and f.name not in teams_withdrawn])\n",
        "  # print(teams_out_folders)\n",
        "  # For each team out folder, check if the path contains one of the keys in dico_ids_per_size\n",
        "  # team_out_folder is something like /content/d2t_outputs-sampled/D2T-1-CFA/en/1\n",
        "  for team_out_folder in teams_out_folders:\n",
        "    num_of_IDs_total = 0\n",
        "    head_tf, team_id = os.path.split(team_out_folder)\n",
        "    print(team_out_folder)\n",
        "    # There are 2 subtasks in dico_ids_per_size: D2T-1 and D2T-2\n",
        "    for task in dico_ids_per_size.keys():\n",
        "      # Make sure we are in the output folder of the right subtask\n",
        "      if task in team_out_folder:\n",
        "        # print(f'  {task} found in {team_out_folder}')\n",
        "        # Within each subtask, we want to create separate files for each input size\n",
        "        for size in dico_ids_per_size[task]:\n",
        "          # print(f'  NumOfIDs: {num_of_IDs_total}')\n",
        "          print(f'    {size}: {dico_ids_per_size[task][size]}')\n",
        "          # Create a folder for each size (to run the eval later on with my notebook, I need to process all hypotheses for one (set of) reference file(s)).\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'Size'+str(size))):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'Size'+str(size)))\n",
        "          # Create a folder where we'll group all the sizes for each system in one file\n",
        "          if not os.path.exists(os.path.join(subfolder_out_grouped, 'AllSizes')):\n",
        "            os.makedirs(os.path.join(subfolder_out_grouped, 'AllSizes'))\n",
        "\n",
        "          # Get a padded version of all IDs in dico_ids_per_size[task][size]\n",
        "          for num_of_IDs_for_a_size, id in enumerate(dico_ids_per_size[task][size]):\n",
        "            # We need to subtract 1 to the original ID to match the ID of the files, for which the numbering starts from 0\n",
        "            padded_aligned_id = str(id-1).zfill(4)\n",
        "            # Now iterate over all the output files of the current folder\n",
        "            for team_out_file_path in glob.glob(os.path.join(team_out_folder, '*.txt')):\n",
        "              head_tf, tail_tf = os.path.split(team_out_file_path)\n",
        "              # Look for files that match one of the IDs of texts of a specific size\n",
        "              if padded_aligned_id in tail_tf:\n",
        "                with codecs.open(team_out_file_path, 'r', 'utf-8') as ftext:\n",
        "                  contents_read = ftext.read()\n",
        "                  # Write separate files for all sizes\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), str(team_id)+'_Size'+str(size)+'.txt'), 'a', 'utf-8') as f_sep:\n",
        "                    writeFile(f_sep, contents_read, len(dico_ids_per_size[task][size]), num_of_IDs_for_a_size)\n",
        "                  # Write one file with all sizes grouped\n",
        "                  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', str(team_id)+'_AllSizes.txt'), 'a', 'utf-8') as f_all:\n",
        "                    writeFile(f_all, contents_read, len(dico_ids_per_size[task][size])*len(dico_ids_per_size[task]), num_of_IDs_total)\n",
        "            num_of_IDs_total += 1\n",
        "\n",
        "          # Now gather the IDs of the inputs that will match the filenames for each dataset and size\n",
        "          padded_aligned_id_list = [str(id-1).zfill(4) for id in dico_ids_per_size[task][size]]\n",
        "          with codecs.open(os.path.join(subfolder_out_grouped, 'Size'+str(size), 'Size'+str(size)+'_sampled_ids.txt'), 'w', 'utf-8') as f_id_sep:\n",
        "            f_id_sep.write(str(sorted(padded_aligned_id_list)))\n",
        "          # Add all the IDs into a list for all sizes\n",
        "          for padded_aligned_id2 in padded_aligned_id_list:\n",
        "            if padded_aligned_id2 not in list_sampled_IDs:\n",
        "              list_sampled_IDs.append(padded_aligned_id2)\n",
        "  with codecs.open(os.path.join(subfolder_out_grouped, 'AllSizes', 'AllSizes_sampled_ids.txt'), 'a', 'utf-8') as f_id_all:\n",
        "    f_id_all.write(str(sorted(list_sampled_IDs)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EC7QaRtTC2qS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download files\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "folder_to_zip = '/content/d2t_outputs-sampled'+folder_suffix+'_grouped'\n",
        "zip_name_inter = folder_to_zip+'.zip'\n",
        "!zip -r {zip_name_inter} {folder_to_zip}\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IMrnK9tDjwci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 - Create plots from automatic evaluation logs"
      ],
      "metadata": {
        "id": "VB_Lq9pFfRgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots per size\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "# First upload the log_eval.zip file (for 1 dataset) obtained with this notebook: https://github.com/mille-s/WebNLG-2020_Metrics\n",
        "\n",
        "dataset = 'D2T-1-CFA' #@param['D2T-1-CFA', 'D2T-1-FA', 'D2T-1-FI', 'D2T-2-CFA', 'D2T-2-FA', 'D2T-2-FI']\n",
        "# Unzip log_eval folder\n",
        "# For when we zip the folders inside the content folder\n",
        "path_logEval = '/content/content/log_eval'\n",
        "path_zip = '/content/log_eval_'+dataset+'.zip'\n",
        "if os.path.exists(path_logEval):\n",
        "  shutil.rmtree(path_logEval)\n",
        "  ! unzip {path_zip}\n",
        "else:\n",
        "  ! unzip {path_zip}\n",
        "\n",
        "# dataset = 'D2T-1-FA'#@param['D2T-1-CFA', 'D2T-1-FA', 'D2T-1-FI', 'D2T-2-CFA', 'D2T-2-FA', 'D2T-2-FI']\n",
        "metrics = ['BLEU', 'METEOR', 'chrF++', 'BERT-SCORE F1']\n",
        "system_IDnames_dico = {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST', '8':'DCU-NLG-Small-noT5', '9':'DCU-NLG-PBN-2nd'}\n",
        "\n",
        "# dico_plot_BLEU = {}\n",
        "# dico_plot_METEOR = {}\n",
        "# dico_plot_chrF = {}\n",
        "# dico_plot_BERT = {}\n",
        "dico_plot_per_size = {}\n",
        "dico_plot_all_sizes = {}\n",
        "eval_files_paths = glob.glob(os.path.join(path_logEval, '*.txt'))\n",
        "for eval_file_path in sorted(eval_files_paths):\n",
        "  head, tail = os.path.split(eval_file_path)\n",
        "  # values for \"size\": 2 to 7, and \"s\" for all sizes\n",
        "  size = tail.rsplit('_', 1)[1].rsplit('.', 1)[0].split('Size')[1]\n",
        "  sys_id = tail.rsplit('_', 1)[0].rsplit('_', 1)[1]\n",
        "  dataset = tail.rsplit('_', 3)[0].rsplit('_', 1)[1]\n",
        "  # print(f'{sys_id}  {size}')\n",
        "\n",
        "  # Make a key in dico_plot with a new system id\n",
        "  # if sys_id not in dico_plot_BLEU:\n",
        "  #   dico_plot_BLEU[sys_id] = []\n",
        "  # if sys_id not in dico_plot_METEOR:\n",
        "  #   dico_plot_METEOR[sys_id] = []\n",
        "  # if sys_id not in dico_plot_chrF:\n",
        "  #   dico_plot_chrF[sys_id] = []\n",
        "  # if sys_id not in dico_plot_BERT:\n",
        "  #   dico_plot_BERT[sys_id] = []\n",
        "\n",
        "  # The scores are on the penultimate line of the log file, the metrics names 2 lines above\n",
        "  log_lines_metrics = [line.rstrip('\\n') for line in open(eval_file_path)][-3]\n",
        "  log_lines_scores = [line.rstrip('\\n') for line in open(eval_file_path)][-1]\n",
        "  # Split lines with names and scores\n",
        "  # Metrics are separated by 4 spaces, with 2 initial spaces; some metrics have a space inside the name\n",
        "  metrics_list = [x for x in log_lines_metrics.split('  ') if not x == '']\n",
        "  # Scores are separated by an unknown number of spaces that depends on the metric name (to show metrics and scores aligned)\n",
        "  scores_list = [x for x in log_lines_scores.split(' ') if not x == '']\n",
        "  # Get the pairs metric/score for the metrics we are interested in\n",
        "  for metric_name, score in zip(metrics_list, scores_list):\n",
        "    if metric_name in metrics:\n",
        "      # print(f'{metric_name}: {score}')\n",
        "      if not size == 's':\n",
        "        if metric_name not in dico_plot_per_size:\n",
        "          dico_plot_per_size[metric_name] = {}\n",
        "        if sys_id not in dico_plot_per_size[metric_name]:\n",
        "          dico_plot_per_size[metric_name][sys_id] = []\n",
        "        dico_plot_per_size[metric_name][sys_id].append(float(score))\n",
        "      else:\n",
        "        # print(f'{sys_id} AllSizes')\n",
        "        # print(f'{metric_name}: {score}')\n",
        "        if sys_id not in dico_plot_all_sizes:\n",
        "          dico_plot_all_sizes[sys_id] = []\n",
        "        dico_plot_all_sizes[sys_id].append(float(score))\n",
        "  # print(metrics_list)\n",
        "  # print(scores_list)\n",
        "\n",
        "\n",
        "# Print the lines to put in a latex table\n",
        "print(f'\\nResults all sizes (order: BLEU, METEOR, chrF++, BERT-SCORE F1):')\n",
        "print(dico_plot_all_sizes)\n",
        "for sys_id in dico_plot_all_sizes:\n",
        "  print(system_IDnames_dico[sys_id]+' & '+' & '.join([str(x) for x in dico_plot_all_sizes[sys_id]])+' \\\\\\\\')\n",
        "\n",
        "print(f'\\nResults per size:')\n",
        "print(dico_plot_per_size)\n",
        "\n",
        "system_colors = {'1':'dodgerblue', '2':'blue', '3':'orange', '4':'green', '5':'red', '6':'purple', '7':'brown', '8':'purple', '9':'purple'}\n",
        "# Make and save plots in png\n",
        "for metric_name in dico_plot_per_size:\n",
        "  for sys_id in dico_plot_per_size[metric_name]:\n",
        "    plt.plot(range(2, len(dico_plot_per_size[metric_name][sys_id]) + 2), dico_plot_per_size[metric_name][sys_id], label=system_IDnames_dico[sys_id], color=system_colors[sys_id])\n",
        "  plt.title(f'{dataset}: {metric_name}')\n",
        "  plt.xlabel(\"Input size\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  plt.legend()\n",
        "  #https://blog.timodenk.com/exporting-matplotlib-plots-to-latex/\n",
        "  # I can't generate the plots inside latex\n",
        "  # plt.savefig(f'{dataset}_{metric_name}.pgf')\n",
        "  # Save the plot as a png file\n",
        "  plt.savefig(f'{dataset}_{metric_name}.png')\n",
        "  # plt.show needs to be after savefig, otherwise the image is blank\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GUt1BfdTfeum",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse log files and create plots for all sizes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metrics order in dico_plot_all_sizes: BLEU, METEOR, chrF++, BERT-SCORE F1\n",
        "metrics_order = ['BLEU', 'METEOR', 'chrF++', 'BERT']\n",
        "metrics_lower_limit = [0, 0.2, 0.4, 0.9]\n",
        "metrics_upper_limit = [40, 0.4, 0.6, 0.95]\n",
        "system_IDShortnames_dico = {'2':'DCU-PBN', '3':'DCU-Sm', '4':'DipInfo', '5':'OSU', '6':'pyrealb', '7':'Saar', '8':'DCU-Sm-noT5', '9':'DCU-PBN-2nd'}\n",
        "teams = [system_IDShortnames_dico[x] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "\n",
        "for metric_id in range(4):\n",
        "  fig, ax = plt.subplots()\n",
        "  bar_labels = teams\n",
        "  bar_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown']\n",
        "  scores = [dico_plot_all_sizes[x][metric_id] for x in dico_plot_all_sizes.keys() if not x == '1']\n",
        "  ax.bar(teams, scores, label=bar_labels, color=bar_colors)\n",
        "  plt.ylim(metrics_lower_limit[metric_id], metrics_upper_limit[metric_id])\n",
        "  ax.set_ylabel('Score')\n",
        "  ax.set_title(metrics_order[metric_id]+'-'+dataset)\n",
        "  # ax.legend(title='Teams')\n",
        "  plt.savefig(f'{dataset}_{metrics_order[metric_id]}_allSys.png')\n",
        "  plt.show()\n",
        "\n",
        "  # Also create a Latex table for the paper\n",
        "\n"
      ],
      "metadata": {
        "id": "4BARvDcHZPr1",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate D2T score differences for each system on the different datasets\n",
        "\n",
        "system_IDShortnames = ['DCU-PBN', 'DCU-Sm', 'DipInfo', 'OSU', 'pyrealb', 'Saar']\n",
        "#BLEU (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bleuScores = [[29.08, 25.2, 26.02, 23.96, 30.34, 20.46], [27.0, 22.98, 20.85, 19.48, 24.9, 16.88], [32.31, 29.01, 28.24, 27.22, 32.01, 21.26], [30.03, 24.45, 21.44, 24.97, 27.06, 16.9], [26.37, 21.67, 21.97, 19.97, 25.05, 16.28], [29.7, 23.48, 20.76, 28.25, 26.47, 20.16]]\n",
        "#METEOR (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "meteorScores = [[0.33, 0.297, 0.322, 0.295, 0.348, 0.3], [0.314, 0.279, 0.292, 0.26, 0.3, 0.267], [0.346, 0.315, 0.342, 0.304, 0.354, 0.307], [0.335, 0.293, 0.306, 0.295, 0.334, 0.282], [0.331, 0.291, 0.31, 0.287, 0.335, 0.286], [0.347, 0.307, 0.331, 0.32, 0.359, 0.315]]\n",
        "#CHRF (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "chrfScores = [[0.555, 0.513, 0.549, 0.49, 0.581, 0.49], [0.537, 0.488, 0.507, 0.438, 0.51, 0.442], [0.58, 0.543, 0.587, 0.512, 0.592, 0.502], [0.566, 0.514, 0.537, 0.496, 0.567, 0.475], [0.551, 0.495, 0.527, 0.479, 0.561, 0.472], [0.581, 0.524, 0.557, 0.538, 0.597, 0.518]]\n",
        "#BERT (Systems 2 to 7): one list of 6 scores per system (scores: 1-FA, 1-CFA, 1-FI, 2-FA, 2-CFA, 2-FI)\n",
        "bertScores = [[0.933, 0.923, 0.92, 0.936, 0.937, 0.924], [0.93, 0.918, 0.914, 0.925, 0.923, 0.914], [0.933, 0.926, 0.924, 0.937, 0.936, 0.923], [0.932, 0.92, 0.915, 0.934, 0.93, 0.917], [0.928, 0.918, 0.917, 0.921, 0.923, 0.916], [0.931, 0.921, 0.917, 0.934, 0.929, 0.919]]\n",
        "\n",
        "def calculateDiffs(list_scores, system_IDShortnames, metric):\n",
        "  print(metric)\n",
        "  for pos, systemScores in enumerate(list_scores):\n",
        "    system_name = system_IDShortnames[pos]\n",
        "    if system_name == 'DCU-Sm':\n",
        "      print(f'  {system_name}')\n",
        "      print(f'    1 FA to CFA: {round(systemScores[1]-systemScores[0], 2)}')\n",
        "      print(f'    1 FA to FI: {round(systemScores[2]-systemScores[0], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    2 FA to CFA: {round(systemScores[4]-systemScores[3], 2)}')\n",
        "      print(f'    2 FA to FI: {round(systemScores[5]-systemScores[3], 2)}')\n",
        "      print(f'    ------')\n",
        "      print(f'    FA 1 to 2: {round(systemScores[3]-systemScores[0], 2)}')\n",
        "      print(f'    CFA 1 to 2: {round(systemScores[4]-systemScores[1], 2)}')\n",
        "      print(f'    FI 1 to 2: {round(systemScores[5]-systemScores[2], 2)}')\n",
        "\n",
        "calculateDiffs(bleuScores, system_IDShortnames, 'BLEU')\n",
        "calculateDiffs(meteorScores, system_IDShortnames, 'METEOR')\n",
        "calculateDiffs(chrfScores, system_IDShortnames, 'chrF++')\n",
        "calculateDiffs(bertScores, system_IDShortnames, 'BERT')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z_I8H9TyF1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "FORGe_input_folder = '/content/wiki-gen-demo/FORGe/DCU_TCD_FORGe_WebNLG23/code'\n",
        "\n",
        "\n",
        "print(FORGe_input_folder.rsplit('/', 1)[0])\n",
        "print(os.path.split(FORGe_input_folder))"
      ],
      "metadata": {
        "id": "I520bR6uDmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 - Qualitative evaluation\n",
        "\n",
        "Choose eval_method and run all cells in this group.\n",
        "This group creates csv files with the following system-level scores for each language (en, es, sw) for each of the 6 D2T subtasks (180 input/output pairs each):\n",
        "\n",
        "(1) Scores of each of the 4 indivual LLMs (1 rating per LLM per input/output pair): /content/sysLevel_scores_perLLM.csv.\n",
        "\n",
        "(2) Average scores of the 4 LLMs (4 ratings per input/output pair): /content/sysLevel_scores_avgLLM.csv.\n",
        "\n",
        "(3) Average human scores (2 human ratings per input/output pair): /content/sysLevel_scores_Humans.csv."
      ],
      "metadata": {
        "id": "N4Pc-zkSRkYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare repo (upload zip with jsons that contain evaluations)\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "eval_method = 'Human'#@param['LLM', 'Human']\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  # Unzip evaluations from Gemini and GPT\n",
        "  if not os.path.exists('/content/LLM-eval_final_jsons'):\n",
        "    assert os.path.exists('/content/LLM-eval_final_jsons.zip'), 'You need to upload LLM-eval_final_jsons.zip'\n",
        "    ! unzip /content/LLM-eval_final_jsons.zip\n",
        "\n",
        "  # Unzip evaluations from DeepSeek\n",
        "  if not os.path.exists('/content/r1_llama_70b_en-es-sw'):\n",
        "    assert os.path.exists('r1_llama_70b_en-es-sw.zip'), 'You need to upload r1_llama_70b_en-es-sw.zip'\n",
        "    ! unzip r1_llama_70b_en-es-sw.zip\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Unzip evaluations from DeepSeek\n",
        "  if not os.path.exists('/content/human-eval_final_jsons'):\n",
        "    assert os.path.exists('en_all_annotations.zip'), 'You need to upload en_all_annotations.zip'\n",
        "    ! unzip en_all_annotations.zip -d /content/human-eval_final_jsons\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KzMtQ7hiRlmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format DeepSeek outputs and save jsons in LLM-eval_final_jsons folder\n",
        "# DeepSeek outputs came out with a slightly different format from the Gemini and GPT outputs.\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# If no score is found for a datapoint, ignore it ('no') or assisng a default score (between 1 and 7)\n",
        "assign_default_score = 'no'#@param['no', '1', '2', '3', '4', '5', '6', '7']\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  llama_jsons = glob.glob(os.path.join('/content/r1_llama_70b_en-es-sw', '*.json'))\n",
        "\n",
        "  languages = ['en', 'es', 'sw']\n",
        "  allowed_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "  criteria = {'No-Omissions':'no-omissions', 'No-Additions':'no-additions', 'Grammaticality':'grammaticality', 'Fluency':'fluency'}\n",
        "\n",
        "  counter_all_datapoints = 0\n",
        "  for language in languages:\n",
        "    print(f'\\n{language}\\n--------')\n",
        "    list_dico_new_json = []\n",
        "    counter_lng_datapoints = 0\n",
        "    for llama_json in llama_jsons:\n",
        "      dico_new_json = {}\n",
        "      data_r1 = ''\n",
        "      with open(llama_json, 'r') as f:\n",
        "        data_r1 = json.load(f)\n",
        "      # print(data_r1['id'])\n",
        "      language_json = data_r1['id'].split('_', 1)[0]\n",
        "      if language_json == language:\n",
        "        dico_new_json['eid'] = data_r1['id']\n",
        "        dico_new_json['annotator_id'] = 'R1-Llama-70B'\n",
        "\n",
        "        # Check data\n",
        "        if data_r1['parsed_response']:\n",
        "          for criterion in sorted(criteria.keys(), reverse=True):\n",
        "            if data_r1['parsed_response'][criterion]['Score']:\n",
        "              if int(data_r1['parsed_response'][criterion]['Score']) in allowed_scores:\n",
        "                key_name = criteria[criterion]\n",
        "                dico_new_json[key_name] = int(data_r1['parsed_response'][criterion]['Score'])\n",
        "              else:\n",
        "                # This does not happen in the data\n",
        "                print(f'{data_r1[\"id\"]} - Criterion score out of range: {criterion}')\n",
        "            else:\n",
        "              # This does not happen either in the data\n",
        "              print(f'{data_r1[\"id\"]} - Criterion score missing: {criterion}')\n",
        "          counter_lng_datapoints += 1\n",
        "          # Add eval point to the main dico\n",
        "          list_dico_new_json.append(dico_new_json)\n",
        "        else:\n",
        "          # We have 11 cases of no answer from the model. Excluding data points for now.\n",
        "          print('  Missing scores for', data_r1['id'], ':', data_r1['parsed_response'])\n",
        "          if not assign_default_score == 'no':\n",
        "            for criterion in sorted(criteria.keys(), reverse=True):\n",
        "              # Assign median score for missing evals\n",
        "              key_name = criteria[criterion]\n",
        "              dico_new_json[key_name] = int(assign_default_score)\n",
        "              print(f'    Assigned {assign_default_score} for {criterion}')\n",
        "            list_dico_new_json.append(dico_new_json)\n",
        "\n",
        "    counter_all_datapoints += counter_lng_datapoints\n",
        "    print(f'Language {language}: {counter_lng_datapoints} data points')\n",
        "\n",
        "    # Save list_dico_new_json in a json file\n",
        "    with open(f'/content/LLM-eval_final_jsons/{language.upper()}_R1-Llama-70B_scores.json', 'w') as f:\n",
        "      json.dump(list_dico_new_json, f, indent=4)\n",
        "\n",
        "    print(f'All datapoints: {counter_all_datapoints}')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "xWYS5pZluSFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate system_level scores D2T assigned by each LLM or by the set of human annotators\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "json_files = ''\n",
        "if eval_method == 'LLM':\n",
        "  # Load the files one at a time; there is one file per LLM-language combination (4*3 = 12 files in total)\n",
        "  json_folder = '/content/LLM-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Load the files one at a time; there is one file per language (since human annotators did not each do all annotations, unlike LLMs)\n",
        "  json_folder = '/content/human-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "pd_list = [pd.read_json(file) for file in sorted(json_files)]\n",
        "\n",
        "# Create dataframes to store system-level scores\n",
        "sysLevel_scores_perLLM = pd.DataFrame(columns=['language', 'task', 'system', eval_method, 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "sysLevel_scores_Humans = pd.DataFrame(columns=['language', 'task', 'system', eval_method, 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "system_task_counter = 0\n",
        "for df in pd_list:\n",
        "  if DEBUG:\n",
        "    print(df)\n",
        "  # LLM and Human files use different IDs (eid VS id)\n",
        "  if eval_method == 'LLM':\n",
        "    # split eid column using underscores and create corresponding columns in the dataframe\n",
        "    df[['language', 'task', 'id', 'system']] = df['eid'].str.split('_', expand=True)\n",
        "  elif eval_method == 'Human':\n",
        "    # split id column using underscores and create corresponding columns in the dataframe\n",
        "    df[['language', 'task', 'id', 'system']] = df['id'].str.split('_', expand=True)\n",
        "\n",
        "  if eval_method == 'LLM':\n",
        "    print('Getting scores for', df['annotator_id'][0], df['language'][0], '...')\n",
        "  elif eval_method == 'Human':\n",
        "    print('Getting human scores for', df['language'][0])\n",
        "\n",
        "  # Get all possible values for task and system column\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  unique_systems = sorted(df['system'].unique())\n",
        "  # print('', unique_tasks)\n",
        "  # print('', unique_systems)\n",
        "\n",
        "  # Get the average scores for each system for each task\n",
        "  for task in unique_tasks:\n",
        "    if DEBUG:\n",
        "      print(f'Task: {task}\\n===========')\n",
        "    for system in unique_systems:\n",
        "      if DEBUG:\n",
        "        print(f'\\nSystem: {system}')\n",
        "      # get scores in each of the 4 score columns\n",
        "      no_omissions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no-omissions']\n",
        "      no_additions_scores = ''\n",
        "      # LLM and human files have different criterion names (hyphen VS underscore)\n",
        "      if eval_method == 'LLM':\n",
        "        no_additions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no-additions']\n",
        "      elif eval_method == 'Human':\n",
        "        no_additions_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'no_additions']\n",
        "      grammaticality_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'grammaticality']\n",
        "      fluency_scores = df.loc[(df['task'] == task) & (df['system'] == system), 'fluency']\n",
        "      #Get average scores\n",
        "      average_no_omissions_score = no_omissions_scores.mean()\n",
        "      average_no_additions_score = no_additions_scores.mean()\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "      if DEBUG:\n",
        "        if len(no_omissions_scores) > 0:\n",
        "          print(f'  Average no-omissions score: {average_no_omissions_score}')\n",
        "          print(f'  Average no-additions score: {average_no_additions_score}')\n",
        "          print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "          print(f'  Average fluency score: {average_fluency_score}')\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "\n",
        "        if eval_method == 'LLM':\n",
        "          # There are scores missing for DeepSeek\n",
        "          if not df['annotator_id'][0] == 'R1-Llama-70B':\n",
        "            assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 180, 'There should be 180 ratings for each criterion!'\n",
        "          else:\n",
        "            if no_omissions_scores.count() == 180:\n",
        "              assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 180, 'There should be 180 ratings for each criterion!'\n",
        "            else:\n",
        "              print(f\"  Less than 180 scores found: {df['language'][0]}-{task}-{system}\", no_omissions_scores.count(), no_additions_scores.count(), grammaticality_scores.count(), fluency_scores.count())\n",
        "          # Add a row to sysLevel_scores_perLLM dataframe\n",
        "          sysLevel_scores_perLLM.loc[system_task_counter] = [df['language'][0], task, system, df['annotator_id'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        elif eval_method == 'Human':\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() , 'There should be the same number of ratings for each criterion!'\n",
        "          if not no_omissions_scores.count() >= 390:\n",
        "            print('??? ERROR??? Less than 390 scores found:', df['language'][0], task, system)\n",
        "          # print(f'Number of no-omissions scores: {no_omissions_scores.count()}')\n",
        "          # print(f'Number of no-additions scores: {no_additions_scores.count()}')\n",
        "          # print(f'Number of grammaticality scores: {grammaticality_scores.count()}')\n",
        "          # print(f'Number of fluency scores: {fluency_scores.count()}')\n",
        "          # Add row to sysLevel_scores_Humans dataframe\n",
        "          sysLevel_scores_Humans.loc[system_task_counter] = [df['language'][0], task, system, eval_method, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "\n",
        "        system_task_counter += 1\n",
        "\n",
        "print()\n",
        "# At this point, all submissions, including the withdrawn one, were evaluated\n",
        "# EN = 46 rows: 7 systems * 6 datasets + 1 system * 3 datasets + human *1 dataset\n",
        "# ES = 18 rows: 3 systems * 6 datasets\n",
        "# SW = 15 rows: 2 systems * 6 datasets + 1 system * 3 datasets\n",
        "# All languages = 79 rows; 4 LLMS-> 316 rows\n",
        "if eval_method == 'LLM':\n",
        "  assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "  print(sysLevel_scores_perLLM)\n",
        "  # Dump dataframe in a CSV file\n",
        "  sysLevel_scores_perLLM.to_csv('sysLevel_scores_perLLM.csv', index=False)\n",
        "elif eval_method == 'Human':\n",
        "  assert len(sysLevel_scores_Humans) == 46, 'There should be 46 rows (EN) with scores for the human evals!'\n",
        "  print(sysLevel_scores_Humans)\n",
        "  # Dump dataframe in a CSV file\n",
        "  sysLevel_scores_Humans.to_csv('sysLevel_scores_Humans.csv', index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "ob48zv9sS_b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get average (across LLMs) system-level scores D2T\n",
        "# There are 4 LLMs which gave evaluations for all datapoints; we need this extra cell for LLMs to get an average LLM score for each system\n",
        "\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  # Create dataframe to store average LLM-assigned system-level scores\n",
        "  avg_sysLevel_scores_LLMs = pd.DataFrame(columns=['language', 'task', 'system', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "  # Now add a row that averages all LLM scores for each system for each task\n",
        "  unique_languages = sorted(sysLevel_scores_perLLM['language'].unique())\n",
        "  unique_tasks_avg = sorted(sysLevel_scores_perLLM['task'].unique())\n",
        "  unique_systems_avg = sorted(sysLevel_scores_perLLM['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  # print(unique_tasks_avg)\n",
        "  # print(unique_systems_avg)\n",
        "\n",
        "  language_system_task_counter = 0\n",
        "  for language in unique_languages:\n",
        "    if DEBUG:\n",
        "      print(f'Language: {language}')\n",
        "    for task in unique_tasks_avg:\n",
        "      if DEBUG:\n",
        "        print(f'Task: {task}')\n",
        "      for system in unique_systems_avg:\n",
        "        if DEBUG:\n",
        "          print(f'System: {system}')\n",
        "        # Get the average scores for each system for each task\n",
        "        no_omissions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_no-omissions']\n",
        "        no_additions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_no-additions']\n",
        "        grammaticality_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_grammaticality']\n",
        "        fluency_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system), 'avg_fluency']\n",
        "        # Get average\n",
        "        average_no_omissions_score = no_omissions_scores.mean()\n",
        "        average_no_additions_score = no_additions_scores.mean()\n",
        "        average_grammaticality_score = grammaticality_scores.mean()\n",
        "        average_fluency_score = fluency_scores.mean()\n",
        "        if DEBUG:\n",
        "          if len(no_omissions_scores) > 0:\n",
        "            print(f'  Average no-omissions score: {average_no_omissions_score}')\n",
        "            print(f'  Average no-additions score: {average_no_additions_score}')\n",
        "            print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "            print(f'  Average fluency score: {average_fluency_score}')\n",
        "\n",
        "        # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "        if not math.isnan(average_fluency_score):\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 4, 'There should be 4 ratings for each criterion!'\n",
        "          # Add a row to avg_scores dataframe\n",
        "          avg_sysLevel_scores_LLMs.loc[language_system_task_counter] = [language, task, system, 'Average_LLM', average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "          language_system_task_counter += 1\n",
        "          # Dump dataframe in a CSV file\n",
        "          avg_sysLevel_scores_LLMs.to_csv('sysLevel_scores_avgLLM.csv', index=False)\n",
        "\n",
        "  pd.set_option('display.max_rows', None)\n",
        "  print()\n",
        "  assert len(avg_sysLevel_scores_LLMs) == 79, 'There should be 79 rows with scores!'\n",
        "  print(avg_sysLevel_scores_LLMs)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zIucLwGOx0EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 - Create plots and LaTeX tables from scores aggregated in \"Qualitative evaluation\" above."
      ],
      "metadata": {
        "id": "FacsCjD-5JzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "fig_path = '/content/figures'\n",
        "\n",
        "def dictionarise_scores_all_datasets(df_scores, name_evaluator_column):\n",
        "  \"\"\"\n",
        "  To get a dico like the following: {'en': {'D2T-1-CFA': {'avg_no-omissions': {'GPT-4o-mini': [None, 6.06, 6.16, 5.37, 6.32, 5.83, 6.44, 6.38, 2.57], 'GPT-o3-mini': [None, 6.13, 6.37, 5.55, 6.46, 6.05, 6.97, 6.77, 2.72], ...} ...} ...} ...}\n",
        "  For plotting scores of an evaluation method across all systems for each criterion, for each dataset, for each language.\n",
        "  \"\"\"\n",
        "  unique_languages = sorted(df_scores['language'].unique())\n",
        "  unique_evaluator = sorted(df_scores[name_evaluator_column].unique())\n",
        "  unique_tasks = sorted(df_scores['task'].unique())\n",
        "  unique_systems = sorted(df_scores['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  # print(unique_LLMs)\n",
        "  # print(unique_tasks)\n",
        "  # print(unique_systems)\n",
        "\n",
        "  # system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  eval_criteria = ['avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency']\n",
        "\n",
        "  no_scores_found = []\n",
        "  counter_row_with_score = 0\n",
        "  # Let's plot how each LLM rates each system, and make a plot for each language*task*criterion\n",
        "  dico_plots = {}\n",
        "  for language in unique_languages:\n",
        "    if not language in dico_plots:\n",
        "      dico_plots[language] = {}\n",
        "    for task in unique_tasks:\n",
        "      if not ('All' in unique_tasks and not task == 'All'):\n",
        "        if not task in dico_plots[language]:\n",
        "          dico_plots[language][task] = {}\n",
        "        for eval_criterion in eval_criteria:\n",
        "          if not eval_criterion in dico_plots[language][task]:\n",
        "            dico_plots[language][task][eval_criterion] = {}\n",
        "          for evaluator in unique_evaluator:\n",
        "            if not evaluator in dico_plots[language][task][eval_criterion]:\n",
        "              dico_plots[language][task][eval_criterion][evaluator] = []\n",
        "            for system in unique_systems:\n",
        "              # Filter out withdrawn system\n",
        "              # if system in system_IDnames_dico[language].keys(): # Actually no, let's filter that later on\n",
        "              # Check that a system has scores for the task\n",
        "              if df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores[name_evaluator_column] == evaluator) & (df_scores['system'] == system)].empty:\n",
        "                if f'{language}-{task}-{system}-{evaluator}-{eval_criterion}' not in no_scores_found:\n",
        "                  no_scores_found.append(f'{language}-{task}-{system}-{evaluator}-{eval_criterion}')\n",
        "                dico_plots[language][task][eval_criterion][evaluator].append(None)\n",
        "              else:\n",
        "                # Append the score for the system found in the column with the header eval_criterion\n",
        "                dico_plots[language][task][eval_criterion][evaluator].append(float(round(df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores[name_evaluator_column] == evaluator) & (df_scores['system'] == system), eval_criterion].values[0], 2)))\n",
        "                counter_row_with_score += 1\n",
        "  return dico_plots, counter_row_with_score\n",
        "\n",
        "def plot_dicoPlots_lng_task_criterion_evalMethod_line (dico_plots, fig_path):\n",
        "  if not os.path.exists(fig_path):\n",
        "    os.mkdir(fig_path)\n",
        "  # system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  system_IDnames_dico = {'en' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  # system_colors = {'0':'dodgerblue', '1':'purple', '2':'blue', '3':'orange', '4':'green', '5':'red', '6':'purple', '7':'brown'}\n",
        "  # plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "  # plt.xlabel(\"System\")\n",
        "  # plt.ylabel(\"Score\")\n",
        "  # plt.legend()\n",
        "  for language in dico_plots.keys():\n",
        "    # print(language)\n",
        "    for task in dico_plots[language].keys():\n",
        "      # print(f'  {task}')\n",
        "      for eval_criterion in dico_plots[language][task]:\n",
        "        # print(f'    {eval_criterion}')\n",
        "        for LLM in dico_plots[language][task][eval_criterion]:\n",
        "          # print(f'      {LLM}')\n",
        "          list_scores = [dico_plots[language][task][eval_criterion][LLM][int(x)] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "          # print(f'        {list_scores}')\n",
        "          # plt.plot(range(1, len(list_scores) + 1), list_scores, label=LLM)\n",
        "          plt.plot([system_IDnames_dico[language][x] for x in sorted(system_IDnames_dico[language].keys())], list_scores, label=LLM)\n",
        "        plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "        if len(system_IDnames_dico[language].keys()) > 4:\n",
        "          plt.xticks(rotation=15)\n",
        "        plt.ylim(1, 7)\n",
        "        plt.xlabel(\"System\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(os.path.join(fig_path, f'{language}-{task}-{eval_criterion}_line.jpg'))\n",
        "        plt.show()\n",
        "\n",
        "def plot_dicoPlots_lng_task_criterion_evalMethod_bars (dico_plots, fig_path):\n",
        "  if not os.path.exists(fig_path):\n",
        "    os.mkdir(fig_path)\n",
        "  system_IDnames_dico = {'en' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  for language in dico_plots.keys():\n",
        "    for task in dico_plots[language].keys():\n",
        "      for eval_criterion in dico_plots[language][task]:\n",
        "        # Get list of systems for this language\n",
        "        systems = [system_IDnames_dico[language][x] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "\n",
        "        # Get list of scores for each LLM for this language, task, and criterion\n",
        "        llm_scores = {}\n",
        "        for LLM in dico_plots[language][task][eval_criterion]:\n",
        "          llm_scores[LLM] = [dico_plots[language][task][eval_criterion][LLM][int(x)] for x in sorted(system_IDnames_dico[language].keys())]\n",
        "\n",
        "        # Create bar plot\n",
        "        bar_width = 0.15  # Adjust as needed\n",
        "        x_pos = np.arange(len(systems))\n",
        "        # print(f'XPOS = {x_pos}')\n",
        "\n",
        "        # Plotting bars for each LLM with offset for readability\n",
        "        for i, (LLM, scores) in enumerate(llm_scores.items()):\n",
        "          scores_wo_None = [0 if x==None else x for x in scores]\n",
        "          # print(f'{x_pos + i * bar_width}, {scores_wo_None}, {bar_width}, {LLM}')\n",
        "          plt.bar(x_pos + i * bar_width, scores_wo_None, bar_width, label=LLM)\n",
        "\n",
        "        plt.title(f'{language}-{task}: {eval_criterion}')\n",
        "        plt.xticks(x_pos + bar_width * (len(llm_scores) -1)/2 , systems, rotation=15 if len(systems) > 4 else 0)  # Center x-axis ticks\n",
        "        plt.ylim(1, 7)\n",
        "        plt.xlabel(\"System\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.savefig(os.path.join(fig_path, f'{language}-{task}-{eval_criterion}_bar.jpg'))\n",
        "        plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T6njsi2VScJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot scores of each LLM independently, for each criterion, for each dataset"
      ],
      "metadata": {
        "id": "E6IxvYJf7Edx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Organise data into a dico and make plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "llm_csv = '/content/sysLevel_scores_perLLM.csv'\n",
        "assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "sysLevel_scores_perLLM = pd.read_csv(llm_csv)\n",
        "assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "\n",
        "dicoPlots_lng_task_criterion_evalMethod, counter_row_with_score = dictionarise_scores_all_datasets(sysLevel_scores_perLLM, 'LLM')\n",
        "assert counter_row_with_score == 1264, f'There should be 1264 rows with scores (with the 8 systems: 4 criteria * 316 rows - 184-en, 72-es, 60-sw), found {counter_row_with_score}!'\n",
        "# print(sorted(no_scores_found))\n",
        "print(dicoPlots_lng_task_criterion_evalMethod)\n",
        "\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_line(dicoPlots_lng_task_criterion_evalMethod, fig_path)\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_bars(dicoPlots_lng_task_criterion_evalMethod, fig_path)"
      ],
      "metadata": {
        "id": "J6xjI9455xW3",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot average scores across all datasets of all evaluations independently (4 LLMs and 1 human) for each criterion"
      ],
      "metadata": {
        "id": "_aJWR6ogjxCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine human and LLM eval dataframes and get averages across tasks for each system\n",
        "import os\n",
        "\n",
        "hum_csv = '/content/sysLevel_scores_Humans.csv'\n",
        "llm_csv = '/content/sysLevel_scores_perLLM.csv'\n",
        "assert os.path.exists(hum_csv), 'sysLevel_scores_Humans.csv file missing! Run cells in 7 above with \"Human\" eval_method'\n",
        "assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "\n",
        "# Load csv files into pandas dataframes\n",
        "df_hum = pd.read_csv(hum_csv)\n",
        "assert len(df_hum) == 46, 'There should be 46 rows (EN) with scores for the human evals!'\n",
        "print('There are 46 rows as expected in the human eval CSV!')\n",
        "df_llm = pd.read_csv(llm_csv)\n",
        "assert len(df_llm) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "print('There are 316 rows as expected in the LLM eval CSV!')\n",
        "# print(df_hum)\n",
        "# print(df_llm)\n",
        "\n",
        "# Combine both dataframes\n",
        "df_all = pd.DataFrame(columns=['language', 'task', 'system', 'evaluator', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "# index is used to store the current index in the dataframe\n",
        "index = 0\n",
        "# Put the human eval data first\n",
        "for index, row in df_hum.iterrows():\n",
        "  df_all.loc[index] = [row['language'], row['task'], row['system'], row['Human'], row['avg_no-omissions'], row['avg_no-additions'], row['avg_grammaticality'], row['avg_fluency']]\n",
        "# Then add the llm eval data on the following rows\n",
        "for _ , row in df_llm.iterrows():\n",
        "  index = index + 1\n",
        "  df_all.loc[index] = [row['language'], row['task'], row['system'], row['LLM'], row['avg_no-omissions'], row['avg_no-additions'], row['avg_grammaticality'], row['avg_fluency']]\n",
        "assert len(df_all) == (len(df_hum) + len(df_llm)), 'Error! The number of rows does not equal the sum of the number of rows in human eval df and llm df.'\n",
        "# print(df_all)\n",
        "\n",
        "# Create dataframe to store average LLM-assigned system-level scores\n",
        "avg_overall_sysLevel_scores_allEvals = pd.DataFrame(columns=['language', 'task', 'system', 'evaluator', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "# Get average scores across all tasks\n",
        "unique_languages = sorted(df_all['language'].unique())\n",
        "unique_evaluators = sorted(df_all['evaluator'].unique())\n",
        "unique_systems = sorted([int(system) for system in df_all['system'].unique()])\n",
        "print(unique_languages)\n",
        "print(unique_evaluators)\n",
        "print(unique_systems)\n",
        "\n",
        "language_system_evaluator_counter = 0\n",
        "for language in unique_languages:\n",
        "  for system in unique_systems:\n",
        "    for evaluator in unique_evaluators:\n",
        "      # Get average scores across all tasks\n",
        "      # print(f'{language}-{system}-{evaluator}')\n",
        "      no_omissions_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_no-omissions']\n",
        "      no_additions_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_no-additions']\n",
        "      grammaticality_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_grammaticality']\n",
        "      fluency_scores = df_all.loc[(df_all['language'] == language) & (df_all['evaluator'] == evaluator) & (df_all['system'] == system), 'avg_fluency']\n",
        "      average_no_omissions_score = no_omissions_scores.mean()\n",
        "      average_no_additions_score = no_additions_scores.mean()\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "        if system == 0:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 1, 'There should be 1 ratings for each criterion for system 0!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'D2T-1-FA', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        elif system == 1:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 3, 'There should be 3 ratings for each criterion for system 1!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'D2T-1', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        else:\n",
        "          assert no_omissions_scores.count() == no_additions_scores.count() == grammaticality_scores.count() == fluency_scores.count() == 6, 'There should be 6 ratings for each criterion for systems 2-8!'\n",
        "          avg_overall_sysLevel_scores_allEvals.loc[language_system_evaluator_counter] = [language, 'All', system, evaluator, average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        # Add a row to avg_scores dataframe\n",
        "        language_system_evaluator_counter += 1\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "print()\n",
        "print(avg_overall_sysLevel_scores_allEvals)\n",
        "assert len(avg_overall_sysLevel_scores_allEvals) == 69, 'There should be 75 rows with scores (en=45, es=15, sw=15)! Or 69 without ES and SW human evals.'\n",
        "print('There are 69 rows as expected in the output dataframe!')\n",
        "\n",
        "# Now only keep a version of the dataframe with only the \"task=All\" column. Not sure it's a good idea, breaks the next funciton to be applied because not all systems have values\n",
        "# avg_overall_sysLevel_scores_allEvals_allTasks = avg_overall_sysLevel_scores_allEvals.loc[avg_overall_sysLevel_scores_allEvals['task'] == 'All']\n",
        "# print()\n",
        "# print(avg_overall_sysLevel_scores_allEvals_allTasks)\n",
        "# assert len(avg_overall_sysLevel_scores_allEvals_allTasks) == 55, 'There should be 60 rows with scores (en=35, es=10, sw=15)! Or 55 without ES and SW human evals.'\n",
        "# print('There are 55 rows as expected in the output dataframe!')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "mfwjM49n5k5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Organise data into a dico and make plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make plots for all tasks separately (just in case we need it)\n",
        "# dicoPlots_lng_task_criterion_evalMethod_all, counter_row_with_score = dictionarise_scores_all_datasets(df_all, 'evaluator')\n",
        "# plot_dicoPlots_lng_task_criterion_evalMethod(dicoPlots_lng_task_criterion_evalMethod_all)\n",
        "\n",
        "# Put all LLM and human system-level evaluations in a dico for plotting\n",
        "dicoPlots_lng_criterion_evalMethod_all, counter_row_with_score = dictionarise_scores_all_datasets(avg_overall_sysLevel_scores_allEvals, 'evaluator')\n",
        "print(dicoPlots_lng_criterion_evalMethod_all)\n",
        "\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_line(dicoPlots_lng_criterion_evalMethod_all, fig_path)\n",
        "plot_dicoPlots_lng_task_criterion_evalMethod_bars(dicoPlots_lng_criterion_evalMethod_all, fig_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BZOtb-dxj0HB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download figures folder\n",
        "from google.colab import files\n",
        "from IPython.display import clear_output\n",
        "\n",
        "folder_to_zip = '/content/figures'\n",
        "zip_name_inter = folder_to_zip+'.zip'\n",
        "!zip -r {zip_name_inter} {folder_to_zip}\n",
        "\n",
        "clear_output()\n",
        "\n",
        "files.download(zip_name_inter)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XWxy_N_-96sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create LaTeX tables"
      ],
      "metadata": {
        "id": "MpML5-DM7SWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create LaTeX table rows with average scores\n",
        "\n",
        "#Get tables for averaged LLM scores (True), or for each LLM separately (False)\n",
        "eval_method_4table = 'Human'#@param['Human', 'LLM-avg', 'LLM-each']\n",
        "\n",
        "def create_LaTeX_table_parts(dico_avg_scores, eval_method, LLM=None):\n",
        "  unique_languages = sorted(dico_avg_scores['language'].unique())\n",
        "  # unique_tasks = sorted(avg_avg_scores['task'].unique())\n",
        "  unique_tasks = ['D2T-1-FA', 'D2T-1-CFA', 'D2T-1-FI', 'D2T-2-FA', 'D2T-2-CFA', 'D2T-2-FI']\n",
        "  unique_systems = sorted(dico_avg_scores['system'].unique())\n",
        "\n",
        "  # print(unique_languages)\n",
        "  print(f'Results printed in the following order of tasks: {unique_tasks}')\n",
        "  # print(unique_systems)\n",
        "\n",
        "  system_IDnames_dico = {'en' : {'0':'WebNLG-Human', '1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '4':'DipInfo-UniTo', '5':'OSU-CompLing', '6':'RDFpyrealb', '7':'SaarLST'}, 'es' : {'2':'DCU-NLG-PBN', '3':'DCU-NLG-Small', '5':'OSU-CompLing'}, 'sw' : {'1':'DCU-ADAPT-modPB', '2':'DCU-NLG-PBN', '3':'DCU-NLG-Small'}}\n",
        "  eval_criteria = {'avg_no-omissions':'No-Omissions', 'avg_no-additions':'No-Additions', 'avg_grammaticality':'Grammaticality', 'avg_fluency':'Fluency'}\n",
        "\n",
        "  # Since the table mixes human and LLM scores, which come from different files, I only print here the LLM scores for each criterion separately, and copy/paste it in the paper by hand (4 copy/paste per language)\n",
        "  counter_scores = 0\n",
        "  dico_plots = {}\n",
        "  for language in unique_languages:\n",
        "    num_systems_for_lng = len(system_IDnames_dico[language])\n",
        "    for eval_criterion in eval_criteria.keys():\n",
        "      if eval_method_4table == 'Human':\n",
        "        line = '\\\\multirow{'+str(num_systems_for_lng*2)+'}{*}{\\\\textbf{'+eval_criteria[eval_criterion]+'}} & \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{Avg. '+str(eval_method_4table)+'} $\\\\uparrow$} & '\n",
        "      elif eval_method_4table == 'LLM-avg':\n",
        "        line = ' & \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{Avg. '+str(eval_method_4table)+'s} $\\\\uparrow$} & '\n",
        "      elif eval_method_4table == 'LLM-each':\n",
        "        line = ' \\\\multirow{'+str(num_systems_for_lng)+'}{*}{\\\\textbf{'+str(LLM)+'} $\\\\uparrow$} & '\n",
        "      # Keep track of how many systems we see, because for the first system, the table line is different\n",
        "      count_systems = 0\n",
        "      for system in unique_systems:\n",
        "        # Exclude system that withdrew\n",
        "        if str(system) in system_IDnames_dico[language].keys():\n",
        "          avg_scores_system_across_tasks = []\n",
        "          # Keep track of if a system has an n/a score, to exclude from general average at the end\n",
        "          has_na = False\n",
        "          if count_systems > 0:\n",
        "            # For lines that are not that of the first system, start with two empty columns\n",
        "            if eval_method_4table == 'LLM-avg' or eval_method_4table == 'Human':\n",
        "              line = line + ' & & '\n",
        "            elif eval_method_4table == 'LLM-each':\n",
        "              line = line + ' & '\n",
        "          line = line + system_IDnames_dico[language][str(system)]\n",
        "          for task in unique_tasks:\n",
        "            # Print the score for this criterion\n",
        "            # Check that a system has scores for the task it is expected to\n",
        "            if dico_avg_scores.loc[(dico_avg_scores['language'] == language) & (dico_avg_scores['task'] == task) & (dico_avg_scores['system'] == system)].empty:\n",
        "              # print(f'  {language}-{eval_criterion}-{system}-{task}: n/a')\n",
        "              line = line + ' & ' + 'n/a'\n",
        "              has_na = True\n",
        "            else:\n",
        "              # print(f'  {language}-{eval_criterion}-{system}-{task}: {round(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]), 2)}')\n",
        "              # Add scores for one system across task to list for calculating the average across tasks\n",
        "              avg_scores_system_across_tasks.append(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]))\n",
        "              # Write the score for the current criterion for each of the 6 subtasks\n",
        "              line = line + ' & ' + str(round(float(dico_avg_scores.loc[(dico_avg_scores[\"language\"] == language) & (dico_avg_scores[\"task\"] == task) & (dico_avg_scores[\"system\"] == system), eval_criterion].values[0]), 2))\n",
        "              counter_scores += 1\n",
        "\n",
        "          # print(f'  {language}-{eval_criterion}-{system}-Avg: {round(sum(avg_scores_system_across_tasks)/len(avg_scores_system_across_tasks), 2)}')\n",
        "          # Only calculate overall average if a system has scores for all datasets\n",
        "          if has_na:\n",
        "            line = line + ' & ' + 'n/a'\n",
        "          else:\n",
        "            line = line + ' & ' + str(round(sum(avg_scores_system_across_tasks)/len(avg_scores_system_across_tasks), 2))\n",
        "          ## Add the latex linebreak at the end of the line, and a console linebreak for human readibility\n",
        "          line = line + '\\\\\\\\\\n'\n",
        "          count_systems += 1\n",
        "\n",
        "      if eval_method_4table == 'Human':\n",
        "        line = line + '  \\cline{2-10}\\n'\n",
        "      elif eval_method_4table == 'LLM':\n",
        "        line = line + '  \\hline\\n'\n",
        "      if LLM == None:\n",
        "        print(f'{language}-{eval_criterion}-{str(eval_method_4table)}\\n------------------------')\n",
        "      else:\n",
        "        print(f'{language}-{eval_criterion}-{LLM}\\n------------------------')\n",
        "      print(line)\n",
        "\n",
        "  assert counter_scores == 292, f'There should be 292 average scores (excluding system 8: 160-en, 72-es, 60-sw), found {counter_scores}!'\n",
        "\n",
        "\n",
        "hum_csv = '/content/sysLevel_scores_Humans.csv'\n",
        "llm_csv = '/content/sysLevel_scores_perLLM.csv'\n",
        "llm_avg_csv = '/content/sysLevel_scores_avgLLM.csv'\n",
        "\n",
        "# Tables for LLM scores\n",
        "if eval_method_4table == 'Human':\n",
        "  assert os.path.exists(hum_csv), 'sysLevel_scores_Humans.csv file missing! Run cells in 7 above with \"Human\" eval_method'\n",
        "  sysLevel_scores_Humans = pd.read_csv(hum_csv)\n",
        "  assert len(sysLevel_scores_Humans) == 46, 'There should be 46 rows (EN) with scores for the human evals!'\n",
        "  # print(sysLevel_scores_Humans)\n",
        "  create_LaTeX_table_parts(sysLevel_scores_Humans, eval_method_4table)\n",
        "elif eval_method_4table == 'LLM-avg':\n",
        "  assert os.path.exists(hum_csv), 'sysLevel_scores_avgLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method and then run cell for getting average LLM score.'\n",
        "  avg_sysLevel_scores_LLMs = pd.read_csv(llm_avg_csv)\n",
        "  assert len(avg_sysLevel_scores_LLMs) == 79, 'There should be 79 rows with average LLM scores!'\n",
        "  create_LaTeX_table_parts(avg_sysLevel_scores_LLMs, eval_method_4table)\n",
        "elif eval_method_4table == 'LLM-each':\n",
        "  assert os.path.exists(llm_csv), 'sysLevel_scores_perLLM.csv file missing! Run cells in 7 above with \"LLM\" eval_method'\n",
        "  sysLevel_scores_perLLM = pd.read_csv(llm_csv)\n",
        "  assert len(sysLevel_scores_perLLM) == 316, 'There should be 316 rows (EN, ES, SW) with scores with the 4 LLMs!'\n",
        "  unique_LLMs = sorted(sysLevel_scores_perLLM['LLM'].unique())\n",
        "  for LLM in unique_LLMs:\n",
        "    # Use only the part of the dataframe with the corresponding LLM\n",
        "    sysLevel_scores_perLLM_LLM = sysLevel_scores_perLLM.loc[sysLevel_scores_perLLM['LLM'] == LLM]\n",
        "    create_LaTeX_table_parts(sysLevel_scores_perLLM_LLM, eval_method_4table, LLM=LLM)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "so5idx2P4znQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NpKJM2gx_ELW",
        "Jo3-O0qFO0yV",
        "HrZjTEDs_MDE",
        "DSxbJ9sfV_cH",
        "6J6QSDw8EkD0",
        "eBhJJ9MYeRFe",
        "VB_Lq9pFfRgN",
        "N4Pc-zkSRkYa",
        "FacsCjD-5JzP",
        "E6IxvYJf7Edx",
        "_aJWR6ogjxCe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}